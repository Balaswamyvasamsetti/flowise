{
  "Tool": [],
  "ChatFlow": [
    {
      "id": "3cbf8053-f820-43ad-b61f-b9943aa48470",
      "name": "chatflow1",
      "flowData": "{\n  \"nodes\": [\n    {\n      \"id\": \"llmChain_0\",\n      \"position\": {\n        \"x\": -29.425790937506633,\n        \"y\": -635.8696651743571\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"llmChain_0\",\n        \"label\": \"LLM Chain\",\n        \"version\": 3,\n        \"name\": \"llmChain\",\n        \"type\": \"LLMChain\",\n        \"baseClasses\": [\n          \"LLMChain\",\n          \"BaseChain\",\n          \"Runnable\"\n        ],\n        \"category\": \"Chains\",\n        \"description\": \"Chain to run queries against LLMs\",\n        \"inputParams\": [\n          {\n            \"label\": \"Chain Name\",\n            \"name\": \"chainName\",\n            \"type\": \"string\",\n            \"placeholder\": \"Name Your Chain\",\n            \"optional\": true,\n            \"id\": \"llmChain_0-input-chainName-string\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Language Model\",\n            \"name\": \"model\",\n            \"type\": \"BaseLanguageModel\",\n            \"id\": \"llmChain_0-input-model-BaseLanguageModel\"\n          },\n          {\n            \"label\": \"Prompt\",\n            \"name\": \"prompt\",\n            \"type\": \"BasePromptTemplate\",\n            \"id\": \"llmChain_0-input-prompt-BasePromptTemplate\"\n          },\n          {\n            \"label\": \"Output Parser\",\n            \"name\": \"outputParser\",\n            \"type\": \"BaseLLMOutputParser\",\n            \"optional\": true,\n            \"id\": \"llmChain_0-input-outputParser-BaseLLMOutputParser\"\n          },\n          {\n            \"label\": \"Input Moderation\",\n            \"description\": \"Detect text that could generate harmful output and prevent it from being sent to the language model\",\n            \"name\": \"inputModeration\",\n            \"type\": \"Moderation\",\n            \"optional\": true,\n            \"list\": true,\n            \"id\": \"llmChain_0-input-inputModeration-Moderation\"\n          }\n        ],\n        \"inputs\": {\n          \"model\": \"{{cohere_0.data.instance}}\",\n          \"prompt\": \"{{promptTemplate_0.data.instance}}\",\n          \"outputParser\": \"\",\n          \"inputModeration\": \"\",\n          \"chainName\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"name\": \"output\",\n            \"label\": \"Output\",\n            \"type\": \"options\",\n            \"description\": \"\",\n            \"options\": [\n              {\n                \"id\": \"llmChain_0-output-llmChain-LLMChain|BaseChain|Runnable\",\n                \"name\": \"llmChain\",\n                \"label\": \"LLM Chain\",\n                \"description\": \"\",\n                \"type\": \"LLMChain | BaseChain | Runnable\"\n              },\n              {\n                \"id\": \"llmChain_0-output-outputPrediction-string|json\",\n                \"name\": \"outputPrediction\",\n                \"label\": \"Output Prediction\",\n                \"description\": \"\",\n                \"type\": \"string | json\"\n              }\n            ],\n            \"default\": \"llmChain\"\n          }\n        ],\n        \"outputs\": {\n          \"output\": \"outputPrediction\"\n        },\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 507,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": -29.425790937506633,\n        \"y\": -635.8696651743571\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"promptTemplate_0\",\n      \"position\": {\n        \"x\": -701.0016039951031,\n        \"y\": -513.5944285947871\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"promptTemplate_0\",\n        \"label\": \"Prompt Template\",\n        \"version\": 1,\n        \"name\": \"promptTemplate\",\n        \"type\": \"PromptTemplate\",\n        \"baseClasses\": [\n          \"PromptTemplate\",\n          \"BaseStringPromptTemplate\",\n          \"BasePromptTemplate\",\n          \"Runnable\"\n        ],\n        \"category\": \"Prompts\",\n        \"description\": \"Schema to represent a basic prompt for an LLM\",\n        \"inputParams\": [\n          {\n            \"label\": \"Template\",\n            \"name\": \"template\",\n            \"type\": \"string\",\n            \"rows\": 4,\n            \"placeholder\": \"What is a good name for a company that makes {product}?\",\n            \"id\": \"promptTemplate_0-input-template-string\"\n          },\n          {\n            \"label\": \"Format Prompt Values\",\n            \"name\": \"promptValues\",\n            \"type\": \"json\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"list\": true,\n            \"id\": \"promptTemplate_0-input-promptValues-json\"\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"template\": \"you are experienced in chef . create a unique recipe using the main ingredient {ingredient}.\",\n          \"promptValues\": \"{\\\"ingredient\\\":\\\"{{question}}\\\"}\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"promptTemplate_0-output-promptTemplate-PromptTemplate|BaseStringPromptTemplate|BasePromptTemplate|Runnable\",\n            \"name\": \"promptTemplate\",\n            \"label\": \"PromptTemplate\",\n            \"description\": \"Schema to represent a basic prompt for an LLM\",\n            \"type\": \"PromptTemplate | BaseStringPromptTemplate | BasePromptTemplate | Runnable\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 512,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": -701.0016039951031,\n        \"y\": -513.5944285947871\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"cohere_0\",\n      \"position\": {\n        \"x\": -708.166461378508,\n        \"y\": -1261.2798323542538\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"cohere_0\",\n        \"label\": \"Cohere\",\n        \"version\": 3,\n        \"name\": \"cohere\",\n        \"type\": \"Cohere\",\n        \"baseClasses\": [\n          \"Cohere\",\n          \"LLM\",\n          \"BaseLLM\",\n          \"BaseLanguageModel\",\n          \"Runnable\"\n        ],\n        \"category\": \"LLMs\",\n        \"description\": \"Wrapper around Cohere large language models\",\n        \"inputParams\": [\n          {\n            \"label\": \"Connect Credential\",\n            \"name\": \"credential\",\n            \"type\": \"credential\",\n            \"credentialNames\": [\n              \"cohereApi\"\n            ],\n            \"id\": \"cohere_0-input-credential-credential\"\n          },\n          {\n            \"label\": \"Model Name\",\n            \"name\": \"modelName\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"default\": \"command\",\n            \"id\": \"cohere_0-input-modelName-asyncOptions\"\n          },\n          {\n            \"label\": \"Temperature\",\n            \"name\": \"temperature\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"default\": 0.7,\n            \"optional\": true,\n            \"id\": \"cohere_0-input-temperature-number\"\n          },\n          {\n            \"label\": \"Max Tokens\",\n            \"name\": \"maxTokens\",\n            \"type\": \"number\",\n            \"step\": 1,\n            \"optional\": true,\n            \"id\": \"cohere_0-input-maxTokens-number\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Cache\",\n            \"name\": \"cache\",\n            \"type\": \"BaseCache\",\n            \"optional\": true,\n            \"id\": \"cohere_0-input-cache-BaseCache\"\n          }\n        ],\n        \"inputs\": {\n          \"cache\": \"\",\n          \"modelName\": \"command\",\n          \"temperature\": 0.7,\n          \"maxTokens\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"cohere_0-output-cohere-Cohere|LLM|BaseLLM|BaseLanguageModel|Runnable\",\n            \"name\": \"cohere\",\n            \"label\": \"Cohere\",\n            \"description\": \"Wrapper around Cohere large language models\",\n            \"type\": \"Cohere | LLM | BaseLLM | BaseLanguageModel | Runnable\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 619,\n      \"selected\": false,\n      \"dragging\": false,\n      \"positionAbsolute\": {\n        \"x\": -708.166461378508,\n        \"y\": -1261.2798323542538\n      }\n    },\n    {\n      \"id\": \"llmChain_1\",\n      \"position\": {\n        \"x\": 1416.4792521980926,\n        \"y\": -1046.1744495457947\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"llmChain_1\",\n        \"label\": \"LLM Chain\",\n        \"version\": 3,\n        \"name\": \"llmChain\",\n        \"type\": \"LLMChain\",\n        \"baseClasses\": [\n          \"LLMChain\",\n          \"BaseChain\",\n          \"Runnable\"\n        ],\n        \"category\": \"Chains\",\n        \"description\": \"Chain to run queries against LLMs\",\n        \"inputParams\": [\n          {\n            \"label\": \"Chain Name\",\n            \"name\": \"chainName\",\n            \"type\": \"string\",\n            \"placeholder\": \"Name Your Chain\",\n            \"optional\": true,\n            \"id\": \"llmChain_1-input-chainName-string\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Language Model\",\n            \"name\": \"model\",\n            \"type\": \"BaseLanguageModel\",\n            \"id\": \"llmChain_1-input-model-BaseLanguageModel\"\n          },\n          {\n            \"label\": \"Prompt\",\n            \"name\": \"prompt\",\n            \"type\": \"BasePromptTemplate\",\n            \"id\": \"llmChain_1-input-prompt-BasePromptTemplate\"\n          },\n          {\n            \"label\": \"Output Parser\",\n            \"name\": \"outputParser\",\n            \"type\": \"BaseLLMOutputParser\",\n            \"optional\": true,\n            \"id\": \"llmChain_1-input-outputParser-BaseLLMOutputParser\"\n          },\n          {\n            \"label\": \"Input Moderation\",\n            \"description\": \"Detect text that could generate harmful output and prevent it from being sent to the language model\",\n            \"name\": \"inputModeration\",\n            \"type\": \"Moderation\",\n            \"optional\": true,\n            \"list\": true,\n            \"id\": \"llmChain_1-input-inputModeration-Moderation\"\n          }\n        ],\n        \"inputs\": {\n          \"model\": \"{{chatCohere_0.data.instance}}\",\n          \"prompt\": \"{{promptTemplate_1.data.instance}}\",\n          \"outputParser\": \"\",\n          \"inputModeration\": \"\",\n          \"chainName\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"name\": \"output\",\n            \"label\": \"Output\",\n            \"type\": \"options\",\n            \"description\": \"\",\n            \"options\": [\n              {\n                \"id\": \"llmChain_1-output-llmChain-LLMChain|BaseChain|Runnable\",\n                \"name\": \"llmChain\",\n                \"label\": \"LLM Chain\",\n                \"description\": \"\",\n                \"type\": \"LLMChain | BaseChain | Runnable\"\n              },\n              {\n                \"id\": \"llmChain_1-output-outputPrediction-string|json\",\n                \"name\": \"outputPrediction\",\n                \"label\": \"Output Prediction\",\n                \"description\": \"\",\n                \"type\": \"string | json\"\n              }\n            ],\n            \"default\": \"llmChain\"\n          }\n        ],\n        \"outputs\": {\n          \"output\": \"llmChain\"\n        },\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 507,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 1416.4792521980926,\n        \"y\": -1046.1744495457947\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"promptTemplate_1\",\n      \"position\": {\n        \"x\": 574.0417896562022,\n        \"y\": -713.0504060531896\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"promptTemplate_1\",\n        \"label\": \"Prompt Template\",\n        \"version\": 1,\n        \"name\": \"promptTemplate\",\n        \"type\": \"PromptTemplate\",\n        \"baseClasses\": [\n          \"PromptTemplate\",\n          \"BaseStringPromptTemplate\",\n          \"BasePromptTemplate\",\n          \"Runnable\"\n        ],\n        \"category\": \"Prompts\",\n        \"description\": \"Schema to represent a basic prompt for an LLM\",\n        \"inputParams\": [\n          {\n            \"label\": \"Template\",\n            \"name\": \"template\",\n            \"type\": \"string\",\n            \"rows\": 4,\n            \"placeholder\": \"What is a good name for a company that makes {product}?\",\n            \"id\": \"promptTemplate_1-input-template-string\"\n          },\n          {\n            \"label\": \"Format Prompt Values\",\n            \"name\": \"promptValues\",\n            \"type\": \"json\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"list\": true,\n            \"id\": \"promptTemplate_1-input-promptValues-json\"\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"template\": \"you are a bala and rude food critic.\\nwrite a review about the fallowing recipe:{recipe}\",\n          \"promptValues\": \"{\\\"recipe\\\":\\\"\\\"}\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"promptTemplate_1-output-promptTemplate-PromptTemplate|BaseStringPromptTemplate|BasePromptTemplate|Runnable\",\n            \"name\": \"promptTemplate\",\n            \"label\": \"PromptTemplate\",\n            \"description\": \"Schema to represent a basic prompt for an LLM\",\n            \"type\": \"PromptTemplate | BaseStringPromptTemplate | BasePromptTemplate | Runnable\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 512,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 574.0417896562022,\n        \"y\": -713.0504060531896\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"chatCohere_0\",\n      \"position\": {\n        \"x\": 659.9887730203106,\n        \"y\": -1428.7980037398534\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"chatCohere_0\",\n        \"label\": \"ChatCohere\",\n        \"version\": 1,\n        \"name\": \"chatCohere\",\n        \"type\": \"ChatCohere\",\n        \"baseClasses\": [\n          \"ChatCohere\",\n          \"BaseChatModel\",\n          \"BaseLanguageModel\",\n          \"Runnable\"\n        ],\n        \"category\": \"Chat Models\",\n        \"description\": \"Wrapper around Cohere Chat Endpoints\",\n        \"inputParams\": [\n          {\n            \"label\": \"Connect Credential\",\n            \"name\": \"credential\",\n            \"type\": \"credential\",\n            \"credentialNames\": [\n              \"cohereApi\"\n            ],\n            \"id\": \"chatCohere_0-input-credential-credential\"\n          },\n          {\n            \"label\": \"Model Name\",\n            \"name\": \"modelName\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"default\": \"command-r\",\n            \"id\": \"chatCohere_0-input-modelName-asyncOptions\"\n          },\n          {\n            \"label\": \"Temperature\",\n            \"name\": \"temperature\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"default\": 0.7,\n            \"optional\": true,\n            \"id\": \"chatCohere_0-input-temperature-number\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Cache\",\n            \"name\": \"cache\",\n            \"type\": \"BaseCache\",\n            \"optional\": true,\n            \"id\": \"chatCohere_0-input-cache-BaseCache\"\n          }\n        ],\n        \"inputs\": {\n          \"cache\": \"\",\n          \"modelName\": \"command-r-plus\",\n          \"temperature\": 0.7\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"chatCohere_0-output-chatCohere-ChatCohere|BaseChatModel|BaseLanguageModel|Runnable\",\n            \"name\": \"chatCohere\",\n            \"label\": \"ChatCohere\",\n            \"description\": \"Wrapper around Cohere Chat Endpoints\",\n            \"type\": \"ChatCohere | BaseChatModel | BaseLanguageModel | Runnable\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 520,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 659.9887730203106,\n        \"y\": -1428.7980037398534\n      },\n      \"dragging\": false\n    }\n  ],\n  \"edges\": [\n    {\n      \"source\": \"promptTemplate_0\",\n      \"sourceHandle\": \"promptTemplate_0-output-promptTemplate-PromptTemplate|BaseStringPromptTemplate|BasePromptTemplate|Runnable\",\n      \"target\": \"llmChain_0\",\n      \"targetHandle\": \"llmChain_0-input-prompt-BasePromptTemplate\",\n      \"type\": \"buttonedge\",\n      \"id\": \"promptTemplate_0-promptTemplate_0-output-promptTemplate-PromptTemplate|BaseStringPromptTemplate|BasePromptTemplate|Runnable-llmChain_0-llmChain_0-input-prompt-BasePromptTemplate\"\n    },\n    {\n      \"source\": \"cohere_0\",\n      \"sourceHandle\": \"cohere_0-output-cohere-Cohere|LLM|BaseLLM|BaseLanguageModel|Runnable\",\n      \"target\": \"llmChain_0\",\n      \"targetHandle\": \"llmChain_0-input-model-BaseLanguageModel\",\n      \"type\": \"buttonedge\",\n      \"id\": \"cohere_0-cohere_0-output-cohere-Cohere|LLM|BaseLLM|BaseLanguageModel|Runnable-llmChain_0-llmChain_0-input-model-BaseLanguageModel\"\n    },\n    {\n      \"source\": \"promptTemplate_1\",\n      \"sourceHandle\": \"promptTemplate_1-output-promptTemplate-PromptTemplate|BaseStringPromptTemplate|BasePromptTemplate|Runnable\",\n      \"target\": \"llmChain_1\",\n      \"targetHandle\": \"llmChain_1-input-prompt-BasePromptTemplate\",\n      \"type\": \"buttonedge\",\n      \"id\": \"promptTemplate_1-promptTemplate_1-output-promptTemplate-PromptTemplate|BaseStringPromptTemplate|BasePromptTemplate|Runnable-llmChain_1-llmChain_1-input-prompt-BasePromptTemplate\"\n    },\n    {\n      \"source\": \"llmChain_0\",\n      \"sourceHandle\": \"llmChain_0-output-outputPrediction-string|json\",\n      \"target\": \"promptTemplate_1\",\n      \"targetHandle\": \"promptTemplate_1-input-promptValues-json\",\n      \"type\": \"buttonedge\",\n      \"id\": \"llmChain_0-llmChain_0-output-outputPrediction-string|json-promptTemplate_1-promptTemplate_1-input-promptValues-json\"\n    },\n    {\n      \"source\": \"chatCohere_0\",\n      \"sourceHandle\": \"chatCohere_0-output-chatCohere-ChatCohere|BaseChatModel|BaseLanguageModel|Runnable\",\n      \"target\": \"llmChain_1\",\n      \"targetHandle\": \"llmChain_1-input-model-BaseLanguageModel\",\n      \"type\": \"buttonedge\",\n      \"id\": \"chatCohere_0-chatCohere_0-output-chatCohere-ChatCohere|BaseChatModel|BaseLanguageModel|Runnable-llmChain_1-llmChain_1-input-model-BaseLanguageModel\"\n    }\n  ]\n}",
      "type": "CHATFLOW"
    },
    {
      "id": "7d8ae845-be54-4b9a-966f-7ba426ee1a94",
      "name": "output parsers",
      "flowData": "{\n  \"nodes\": [\n    {\n      \"id\": \"llmChain_0\",\n      \"position\": {\n        \"x\": 1273.325620983927,\n        \"y\": -635.6184884760307\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"llmChain_0\",\n        \"label\": \"LLM Chain\",\n        \"version\": 3,\n        \"name\": \"llmChain\",\n        \"type\": \"LLMChain\",\n        \"baseClasses\": [\n          \"LLMChain\",\n          \"BaseChain\",\n          \"Runnable\"\n        ],\n        \"category\": \"Chains\",\n        \"description\": \"Chain to run queries against LLMs\",\n        \"inputParams\": [\n          {\n            \"label\": \"Chain Name\",\n            \"name\": \"chainName\",\n            \"type\": \"string\",\n            \"placeholder\": \"Name Your Chain\",\n            \"optional\": true,\n            \"id\": \"llmChain_0-input-chainName-string\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Language Model\",\n            \"name\": \"model\",\n            \"type\": \"BaseLanguageModel\",\n            \"id\": \"llmChain_0-input-model-BaseLanguageModel\"\n          },\n          {\n            \"label\": \"Prompt\",\n            \"name\": \"prompt\",\n            \"type\": \"BasePromptTemplate\",\n            \"id\": \"llmChain_0-input-prompt-BasePromptTemplate\"\n          },\n          {\n            \"label\": \"Output Parser\",\n            \"name\": \"outputParser\",\n            \"type\": \"BaseLLMOutputParser\",\n            \"optional\": true,\n            \"id\": \"llmChain_0-input-outputParser-BaseLLMOutputParser\"\n          },\n          {\n            \"label\": \"Input Moderation\",\n            \"description\": \"Detect text that could generate harmful output and prevent it from being sent to the language model\",\n            \"name\": \"inputModeration\",\n            \"type\": \"Moderation\",\n            \"optional\": true,\n            \"list\": true,\n            \"id\": \"llmChain_0-input-inputModeration-Moderation\"\n          }\n        ],\n        \"inputs\": {\n          \"model\": \"{{chatGoogleGenerativeAI_0.data.instance}}\",\n          \"prompt\": \"{{promptTemplate_0.data.instance}}\",\n          \"outputParser\": \"\",\n          \"inputModeration\": \"\",\n          \"chainName\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"name\": \"output\",\n            \"label\": \"Output\",\n            \"type\": \"options\",\n            \"description\": \"\",\n            \"options\": [\n              {\n                \"id\": \"llmChain_0-output-llmChain-LLMChain|BaseChain|Runnable\",\n                \"name\": \"llmChain\",\n                \"label\": \"LLM Chain\",\n                \"description\": \"\",\n                \"type\": \"LLMChain | BaseChain | Runnable\"\n              },\n              {\n                \"id\": \"llmChain_0-output-outputPrediction-string|json\",\n                \"name\": \"outputPrediction\",\n                \"label\": \"Output Prediction\",\n                \"description\": \"\",\n                \"type\": \"string | json\"\n              }\n            ],\n            \"default\": \"llmChain\"\n          }\n        ],\n        \"outputs\": {\n          \"output\": \"llmChain\"\n        },\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 506,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 1273.325620983927,\n        \"y\": -635.6184884760307\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"chatGoogleGenerativeAI_0\",\n      \"position\": {\n        \"x\": 859.75176680683,\n        \"y\": -1000.2457094317386\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"chatGoogleGenerativeAI_0\",\n        \"label\": \"ChatGoogleGenerativeAI\",\n        \"version\": 2.1,\n        \"name\": \"chatGoogleGenerativeAI\",\n        \"type\": \"ChatGoogleGenerativeAI\",\n        \"baseClasses\": [\n          \"ChatGoogleGenerativeAI\",\n          \"LangchainChatGoogleGenerativeAI\",\n          \"BaseChatModel\",\n          \"BaseLanguageModel\",\n          \"Runnable\"\n        ],\n        \"category\": \"Chat Models\",\n        \"description\": \"Wrapper around Google Gemini large language models that use the Chat endpoint\",\n        \"inputParams\": [\n          {\n            \"label\": \"Connect Credential\",\n            \"name\": \"credential\",\n            \"type\": \"credential\",\n            \"credentialNames\": [\n              \"googleGenerativeAI\"\n            ],\n            \"optional\": false,\n            \"description\": \"Google Generative AI credential.\",\n            \"id\": \"chatGoogleGenerativeAI_0-input-credential-credential\"\n          },\n          {\n            \"label\": \"Model Name\",\n            \"name\": \"modelName\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"default\": \"gemini-pro\",\n            \"id\": \"chatGoogleGenerativeAI_0-input-modelName-asyncOptions\"\n          },\n          {\n            \"label\": \"Custom Model Name\",\n            \"name\": \"customModelName\",\n            \"type\": \"string\",\n            \"placeholder\": \"gemini-1.5-pro-exp-0801\",\n            \"description\": \"Custom model name to use. If provided, it will override the model selected\",\n            \"additionalParams\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-customModelName-string\"\n          },\n          {\n            \"label\": \"Temperature\",\n            \"name\": \"temperature\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"default\": 0.9,\n            \"optional\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-temperature-number\"\n          },\n          {\n            \"label\": \"Max Output Tokens\",\n            \"name\": \"maxOutputTokens\",\n            \"type\": \"number\",\n            \"step\": 1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-maxOutputTokens-number\"\n          },\n          {\n            \"label\": \"Top Probability\",\n            \"name\": \"topP\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-topP-number\"\n          },\n          {\n            \"label\": \"Top Next Highest Probability Tokens\",\n            \"name\": \"topK\",\n            \"type\": \"number\",\n            \"description\": \"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive\",\n            \"step\": 1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-topK-number\"\n          },\n          {\n            \"label\": \"Harm Category\",\n            \"name\": \"harmCategory\",\n            \"type\": \"multiOptions\",\n            \"description\": \"Refer to <a target=\\\"_blank\\\" href=\\\"https://cloud.google.com/vertex-ai/docs/generative-ai/multimodal/configure-safety-attributes#safety_attribute_definitions\\\">official guide</a> on how to use Harm Category\",\n            \"options\": [\n              {\n                \"label\": \"Dangerous\",\n                \"name\": \"HARM_CATEGORY_DANGEROUS_CONTENT\"\n              },\n              {\n                \"label\": \"Harassment\",\n                \"name\": \"HARM_CATEGORY_HARASSMENT\"\n              },\n              {\n                \"label\": \"Hate Speech\",\n                \"name\": \"HARM_CATEGORY_HATE_SPEECH\"\n              },\n              {\n                \"label\": \"Sexually Explicit\",\n                \"name\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\"\n              }\n            ],\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-harmCategory-multiOptions\"\n          },\n          {\n            \"label\": \"Harm Block Threshold\",\n            \"name\": \"harmBlockThreshold\",\n            \"type\": \"multiOptions\",\n            \"description\": \"Refer to <a target=\\\"_blank\\\" href=\\\"https://cloud.google.com/vertex-ai/docs/generative-ai/multimodal/configure-safety-attributes#safety_setting_thresholds\\\">official guide</a> on how to use Harm Block Threshold\",\n            \"options\": [\n              {\n                \"label\": \"Low and Above\",\n                \"name\": \"BLOCK_LOW_AND_ABOVE\"\n              },\n              {\n                \"label\": \"Medium and Above\",\n                \"name\": \"BLOCK_MEDIUM_AND_ABOVE\"\n              },\n              {\n                \"label\": \"None\",\n                \"name\": \"BLOCK_NONE\"\n              },\n              {\n                \"label\": \"Only High\",\n                \"name\": \"BLOCK_ONLY_HIGH\"\n              },\n              {\n                \"label\": \"Threshold Unspecified\",\n                \"name\": \"HARM_BLOCK_THRESHOLD_UNSPECIFIED\"\n              }\n            ],\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-harmBlockThreshold-multiOptions\"\n          },\n          {\n            \"label\": \"Allow Image Uploads\",\n            \"name\": \"allowImageUploads\",\n            \"type\": \"boolean\",\n            \"description\": \"Automatically uses vision model when image is being uploaded from chat. Only works with LLMChain, Conversation Chain, ReAct Agent, Conversational Agent, Tool Agent\",\n            \"default\": false,\n            \"optional\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-allowImageUploads-boolean\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Cache\",\n            \"name\": \"cache\",\n            \"type\": \"BaseCache\",\n            \"optional\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-cache-BaseCache\"\n          }\n        ],\n        \"inputs\": {\n          \"cache\": \"\",\n          \"modelName\": \"gemini-1.5-pro-latest\",\n          \"customModelName\": \"\",\n          \"temperature\": \"0.7\",\n          \"maxOutputTokens\": \"\",\n          \"topP\": \"\",\n          \"topK\": \"\",\n          \"harmCategory\": \"\",\n          \"harmBlockThreshold\": \"\",\n          \"allowImageUploads\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"chatGoogleGenerativeAI_0-output-chatGoogleGenerativeAI-ChatGoogleGenerativeAI|LangchainChatGoogleGenerativeAI|BaseChatModel|BaseLanguageModel|Runnable\",\n            \"name\": \"chatGoogleGenerativeAI\",\n            \"label\": \"ChatGoogleGenerativeAI\",\n            \"description\": \"Wrapper around Google Gemini large language models that use the Chat endpoint\",\n            \"type\": \"ChatGoogleGenerativeAI | LangchainChatGoogleGenerativeAI | BaseChatModel | BaseLanguageModel | Runnable\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 668,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 859.75176680683,\n        \"y\": -1000.2457094317386\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"promptTemplate_0\",\n      \"position\": {\n        \"x\": 845.7266574466618,\n        \"y\": -286.6742325110172\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"promptTemplate_0\",\n        \"label\": \"Prompt Template\",\n        \"version\": 1,\n        \"name\": \"promptTemplate\",\n        \"type\": \"PromptTemplate\",\n        \"baseClasses\": [\n          \"PromptTemplate\",\n          \"BaseStringPromptTemplate\",\n          \"BasePromptTemplate\",\n          \"Runnable\"\n        ],\n        \"category\": \"Prompts\",\n        \"description\": \"Schema to represent a basic prompt for an LLM\",\n        \"inputParams\": [\n          {\n            \"label\": \"Template\",\n            \"name\": \"template\",\n            \"type\": \"string\",\n            \"rows\": 4,\n            \"placeholder\": \"What is a good name for a company that makes {product}?\",\n            \"id\": \"promptTemplate_0-input-template-string\"\n          },\n          {\n            \"label\": \"Format Prompt Values\",\n            \"name\": \"promptValues\",\n            \"type\": \"json\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"list\": true,\n            \"id\": \"promptTemplate_0-input-promptValues-json\"\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"template\": \"\",\n          \"promptValues\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"promptTemplate_0-output-promptTemplate-PromptTemplate|BaseStringPromptTemplate|BasePromptTemplate|Runnable\",\n            \"name\": \"promptTemplate\",\n            \"label\": \"PromptTemplate\",\n            \"description\": \"Schema to represent a basic prompt for an LLM\",\n            \"type\": \"PromptTemplate | BaseStringPromptTemplate | BasePromptTemplate | Runnable\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 509,\n      \"positionAbsolute\": {\n        \"x\": 845.7266574466618,\n        \"y\": -286.6742325110172\n      },\n      \"selected\": false,\n      \"dragging\": false\n    }\n  ],\n  \"edges\": [\n    {\n      \"source\": \"chatGoogleGenerativeAI_0\",\n      \"sourceHandle\": \"chatGoogleGenerativeAI_0-output-chatGoogleGenerativeAI-ChatGoogleGenerativeAI|LangchainChatGoogleGenerativeAI|BaseChatModel|BaseLanguageModel|Runnable\",\n      \"target\": \"llmChain_0\",\n      \"targetHandle\": \"llmChain_0-input-model-BaseLanguageModel\",\n      \"type\": \"buttonedge\",\n      \"id\": \"chatGoogleGenerativeAI_0-chatGoogleGenerativeAI_0-output-chatGoogleGenerativeAI-ChatGoogleGenerativeAI|LangchainChatGoogleGenerativeAI|BaseChatModel|BaseLanguageModel|Runnable-llmChain_0-llmChain_0-input-model-BaseLanguageModel\"\n    },\n    {\n      \"source\": \"promptTemplate_0\",\n      \"sourceHandle\": \"promptTemplate_0-output-promptTemplate-PromptTemplate|BaseStringPromptTemplate|BasePromptTemplate|Runnable\",\n      \"target\": \"llmChain_0\",\n      \"targetHandle\": \"llmChain_0-input-prompt-BasePromptTemplate\",\n      \"type\": \"buttonedge\",\n      \"id\": \"promptTemplate_0-promptTemplate_0-output-promptTemplate-PromptTemplate|BaseStringPromptTemplate|BasePromptTemplate|Runnable-llmChain_0-llmChain_0-input-prompt-BasePromptTemplate\"\n    }\n  ]\n}",
      "type": "CHATFLOW"
    },
    {
      "id": "4d46c772-6ed7-47bf-b6f0-f5272154fcec",
      "name": "gujaratdocument",
      "flowData": "{\n  \"nodes\": [\n    {\n      \"id\": \"openAIEmbeddings_0\",\n      \"position\": {\n        \"x\": 220.09251986430831,\n        \"y\": 300.142353814901\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"openAIEmbeddings_0\",\n        \"label\": \"OpenAI Embeddings\",\n        \"version\": 4,\n        \"name\": \"openAIEmbeddings\",\n        \"type\": \"OpenAIEmbeddings\",\n        \"baseClasses\": [\n          \"OpenAIEmbeddings\",\n          \"Embeddings\"\n        ],\n        \"category\": \"Embeddings\",\n        \"description\": \"OpenAI API to generate embeddings for a given text\",\n        \"inputParams\": [\n          {\n            \"label\": \"Connect Credential\",\n            \"name\": \"credential\",\n            \"type\": \"credential\",\n            \"credentialNames\": [\n              \"openAIApi\"\n            ],\n            \"id\": \"openAIEmbeddings_0-input-credential-credential\"\n          },\n          {\n            \"label\": \"Model Name\",\n            \"name\": \"modelName\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"default\": \"text-embedding-ada-002\",\n            \"id\": \"openAIEmbeddings_0-input-modelName-asyncOptions\"\n          },\n          {\n            \"label\": \"Strip New Lines\",\n            \"name\": \"stripNewLines\",\n            \"type\": \"boolean\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"openAIEmbeddings_0-input-stripNewLines-boolean\"\n          },\n          {\n            \"label\": \"Batch Size\",\n            \"name\": \"batchSize\",\n            \"type\": \"number\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"openAIEmbeddings_0-input-batchSize-number\"\n          },\n          {\n            \"label\": \"Timeout\",\n            \"name\": \"timeout\",\n            \"type\": \"number\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"openAIEmbeddings_0-input-timeout-number\"\n          },\n          {\n            \"label\": \"BasePath\",\n            \"name\": \"basepath\",\n            \"type\": \"string\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"openAIEmbeddings_0-input-basepath-string\"\n          },\n          {\n            \"label\": \"Dimensions\",\n            \"name\": \"dimensions\",\n            \"type\": \"number\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"openAIEmbeddings_0-input-dimensions-number\"\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"modelName\": \"text-embedding-3-large\",\n          \"stripNewLines\": \"\",\n          \"batchSize\": \"\",\n          \"timeout\": \"\",\n          \"basepath\": \"\",\n          \"dimensions\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"openAIEmbeddings_0-output-openAIEmbeddings-OpenAIEmbeddings|Embeddings\",\n            \"name\": \"openAIEmbeddings\",\n            \"label\": \"OpenAIEmbeddings\",\n            \"description\": \"OpenAI API to generate embeddings for a given text\",\n            \"type\": \"OpenAIEmbeddings | Embeddings\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 423,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 220.09251986430831,\n        \"y\": 300.142353814901\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"pdfFile_0\",\n      \"position\": {\n        \"x\": 579.3736472228114,\n        \"y\": -1142.8256067185089\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"pdfFile_0\",\n        \"label\": \"Pdf File\",\n        \"version\": 1,\n        \"name\": \"pdfFile\",\n        \"type\": \"Document\",\n        \"baseClasses\": [\n          \"Document\"\n        ],\n        \"category\": \"Document Loaders\",\n        \"description\": \"Load data from PDF files\",\n        \"inputParams\": [\n          {\n            \"label\": \"Pdf File\",\n            \"name\": \"pdfFile\",\n            \"type\": \"file\",\n            \"fileType\": \".pdf\",\n            \"id\": \"pdfFile_0-input-pdfFile-file\"\n          },\n          {\n            \"label\": \"Usage\",\n            \"name\": \"usage\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"One document per page\",\n                \"name\": \"perPage\"\n              },\n              {\n                \"label\": \"One document per file\",\n                \"name\": \"perFile\"\n              }\n            ],\n            \"default\": \"perPage\",\n            \"id\": \"pdfFile_0-input-usage-options\"\n          },\n          {\n            \"label\": \"Use Legacy Build\",\n            \"name\": \"legacyBuild\",\n            \"type\": \"boolean\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"pdfFile_0-input-legacyBuild-boolean\"\n          },\n          {\n            \"label\": \"Additional Metadata\",\n            \"name\": \"metadata\",\n            \"type\": \"json\",\n            \"description\": \"Additional metadata to be added to the extracted documents\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"pdfFile_0-input-metadata-json\"\n          },\n          {\n            \"label\": \"Omit Metadata Keys\",\n            \"name\": \"omitMetadataKeys\",\n            \"type\": \"string\",\n            \"rows\": 4,\n            \"description\": \"Each document loader comes with a default set of metadata keys that are extracted from the document. You can use this field to omit some of the default metadata keys. The value should be a list of keys, seperated by comma. Use * to omit all metadata keys execept the ones you specify in the Additional Metadata field\",\n            \"placeholder\": \"key1, key2, key3.nestedKey1\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"pdfFile_0-input-omitMetadataKeys-string\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Text Splitter\",\n            \"name\": \"textSplitter\",\n            \"type\": \"TextSplitter\",\n            \"optional\": true,\n            \"id\": \"pdfFile_0-input-textSplitter-TextSplitter\"\n          }\n        ],\n        \"inputs\": {\n          \"textSplitter\": \"\",\n          \"usage\": \"perFile\",\n          \"legacyBuild\": \"\",\n          \"metadata\": \"\",\n          \"omitMetadataKeys\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"pdfFile_0-output-pdfFile-Document\",\n            \"name\": \"pdfFile\",\n            \"label\": \"Document\",\n            \"description\": \"Load data from PDF files\",\n            \"type\": \"Document\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 508,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 579.3736472228114,\n        \"y\": -1142.8256067185089\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"tokenTextSplitter_0\",\n      \"position\": {\n        \"x\": 278.11347700836836,\n        \"y\": -235.12078789520618\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"tokenTextSplitter_0\",\n        \"label\": \"Token Text Splitter\",\n        \"version\": 1,\n        \"name\": \"tokenTextSplitter\",\n        \"type\": \"TokenTextSplitter\",\n        \"baseClasses\": [\n          \"TokenTextSplitter\",\n          \"TextSplitter\",\n          \"BaseDocumentTransformer\",\n          \"Runnable\"\n        ],\n        \"category\": \"Text Splitters\",\n        \"description\": \"Splits a raw text string by first converting the text into BPE tokens, then split these tokens into chunks and convert the tokens within a single chunk back into text.\",\n        \"inputParams\": [\n          {\n            \"label\": \"Encoding Name\",\n            \"name\": \"encodingName\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"gpt2\",\n                \"name\": \"gpt2\"\n              },\n              {\n                \"label\": \"r50k_base\",\n                \"name\": \"r50k_base\"\n              },\n              {\n                \"label\": \"p50k_base\",\n                \"name\": \"p50k_base\"\n              },\n              {\n                \"label\": \"p50k_edit\",\n                \"name\": \"p50k_edit\"\n              },\n              {\n                \"label\": \"cl100k_base\",\n                \"name\": \"cl100k_base\"\n              }\n            ],\n            \"default\": \"gpt2\",\n            \"id\": \"tokenTextSplitter_0-input-encodingName-options\"\n          },\n          {\n            \"label\": \"Chunk Size\",\n            \"name\": \"chunkSize\",\n            \"type\": \"number\",\n            \"description\": \"Number of characters in each chunk. Default is 1000.\",\n            \"default\": 1000,\n            \"optional\": true,\n            \"id\": \"tokenTextSplitter_0-input-chunkSize-number\"\n          },\n          {\n            \"label\": \"Chunk Overlap\",\n            \"name\": \"chunkOverlap\",\n            \"type\": \"number\",\n            \"description\": \"Number of characters to overlap between chunks. Default is 200.\",\n            \"default\": 200,\n            \"optional\": true,\n            \"id\": \"tokenTextSplitter_0-input-chunkOverlap-number\"\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"encodingName\": \"r50k_base\",\n          \"chunkSize\": \"150\",\n          \"chunkOverlap\": \"50\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"tokenTextSplitter_0-output-tokenTextSplitter-TokenTextSplitter|TextSplitter|BaseDocumentTransformer|Runnable\",\n            \"name\": \"tokenTextSplitter\",\n            \"label\": \"TokenTextSplitter\",\n            \"description\": \"Splits a raw text string by first converting the text into BPE tokens, then split these tokens into chunks and convert the tokens within a single chunk back into text.\",\n            \"type\": \"TokenTextSplitter | TextSplitter | BaseDocumentTransformer | Runnable\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 473,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 278.11347700836836,\n        \"y\": -235.12078789520618\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"conversationalRetrievalQAChain_0\",\n      \"position\": {\n        \"x\": 1763.6870752061925,\n        \"y\": -172.03873613715547\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"conversationalRetrievalQAChain_0\",\n        \"label\": \"Conversational Retrieval QA Chain\",\n        \"version\": 3,\n        \"name\": \"conversationalRetrievalQAChain\",\n        \"type\": \"ConversationalRetrievalQAChain\",\n        \"baseClasses\": [\n          \"ConversationalRetrievalQAChain\",\n          \"BaseChain\",\n          \"Runnable\"\n        ],\n        \"category\": \"Chains\",\n        \"description\": \"Document QA - built on RetrievalQAChain to provide a chat history component\",\n        \"inputParams\": [\n          {\n            \"label\": \"Return Source Documents\",\n            \"name\": \"returnSourceDocuments\",\n            \"type\": \"boolean\",\n            \"optional\": true,\n            \"id\": \"conversationalRetrievalQAChain_0-input-returnSourceDocuments-boolean\"\n          },\n          {\n            \"label\": \"Rephrase Prompt\",\n            \"name\": \"rephrasePrompt\",\n            \"type\": \"string\",\n            \"description\": \"Using previous chat history, rephrase question into a standalone question\",\n            \"warning\": \"Prompt must include input variables: {chat_history} and {question}\",\n            \"rows\": 4,\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"default\": \"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone Question:\",\n            \"id\": \"conversationalRetrievalQAChain_0-input-rephrasePrompt-string\"\n          },\n          {\n            \"label\": \"Response Prompt\",\n            \"name\": \"responsePrompt\",\n            \"type\": \"string\",\n            \"description\": \"Taking the rephrased question, search for answer from the provided context\",\n            \"warning\": \"Prompt must include input variable: {context}\",\n            \"rows\": 4,\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"default\": \"I want you to act as a document that I am having a conversation with. Your name is \\\"AI Assistant\\\". Using the provided context, answer the user's question to the best of your ability using the resources provided.\\nIf there is nothing in the context relevant to the question at hand, just say \\\"Hmm, I'm not sure\\\" and stop after that. Refuse to answer any question not about the info. Never break character.\\n------------\\n{context}\\n------------\\nREMEMBER: If there is no relevant information within the context, just say \\\"Hmm, I'm not sure\\\". Don't try to make up an answer. Never break character.\",\n            \"id\": \"conversationalRetrievalQAChain_0-input-responsePrompt-string\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Chat Model\",\n            \"name\": \"model\",\n            \"type\": \"BaseChatModel\",\n            \"id\": \"conversationalRetrievalQAChain_0-input-model-BaseChatModel\"\n          },\n          {\n            \"label\": \"Vector Store Retriever\",\n            \"name\": \"vectorStoreRetriever\",\n            \"type\": \"BaseRetriever\",\n            \"id\": \"conversationalRetrievalQAChain_0-input-vectorStoreRetriever-BaseRetriever\"\n          },\n          {\n            \"label\": \"Memory\",\n            \"name\": \"memory\",\n            \"type\": \"BaseMemory\",\n            \"optional\": true,\n            \"description\": \"If left empty, a default BufferMemory will be used\",\n            \"id\": \"conversationalRetrievalQAChain_0-input-memory-BaseMemory\"\n          },\n          {\n            \"label\": \"Input Moderation\",\n            \"description\": \"Detect text that could generate harmful output and prevent it from being sent to the language model\",\n            \"name\": \"inputModeration\",\n            \"type\": \"Moderation\",\n            \"optional\": true,\n            \"list\": true,\n            \"id\": \"conversationalRetrievalQAChain_0-input-inputModeration-Moderation\"\n          }\n        ],\n        \"inputs\": {\n          \"model\": \"{{chatGoogleGenerativeAI_0.data.instance}}\",\n          \"vectorStoreRetriever\": \"{{pinecone_0.data.instance}}\",\n          \"memory\": \"{{bufferMemory_0.data.instance}}\",\n          \"returnSourceDocuments\": true,\n          \"rephrasePrompt\": \"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone Question:\",\n          \"responsePrompt\": \"I want you to act as a document that I am having a conversation with. Your name is \\\"AI Assistant\\\". Using the provided context, answer the user's question to the best of your ability using the resources provided.\\nIf there is nothing in the context relevant to the question at hand, just say \\\"Hmm, I'm not sure\\\" and stop after that. Refuse to answer any question not about the info. Never break character.\\n------------\\n{context}\\n------------\\nREMEMBER: If there is no relevant information within the context, just say \\\"Hmm, I'm not sure\\\". Don't try to make up an answer. Never break character.\",\n          \"inputModeration\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"conversationalRetrievalQAChain_0-output-conversationalRetrievalQAChain-ConversationalRetrievalQAChain|BaseChain|Runnable\",\n            \"name\": \"conversationalRetrievalQAChain\",\n            \"label\": \"ConversationalRetrievalQAChain\",\n            \"description\": \"Document QA - built on RetrievalQAChain to provide a chat history component\",\n            \"type\": \"ConversationalRetrievalQAChain | BaseChain | Runnable\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 531,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 1763.6870752061925,\n        \"y\": -172.03873613715547\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"chatOpenAI_0\",\n      \"position\": {\n        \"x\": 201.13846186610823,\n        \"y\": -1060.1404015828052\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"chatOpenAI_0\",\n        \"label\": \"ChatOpenAI\",\n        \"version\": 7,\n        \"name\": \"chatOpenAI\",\n        \"type\": \"ChatOpenAI\",\n        \"baseClasses\": [\n          \"ChatOpenAI\",\n          \"BaseChatModel\",\n          \"BaseLanguageModel\",\n          \"Runnable\"\n        ],\n        \"category\": \"Chat Models\",\n        \"description\": \"Wrapper around OpenAI large language models that use the Chat endpoint\",\n        \"inputParams\": [\n          {\n            \"label\": \"Connect Credential\",\n            \"name\": \"credential\",\n            \"type\": \"credential\",\n            \"credentialNames\": [\n              \"openAIApi\"\n            ],\n            \"id\": \"chatOpenAI_0-input-credential-credential\"\n          },\n          {\n            \"label\": \"Model Name\",\n            \"name\": \"modelName\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"default\": \"gpt-3.5-turbo\",\n            \"id\": \"chatOpenAI_0-input-modelName-asyncOptions\"\n          },\n          {\n            \"label\": \"Temperature\",\n            \"name\": \"temperature\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"default\": 0.9,\n            \"optional\": true,\n            \"id\": \"chatOpenAI_0-input-temperature-number\"\n          },\n          {\n            \"label\": \"Max Tokens\",\n            \"name\": \"maxTokens\",\n            \"type\": \"number\",\n            \"step\": 1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_0-input-maxTokens-number\"\n          },\n          {\n            \"label\": \"Top Probability\",\n            \"name\": \"topP\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_0-input-topP-number\"\n          },\n          {\n            \"label\": \"Frequency Penalty\",\n            \"name\": \"frequencyPenalty\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_0-input-frequencyPenalty-number\"\n          },\n          {\n            \"label\": \"Presence Penalty\",\n            \"name\": \"presencePenalty\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_0-input-presencePenalty-number\"\n          },\n          {\n            \"label\": \"Timeout\",\n            \"name\": \"timeout\",\n            \"type\": \"number\",\n            \"step\": 1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_0-input-timeout-number\"\n          },\n          {\n            \"label\": \"BasePath\",\n            \"name\": \"basepath\",\n            \"type\": \"string\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_0-input-basepath-string\"\n          },\n          {\n            \"label\": \"Proxy Url\",\n            \"name\": \"proxyUrl\",\n            \"type\": \"string\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_0-input-proxyUrl-string\"\n          },\n          {\n            \"label\": \"Stop Sequence\",\n            \"name\": \"stopSequence\",\n            \"type\": \"string\",\n            \"rows\": 4,\n            \"optional\": true,\n            \"description\": \"List of stop words to use when generating. Use comma to separate multiple stop words.\",\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_0-input-stopSequence-string\"\n          },\n          {\n            \"label\": \"BaseOptions\",\n            \"name\": \"baseOptions\",\n            \"type\": \"json\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_0-input-baseOptions-json\"\n          },\n          {\n            \"label\": \"Allow Image Uploads\",\n            \"name\": \"allowImageUploads\",\n            \"type\": \"boolean\",\n            \"description\": \"Automatically uses gpt-4-vision-preview when image is being uploaded from chat. Only works with LLMChain, Conversation Chain, ReAct Agent, Conversational Agent, Tool Agent\",\n            \"default\": false,\n            \"optional\": true,\n            \"id\": \"chatOpenAI_0-input-allowImageUploads-boolean\"\n          },\n          {\n            \"label\": \"Image Resolution\",\n            \"description\": \"This parameter controls the resolution in which the model views the image.\",\n            \"name\": \"imageResolution\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"Low\",\n                \"name\": \"low\"\n              },\n              {\n                \"label\": \"High\",\n                \"name\": \"high\"\n              },\n              {\n                \"label\": \"Auto\",\n                \"name\": \"auto\"\n              }\n            ],\n            \"default\": \"low\",\n            \"optional\": false,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_0-input-imageResolution-options\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Cache\",\n            \"name\": \"cache\",\n            \"type\": \"BaseCache\",\n            \"optional\": true,\n            \"id\": \"chatOpenAI_0-input-cache-BaseCache\"\n          }\n        ],\n        \"inputs\": {\n          \"cache\": \"\",\n          \"modelName\": \"gpt-4-turbo\",\n          \"temperature\": \"0.7\",\n          \"maxTokens\": \"\",\n          \"topP\": \"\",\n          \"frequencyPenalty\": \"\",\n          \"presencePenalty\": \"\",\n          \"timeout\": \"\",\n          \"basepath\": \"\",\n          \"proxyUrl\": \"\",\n          \"stopSequence\": \"\",\n          \"baseOptions\": \"\",\n          \"allowImageUploads\": false,\n          \"imageResolution\": \"low\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"chatOpenAI_0-output-chatOpenAI-ChatOpenAI|BaseChatModel|BaseLanguageModel|Runnable\",\n            \"name\": \"chatOpenAI\",\n            \"label\": \"ChatOpenAI\",\n            \"description\": \"Wrapper around OpenAI large language models that use the Chat endpoint\",\n            \"type\": \"ChatOpenAI | BaseChatModel | BaseLanguageModel | Runnable\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 669,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 201.13846186610823,\n        \"y\": -1060.1404015828052\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"pinecone_0\",\n      \"position\": {\n        \"x\": 1222.1946388898364,\n        \"y\": -65.39062703674432\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"pinecone_0\",\n        \"label\": \"Pinecone\",\n        \"version\": 5,\n        \"name\": \"pinecone\",\n        \"type\": \"Pinecone\",\n        \"baseClasses\": [\n          \"Pinecone\",\n          \"VectorStoreRetriever\",\n          \"BaseRetriever\"\n        ],\n        \"category\": \"Vector Stores\",\n        \"description\": \"Upsert embedded data and perform similarity or mmr search using Pinecone, a leading fully managed hosted vector database\",\n        \"inputParams\": [\n          {\n            \"label\": \"Connect Credential\",\n            \"name\": \"credential\",\n            \"type\": \"credential\",\n            \"credentialNames\": [\n              \"pineconeApi\"\n            ],\n            \"id\": \"pinecone_0-input-credential-credential\"\n          },\n          {\n            \"label\": \"Pinecone Index\",\n            \"name\": \"pineconeIndex\",\n            \"type\": \"string\",\n            \"id\": \"pinecone_0-input-pineconeIndex-string\"\n          },\n          {\n            \"label\": \"Pinecone Namespace\",\n            \"name\": \"pineconeNamespace\",\n            \"type\": \"string\",\n            \"placeholder\": \"my-first-namespace\",\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"id\": \"pinecone_0-input-pineconeNamespace-string\"\n          },\n          {\n            \"label\": \"File Upload\",\n            \"name\": \"fileUpload\",\n            \"description\": \"Allow file upload on the chat\",\n            \"hint\": {\n              \"label\": \"How to use\",\n              \"value\": \"\\n**File Upload**\\n\\nThis allows file upload on the chat. Uploaded files will be upserted on the fly to the vector store.\\n\\n**Note:**\\n- You can only turn on file upload for one vector store at a time.\\n- At least one Document Loader node should be connected to the document input.\\n- Document Loader should be file types like PDF, DOCX, TXT, etc.\\n\\n**How it works**\\n- Uploaded files will have the metadata updated with the chatId.\\n- This will allow the file to be associated with the chatId.\\n- When querying, metadata will be filtered by chatId to retrieve files associated with the chatId.\\n\"\n            },\n            \"type\": \"boolean\",\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"id\": \"pinecone_0-input-fileUpload-boolean\"\n          },\n          {\n            \"label\": \"Pinecone Text Key\",\n            \"name\": \"pineconeTextKey\",\n            \"description\": \"The key in the metadata for storing text. Default to `text`\",\n            \"type\": \"string\",\n            \"placeholder\": \"text\",\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"id\": \"pinecone_0-input-pineconeTextKey-string\"\n          },\n          {\n            \"label\": \"Pinecone Metadata Filter\",\n            \"name\": \"pineconeMetadataFilter\",\n            \"type\": \"json\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"pinecone_0-input-pineconeMetadataFilter-json\"\n          },\n          {\n            \"label\": \"Top K\",\n            \"name\": \"topK\",\n            \"description\": \"Number of top results to fetch. Default to 4\",\n            \"placeholder\": \"4\",\n            \"type\": \"number\",\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"id\": \"pinecone_0-input-topK-number\"\n          },\n          {\n            \"label\": \"Search Type\",\n            \"name\": \"searchType\",\n            \"type\": \"options\",\n            \"default\": \"similarity\",\n            \"options\": [\n              {\n                \"label\": \"Similarity\",\n                \"name\": \"similarity\"\n              },\n              {\n                \"label\": \"Max Marginal Relevance\",\n                \"name\": \"mmr\"\n              }\n            ],\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"id\": \"pinecone_0-input-searchType-options\"\n          },\n          {\n            \"label\": \"Fetch K (for MMR Search)\",\n            \"name\": \"fetchK\",\n            \"description\": \"Number of initial documents to fetch for MMR reranking. Default to 20. Used only when the search type is MMR\",\n            \"placeholder\": \"20\",\n            \"type\": \"number\",\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"id\": \"pinecone_0-input-fetchK-number\"\n          },\n          {\n            \"label\": \"Lambda (for MMR Search)\",\n            \"name\": \"lambda\",\n            \"description\": \"Number between 0 and 1 that determines the degree of diversity among the results, where 0 corresponds to maximum diversity and 1 to minimum diversity. Used only when the search type is MMR\",\n            \"placeholder\": \"0.5\",\n            \"type\": \"number\",\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"id\": \"pinecone_0-input-lambda-number\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Document\",\n            \"name\": \"document\",\n            \"type\": \"Document\",\n            \"list\": true,\n            \"optional\": true,\n            \"id\": \"pinecone_0-input-document-Document\"\n          },\n          {\n            \"label\": \"Embeddings\",\n            \"name\": \"embeddings\",\n            \"type\": \"Embeddings\",\n            \"id\": \"pinecone_0-input-embeddings-Embeddings\"\n          },\n          {\n            \"label\": \"Record Manager\",\n            \"name\": \"recordManager\",\n            \"type\": \"RecordManager\",\n            \"description\": \"Keep track of the record to prevent duplication\",\n            \"optional\": true,\n            \"id\": \"pinecone_0-input-recordManager-RecordManager\"\n          }\n        ],\n        \"inputs\": {\n          \"document\": [\n            \"{{docxFile_0.data.instance}}\"\n          ],\n          \"embeddings\": \"{{googleGenerativeAiEmbeddings_0.data.instance}}\",\n          \"recordManager\": \"\",\n          \"pineconeIndex\": \"portfolioless\",\n          \"pineconeNamespace\": \"\",\n          \"fileUpload\": \"\",\n          \"pineconeTextKey\": \"\",\n          \"pineconeMetadataFilter\": \"\",\n          \"topK\": \"\",\n          \"searchType\": \"similarity\",\n          \"fetchK\": \"\",\n          \"lambda\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"name\": \"output\",\n            \"label\": \"Output\",\n            \"type\": \"options\",\n            \"description\": \"\",\n            \"options\": [\n              {\n                \"id\": \"pinecone_0-output-retriever-Pinecone|VectorStoreRetriever|BaseRetriever\",\n                \"name\": \"retriever\",\n                \"label\": \"Pinecone Retriever\",\n                \"description\": \"\",\n                \"type\": \"Pinecone | VectorStoreRetriever | BaseRetriever\"\n              },\n              {\n                \"id\": \"pinecone_0-output-vectorStore-Pinecone|VectorStore\",\n                \"name\": \"vectorStore\",\n                \"label\": \"Pinecone Vector Store\",\n                \"description\": \"\",\n                \"type\": \"Pinecone | VectorStore\"\n              }\n            ],\n            \"default\": \"retriever\"\n          }\n        ],\n        \"outputs\": {\n          \"output\": \"vectorStore\"\n        },\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 605,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 1222.1946388898364,\n        \"y\": -65.39062703674432\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"bufferMemory_0\",\n      \"position\": {\n        \"x\": 1671.2383101055193,\n        \"y\": -533.7037857158773\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"bufferMemory_0\",\n        \"label\": \"Buffer Memory\",\n        \"version\": 2,\n        \"name\": \"bufferMemory\",\n        \"type\": \"BufferMemory\",\n        \"baseClasses\": [\n          \"BufferMemory\",\n          \"BaseChatMemory\",\n          \"BaseMemory\"\n        ],\n        \"category\": \"Memory\",\n        \"description\": \"Retrieve chat messages stored in database\",\n        \"inputParams\": [\n          {\n            \"label\": \"Session Id\",\n            \"name\": \"sessionId\",\n            \"type\": \"string\",\n            \"description\": \"If not specified, a random id will be used. Learn <a target=\\\"_blank\\\" href=\\\"https://docs.flowiseai.com/memory#ui-and-embedded-chat\\\">more</a>\",\n            \"default\": \"\",\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"id\": \"bufferMemory_0-input-sessionId-string\"\n          },\n          {\n            \"label\": \"Memory Key\",\n            \"name\": \"memoryKey\",\n            \"type\": \"string\",\n            \"default\": \"chat_history\",\n            \"additionalParams\": true,\n            \"id\": \"bufferMemory_0-input-memoryKey-string\"\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"sessionId\": \"\",\n          \"memoryKey\": \"chat_history\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"bufferMemory_0-output-bufferMemory-BufferMemory|BaseChatMemory|BaseMemory\",\n            \"name\": \"bufferMemory\",\n            \"label\": \"BufferMemory\",\n            \"description\": \"Retrieve chat messages stored in database\",\n            \"type\": \"BufferMemory | BaseChatMemory | BaseMemory\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 252,\n      \"positionAbsolute\": {\n        \"x\": 1671.2383101055193,\n        \"y\": -533.7037857158773\n      },\n      \"selected\": false\n    },\n    {\n      \"id\": \"googleGenerativeAiEmbeddings_0\",\n      \"position\": {\n        \"x\": 784.7383305359807,\n        \"y\": -103.22286945362391\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"googleGenerativeAiEmbeddings_0\",\n        \"label\": \"GoogleGenerativeAI Embeddings\",\n        \"version\": 2,\n        \"name\": \"googleGenerativeAiEmbeddings\",\n        \"type\": \"GoogleGenerativeAiEmbeddings\",\n        \"baseClasses\": [\n          \"GoogleGenerativeAiEmbeddings\",\n          \"Embeddings\"\n        ],\n        \"category\": \"Embeddings\",\n        \"description\": \"Google Generative API to generate embeddings for a given text\",\n        \"inputParams\": [\n          {\n            \"label\": \"Connect Credential\",\n            \"name\": \"credential\",\n            \"type\": \"credential\",\n            \"credentialNames\": [\n              \"googleGenerativeAI\"\n            ],\n            \"optional\": false,\n            \"description\": \"Google Generative AI credential.\",\n            \"id\": \"googleGenerativeAiEmbeddings_0-input-credential-credential\"\n          },\n          {\n            \"label\": \"Model Name\",\n            \"name\": \"modelName\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"default\": \"embedding-001\",\n            \"id\": \"googleGenerativeAiEmbeddings_0-input-modelName-asyncOptions\"\n          },\n          {\n            \"label\": \"Task Type\",\n            \"name\": \"tasktype\",\n            \"type\": \"options\",\n            \"description\": \"Type of task for which the embedding will be used\",\n            \"options\": [\n              {\n                \"label\": \"TASK_TYPE_UNSPECIFIED\",\n                \"name\": \"TASK_TYPE_UNSPECIFIED\"\n              },\n              {\n                \"label\": \"RETRIEVAL_QUERY\",\n                \"name\": \"RETRIEVAL_QUERY\"\n              },\n              {\n                \"label\": \"RETRIEVAL_DOCUMENT\",\n                \"name\": \"RETRIEVAL_DOCUMENT\"\n              },\n              {\n                \"label\": \"SEMANTIC_SIMILARITY\",\n                \"name\": \"SEMANTIC_SIMILARITY\"\n              },\n              {\n                \"label\": \"CLASSIFICATION\",\n                \"name\": \"CLASSIFICATION\"\n              },\n              {\n                \"label\": \"CLUSTERING\",\n                \"name\": \"CLUSTERING\"\n              }\n            ],\n            \"default\": \"TASK_TYPE_UNSPECIFIED\",\n            \"id\": \"googleGenerativeAiEmbeddings_0-input-tasktype-options\"\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"modelName\": \"embedding-001\",\n          \"tasktype\": \"TASK_TYPE_UNSPECIFIED\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"googleGenerativeAiEmbeddings_0-output-googleGenerativeAiEmbeddings-GoogleGenerativeAiEmbeddings|Embeddings\",\n            \"name\": \"googleGenerativeAiEmbeddings\",\n            \"label\": \"GoogleGenerativeAiEmbeddings\",\n            \"description\": \"Google Generative API to generate embeddings for a given text\",\n            \"type\": \"GoogleGenerativeAiEmbeddings | Embeddings\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 467,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 784.7383305359807,\n        \"y\": -103.22286945362391\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"chatGoogleGenerativeAI_0\",\n      \"position\": {\n        \"x\": 1289.8475376360727,\n        \"y\": -840.2962410882164\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"chatGoogleGenerativeAI_0\",\n        \"label\": \"ChatGoogleGenerativeAI\",\n        \"version\": 3,\n        \"name\": \"chatGoogleGenerativeAI\",\n        \"type\": \"ChatGoogleGenerativeAI\",\n        \"baseClasses\": [\n          \"ChatGoogleGenerativeAI\",\n          \"LangchainChatGoogleGenerativeAI\",\n          \"BaseChatModel\",\n          \"BaseLanguageModel\",\n          \"Runnable\"\n        ],\n        \"category\": \"Chat Models\",\n        \"description\": \"Wrapper around Google Gemini large language models that use the Chat endpoint\",\n        \"inputParams\": [\n          {\n            \"label\": \"Connect Credential\",\n            \"name\": \"credential\",\n            \"type\": \"credential\",\n            \"credentialNames\": [\n              \"googleGenerativeAI\"\n            ],\n            \"optional\": false,\n            \"description\": \"Google Generative AI credential.\",\n            \"id\": \"chatGoogleGenerativeAI_0-input-credential-credential\"\n          },\n          {\n            \"label\": \"Model Name\",\n            \"name\": \"modelName\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"default\": \"gemini-1.5-flash-latest\",\n            \"id\": \"chatGoogleGenerativeAI_0-input-modelName-asyncOptions\"\n          },\n          {\n            \"label\": \"Custom Model Name\",\n            \"name\": \"customModelName\",\n            \"type\": \"string\",\n            \"placeholder\": \"gemini-1.5-pro-exp-0801\",\n            \"description\": \"Custom model name to use. If provided, it will override the model selected\",\n            \"additionalParams\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-customModelName-string\"\n          },\n          {\n            \"label\": \"Temperature\",\n            \"name\": \"temperature\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"default\": 0.9,\n            \"optional\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-temperature-number\"\n          },\n          {\n            \"label\": \"Streaming\",\n            \"name\": \"streaming\",\n            \"type\": \"boolean\",\n            \"default\": true,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-streaming-boolean\"\n          },\n          {\n            \"label\": \"Max Output Tokens\",\n            \"name\": \"maxOutputTokens\",\n            \"type\": \"number\",\n            \"step\": 1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-maxOutputTokens-number\"\n          },\n          {\n            \"label\": \"Top Probability\",\n            \"name\": \"topP\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-topP-number\"\n          },\n          {\n            \"label\": \"Top Next Highest Probability Tokens\",\n            \"name\": \"topK\",\n            \"type\": \"number\",\n            \"description\": \"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive\",\n            \"step\": 1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-topK-number\"\n          },\n          {\n            \"label\": \"Harm Category\",\n            \"name\": \"harmCategory\",\n            \"type\": \"multiOptions\",\n            \"description\": \"Refer to <a target=\\\"_blank\\\" href=\\\"https://cloud.google.com/vertex-ai/docs/generative-ai/multimodal/configure-safety-attributes#safety_attribute_definitions\\\">official guide</a> on how to use Harm Category\",\n            \"options\": [\n              {\n                \"label\": \"Dangerous\",\n                \"name\": \"HARM_CATEGORY_DANGEROUS_CONTENT\"\n              },\n              {\n                \"label\": \"Harassment\",\n                \"name\": \"HARM_CATEGORY_HARASSMENT\"\n              },\n              {\n                \"label\": \"Hate Speech\",\n                \"name\": \"HARM_CATEGORY_HATE_SPEECH\"\n              },\n              {\n                \"label\": \"Sexually Explicit\",\n                \"name\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\"\n              }\n            ],\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-harmCategory-multiOptions\"\n          },\n          {\n            \"label\": \"Harm Block Threshold\",\n            \"name\": \"harmBlockThreshold\",\n            \"type\": \"multiOptions\",\n            \"description\": \"Refer to <a target=\\\"_blank\\\" href=\\\"https://cloud.google.com/vertex-ai/docs/generative-ai/multimodal/configure-safety-attributes#safety_setting_thresholds\\\">official guide</a> on how to use Harm Block Threshold\",\n            \"options\": [\n              {\n                \"label\": \"Low and Above\",\n                \"name\": \"BLOCK_LOW_AND_ABOVE\"\n              },\n              {\n                \"label\": \"Medium and Above\",\n                \"name\": \"BLOCK_MEDIUM_AND_ABOVE\"\n              },\n              {\n                \"label\": \"None\",\n                \"name\": \"BLOCK_NONE\"\n              },\n              {\n                \"label\": \"Only High\",\n                \"name\": \"BLOCK_ONLY_HIGH\"\n              },\n              {\n                \"label\": \"Threshold Unspecified\",\n                \"name\": \"HARM_BLOCK_THRESHOLD_UNSPECIFIED\"\n              }\n            ],\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-harmBlockThreshold-multiOptions\"\n          },\n          {\n            \"label\": \"Allow Image Uploads\",\n            \"name\": \"allowImageUploads\",\n            \"type\": \"boolean\",\n            \"description\": \"Allow image input. Refer to the <a href=\\\"https://docs.flowiseai.com/using-flowise/uploads#image\\\" target=\\\"_blank\\\">docs</a> for more details.\",\n            \"default\": false,\n            \"optional\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-allowImageUploads-boolean\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Cache\",\n            \"name\": \"cache\",\n            \"type\": \"BaseCache\",\n            \"optional\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-cache-BaseCache\"\n          }\n        ],\n        \"inputs\": {\n          \"cache\": \"\",\n          \"modelName\": \"gemini-2.0-flash-001\",\n          \"customModelName\": \"\",\n          \"temperature\": \"0.7\",\n          \"streaming\": true,\n          \"maxOutputTokens\": \"\",\n          \"topP\": \"\",\n          \"topK\": \"\",\n          \"harmCategory\": \"\",\n          \"harmBlockThreshold\": \"\",\n          \"allowImageUploads\": true\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"chatGoogleGenerativeAI_0-output-chatGoogleGenerativeAI-ChatGoogleGenerativeAI|LangchainChatGoogleGenerativeAI|BaseChatModel|BaseLanguageModel|Runnable\",\n            \"name\": \"chatGoogleGenerativeAI\",\n            \"label\": \"ChatGoogleGenerativeAI\",\n            \"description\": \"Wrapper around Google Gemini large language models that use the Chat endpoint\",\n            \"type\": \"ChatGoogleGenerativeAI | LangchainChatGoogleGenerativeAI | BaseChatModel | BaseLanguageModel | Runnable\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 669,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 1289.8475376360727,\n        \"y\": -840.2962410882164\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"docxFile_0\",\n      \"position\": {\n        \"x\": 738.2892369083152,\n        \"y\": -596.450299179969\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"docxFile_0\",\n        \"label\": \"Docx File\",\n        \"version\": 2,\n        \"name\": \"docxFile\",\n        \"type\": \"Document\",\n        \"baseClasses\": [\n          \"Document\"\n        ],\n        \"category\": \"Document Loaders\",\n        \"description\": \"Load data from DOCX files\",\n        \"inputParams\": [\n          {\n            \"label\": \"Docx File\",\n            \"name\": \"docxFile\",\n            \"type\": \"file\",\n            \"fileType\": \".docx\",\n            \"id\": \"docxFile_0-input-docxFile-file\"\n          },\n          {\n            \"label\": \"Additional Metadata\",\n            \"name\": \"metadata\",\n            \"type\": \"json\",\n            \"description\": \"Additional metadata to be added to the extracted documents\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"docxFile_0-input-metadata-json\"\n          },\n          {\n            \"label\": \"Omit Metadata Keys\",\n            \"name\": \"omitMetadataKeys\",\n            \"type\": \"string\",\n            \"rows\": 4,\n            \"description\": \"Each document loader comes with a default set of metadata keys that are extracted from the document. You can use this field to omit some of the default metadata keys. The value should be a list of keys, seperated by comma. Use * to omit all metadata keys execept the ones you specify in the Additional Metadata field\",\n            \"placeholder\": \"key1, key2, key3.nestedKey1\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"docxFile_0-input-omitMetadataKeys-string\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Text Splitter\",\n            \"name\": \"textSplitter\",\n            \"type\": \"TextSplitter\",\n            \"optional\": true,\n            \"id\": \"docxFile_0-input-textSplitter-TextSplitter\"\n          }\n        ],\n        \"inputs\": {\n          \"textSplitter\": \"{{tokenTextSplitter_0.data.instance}}\",\n          \"metadata\": \"\",\n          \"omitMetadataKeys\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"name\": \"output\",\n            \"label\": \"Output\",\n            \"type\": \"options\",\n            \"description\": \"Array of document objects containing metadata and pageContent\",\n            \"options\": [\n              {\n                \"id\": \"docxFile_0-output-document-Document|json\",\n                \"name\": \"document\",\n                \"label\": \"Document\",\n                \"description\": \"Array of document objects containing metadata and pageContent\",\n                \"type\": \"Document | json\"\n              },\n              {\n                \"id\": \"docxFile_0-output-text-string|json\",\n                \"name\": \"text\",\n                \"label\": \"Text\",\n                \"description\": \"Concatenated string from pageContent of documents\",\n                \"type\": \"string | json\"\n              }\n            ],\n            \"default\": \"document\"\n          }\n        ],\n        \"outputs\": {\n          \"output\": \"document\"\n        },\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 438,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 738.2892369083152,\n        \"y\": -596.450299179969\n      },\n      \"dragging\": false\n    }\n  ],\n  \"edges\": [\n    {\n      \"source\": \"pinecone_0\",\n      \"sourceHandle\": \"pinecone_0-output-retriever-Pinecone|VectorStoreRetriever|BaseRetriever\",\n      \"target\": \"conversationalRetrievalQAChain_0\",\n      \"targetHandle\": \"conversationalRetrievalQAChain_0-input-vectorStoreRetriever-BaseRetriever\",\n      \"type\": \"buttonedge\",\n      \"id\": \"pinecone_0-pinecone_0-output-retriever-Pinecone|VectorStoreRetriever|BaseRetriever-conversationalRetrievalQAChain_0-conversationalRetrievalQAChain_0-input-vectorStoreRetriever-BaseRetriever\"\n    },\n    {\n      \"source\": \"bufferMemory_0\",\n      \"sourceHandle\": \"bufferMemory_0-output-bufferMemory-BufferMemory|BaseChatMemory|BaseMemory\",\n      \"target\": \"conversationalRetrievalQAChain_0\",\n      \"targetHandle\": \"conversationalRetrievalQAChain_0-input-memory-BaseMemory\",\n      \"type\": \"buttonedge\",\n      \"id\": \"bufferMemory_0-bufferMemory_0-output-bufferMemory-BufferMemory|BaseChatMemory|BaseMemory-conversationalRetrievalQAChain_0-conversationalRetrievalQAChain_0-input-memory-BaseMemory\"\n    },\n    {\n      \"source\": \"googleGenerativeAiEmbeddings_0\",\n      \"sourceHandle\": \"googleGenerativeAiEmbeddings_0-output-googleGenerativeAiEmbeddings-GoogleGenerativeAiEmbeddings|Embeddings\",\n      \"target\": \"pinecone_0\",\n      \"targetHandle\": \"pinecone_0-input-embeddings-Embeddings\",\n      \"type\": \"buttonedge\",\n      \"id\": \"googleGenerativeAiEmbeddings_0-googleGenerativeAiEmbeddings_0-output-googleGenerativeAiEmbeddings-GoogleGenerativeAiEmbeddings|Embeddings-pinecone_0-pinecone_0-input-embeddings-Embeddings\"\n    },\n    {\n      \"source\": \"chatGoogleGenerativeAI_0\",\n      \"sourceHandle\": \"chatGoogleGenerativeAI_0-output-chatGoogleGenerativeAI-ChatGoogleGenerativeAI|LangchainChatGoogleGenerativeAI|BaseChatModel|BaseLanguageModel|Runnable\",\n      \"target\": \"conversationalRetrievalQAChain_0\",\n      \"targetHandle\": \"conversationalRetrievalQAChain_0-input-model-BaseChatModel\",\n      \"type\": \"buttonedge\",\n      \"id\": \"chatGoogleGenerativeAI_0-chatGoogleGenerativeAI_0-output-chatGoogleGenerativeAI-ChatGoogleGenerativeAI|LangchainChatGoogleGenerativeAI|BaseChatModel|BaseLanguageModel|Runnable-conversationalRetrievalQAChain_0-conversationalRetrievalQAChain_0-input-model-BaseChatModel\"\n    },\n    {\n      \"source\": \"docxFile_0\",\n      \"sourceHandle\": \"docxFile_0-output-document-Document|json\",\n      \"target\": \"pinecone_0\",\n      \"targetHandle\": \"pinecone_0-input-document-Document\",\n      \"type\": \"buttonedge\",\n      \"id\": \"docxFile_0-docxFile_0-output-document-Document|json-pinecone_0-pinecone_0-input-document-Document\"\n    },\n    {\n      \"source\": \"tokenTextSplitter_0\",\n      \"sourceHandle\": \"tokenTextSplitter_0-output-tokenTextSplitter-TokenTextSplitter|TextSplitter|BaseDocumentTransformer|Runnable\",\n      \"target\": \"docxFile_0\",\n      \"targetHandle\": \"docxFile_0-input-textSplitter-TextSplitter\",\n      \"type\": \"buttonedge\",\n      \"id\": \"tokenTextSplitter_0-tokenTextSplitter_0-output-tokenTextSplitter-TokenTextSplitter|TextSplitter|BaseDocumentTransformer|Runnable-docxFile_0-docxFile_0-input-textSplitter-TextSplitter\"\n    }\n  ]\n}",
      "type": "CHATFLOW"
    },
    {
      "id": "8e8983ba-179f-4e5f-984f-bcb8f57c7935",
      "name": "multilingualbot",
      "flowData": "{\n  \"nodes\": [\n    {\n      \"id\": \"chatCohere_0\",\n      \"position\": {\n        \"x\": 884.1520741281954,\n        \"y\": -523.816671932461\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"chatCohere_0\",\n        \"label\": \"ChatCohere\",\n        \"version\": 1,\n        \"name\": \"chatCohere\",\n        \"type\": \"ChatCohere\",\n        \"baseClasses\": [\n          \"ChatCohere\",\n          \"BaseChatModel\",\n          \"BaseLanguageModel\",\n          \"Runnable\"\n        ],\n        \"category\": \"Chat Models\",\n        \"description\": \"Wrapper around Cohere Chat Endpoints\",\n        \"inputParams\": [\n          {\n            \"label\": \"Connect Credential\",\n            \"name\": \"credential\",\n            \"type\": \"credential\",\n            \"credentialNames\": [\n              \"cohereApi\"\n            ],\n            \"id\": \"chatCohere_0-input-credential-credential\"\n          },\n          {\n            \"label\": \"Model Name\",\n            \"name\": \"modelName\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"default\": \"command-r\",\n            \"id\": \"chatCohere_0-input-modelName-asyncOptions\"\n          },\n          {\n            \"label\": \"Temperature\",\n            \"name\": \"temperature\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"default\": 0.7,\n            \"optional\": true,\n            \"id\": \"chatCohere_0-input-temperature-number\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Cache\",\n            \"name\": \"cache\",\n            \"type\": \"BaseCache\",\n            \"optional\": true,\n            \"id\": \"chatCohere_0-input-cache-BaseCache\"\n          }\n        ],\n        \"inputs\": {\n          \"cache\": \"\",\n          \"modelName\": \"command-r\",\n          \"temperature\": \"0.1\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"chatCohere_0-output-chatCohere-ChatCohere|BaseChatModel|BaseLanguageModel|Runnable\",\n            \"name\": \"chatCohere\",\n            \"label\": \"ChatCohere\",\n            \"description\": \"Wrapper around Cohere Chat Endpoints\",\n            \"type\": \"ChatCohere | BaseChatModel | BaseLanguageModel | Runnable\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 519,\n      \"selected\": false,\n      \"dragging\": false,\n      \"positionAbsolute\": {\n        \"x\": 884.1520741281954,\n        \"y\": -523.816671932461\n      }\n    },\n    {\n      \"id\": \"cohereEmbeddings_0\",\n      \"position\": {\n        \"x\": 357.7799393604005,\n        \"y\": 243.15030854083784\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"cohereEmbeddings_0\",\n        \"label\": \"Cohere Embeddings\",\n        \"version\": 3,\n        \"name\": \"cohereEmbeddings\",\n        \"type\": \"CohereEmbeddings\",\n        \"baseClasses\": [\n          \"CohereEmbeddings\",\n          \"Embeddings\"\n        ],\n        \"category\": \"Embeddings\",\n        \"description\": \"Cohere API to generate embeddings for a given text\",\n        \"inputParams\": [\n          {\n            \"label\": \"Connect Credential\",\n            \"name\": \"credential\",\n            \"type\": \"credential\",\n            \"credentialNames\": [\n              \"cohereApi\"\n            ],\n            \"id\": \"cohereEmbeddings_0-input-credential-credential\"\n          },\n          {\n            \"label\": \"Model Name\",\n            \"name\": \"modelName\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"default\": \"embed-english-v2.0\",\n            \"id\": \"cohereEmbeddings_0-input-modelName-asyncOptions\"\n          },\n          {\n            \"label\": \"Type\",\n            \"name\": \"inputType\",\n            \"type\": \"options\",\n            \"description\": \"Specifies the type of input passed to the model. Required for embedding models v3 and higher. <a target=\\\"_blank\\\" href=\\\"https://docs.cohere.com/reference/embed\\\">Official Docs</a>\",\n            \"options\": [\n              {\n                \"label\": \"search_document\",\n                \"name\": \"search_document\",\n                \"description\": \"Use this to encode documents for embeddings that you store in a vector database for search use-cases\"\n              },\n              {\n                \"label\": \"search_query\",\n                \"name\": \"search_query\",\n                \"description\": \"Use this when you query your vector DB to find relevant documents.\"\n              },\n              {\n                \"label\": \"classification\",\n                \"name\": \"classification\",\n                \"description\": \"Use this when you use the embeddings as an input to a text classifier\"\n              },\n              {\n                \"label\": \"clustering\",\n                \"name\": \"clustering\",\n                \"description\": \"Use this when you want to cluster the embeddings.\"\n              }\n            ],\n            \"default\": \"search_query\",\n            \"optional\": true,\n            \"id\": \"cohereEmbeddings_0-input-inputType-options\"\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"modelName\": \"embed-multilingual-v3.0\",\n          \"inputType\": \"search_query\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"cohereEmbeddings_0-output-cohereEmbeddings-CohereEmbeddings|Embeddings\",\n            \"name\": \"cohereEmbeddings\",\n            \"label\": \"CohereEmbeddings\",\n            \"description\": \"Cohere API to generate embeddings for a given text\",\n            \"type\": \"CohereEmbeddings | Embeddings\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 465,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 357.7799393604005,\n        \"y\": 243.15030854083784\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"pdfFile_0\",\n      \"position\": {\n        \"x\": 420.7934801736799,\n        \"y\": -414.9417100954356\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"pdfFile_0\",\n        \"label\": \"Pdf File\",\n        \"version\": 1,\n        \"name\": \"pdfFile\",\n        \"type\": \"Document\",\n        \"baseClasses\": [\n          \"Document\"\n        ],\n        \"category\": \"Document Loaders\",\n        \"description\": \"Load data from PDF files\",\n        \"inputParams\": [\n          {\n            \"label\": \"Pdf File\",\n            \"name\": \"pdfFile\",\n            \"type\": \"file\",\n            \"fileType\": \".pdf\",\n            \"id\": \"pdfFile_0-input-pdfFile-file\"\n          },\n          {\n            \"label\": \"Usage\",\n            \"name\": \"usage\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"One document per page\",\n                \"name\": \"perPage\"\n              },\n              {\n                \"label\": \"One document per file\",\n                \"name\": \"perFile\"\n              }\n            ],\n            \"default\": \"perPage\",\n            \"id\": \"pdfFile_0-input-usage-options\"\n          },\n          {\n            \"label\": \"Use Legacy Build\",\n            \"name\": \"legacyBuild\",\n            \"type\": \"boolean\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"pdfFile_0-input-legacyBuild-boolean\"\n          },\n          {\n            \"label\": \"Additional Metadata\",\n            \"name\": \"metadata\",\n            \"type\": \"json\",\n            \"description\": \"Additional metadata to be added to the extracted documents\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"pdfFile_0-input-metadata-json\"\n          },\n          {\n            \"label\": \"Omit Metadata Keys\",\n            \"name\": \"omitMetadataKeys\",\n            \"type\": \"string\",\n            \"rows\": 4,\n            \"description\": \"Each document loader comes with a default set of metadata keys that are extracted from the document. You can use this field to omit some of the default metadata keys. The value should be a list of keys, seperated by comma. Use * to omit all metadata keys execept the ones you specify in the Additional Metadata field\",\n            \"placeholder\": \"key1, key2, key3.nestedKey1\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"pdfFile_0-input-omitMetadataKeys-string\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Text Splitter\",\n            \"name\": \"textSplitter\",\n            \"type\": \"TextSplitter\",\n            \"optional\": true,\n            \"id\": \"pdfFile_0-input-textSplitter-TextSplitter\"\n          }\n        ],\n        \"inputs\": {\n          \"textSplitter\": \"{{recursiveCharacterTextSplitter_0.data.instance}}\",\n          \"usage\": \"perPage\",\n          \"legacyBuild\": \"\",\n          \"metadata\": \"\",\n          \"omitMetadataKeys\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"pdfFile_0-output-pdfFile-Document\",\n            \"name\": \"pdfFile\",\n            \"label\": \"Document\",\n            \"description\": \"Load data from PDF files\",\n            \"type\": \"Document\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 506,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 420.7934801736799,\n        \"y\": -414.9417100954356\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"faiss_0\",\n      \"position\": {\n        \"x\": 913.1694280026722,\n        \"y\": 214.85554371232524\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"faiss_0\",\n        \"label\": \"Faiss\",\n        \"version\": 1,\n        \"name\": \"faiss\",\n        \"type\": \"Faiss\",\n        \"baseClasses\": [\n          \"Faiss\",\n          \"VectorStoreRetriever\",\n          \"BaseRetriever\"\n        ],\n        \"category\": \"Vector Stores\",\n        \"description\": \"Upsert embedded data and perform similarity search upon query using Faiss library from Meta\",\n        \"inputParams\": [\n          {\n            \"label\": \"Base Path to load\",\n            \"name\": \"basePath\",\n            \"description\": \"Path to load faiss.index file\",\n            \"placeholder\": \"C:\\\\Users\\\\User\\\\Desktop\",\n            \"type\": \"string\",\n            \"id\": \"faiss_0-input-basePath-string\"\n          },\n          {\n            \"label\": \"Top K\",\n            \"name\": \"topK\",\n            \"description\": \"Number of top results to fetch. Default to 4\",\n            \"placeholder\": \"4\",\n            \"type\": \"number\",\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"id\": \"faiss_0-input-topK-number\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Document\",\n            \"name\": \"document\",\n            \"type\": \"Document\",\n            \"list\": true,\n            \"optional\": true,\n            \"id\": \"faiss_0-input-document-Document\"\n          },\n          {\n            \"label\": \"Embeddings\",\n            \"name\": \"embeddings\",\n            \"type\": \"Embeddings\",\n            \"id\": \"faiss_0-input-embeddings-Embeddings\"\n          }\n        ],\n        \"inputs\": {\n          \"document\": [\n            \"{{pdfFile_0.data.instance}}\"\n          ],\n          \"embeddings\": \"{{cohereEmbeddings_0.data.instance}}\",\n          \"basePath\": \"/opt/render/.flowise/1177-child\",\n          \"topK\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"name\": \"output\",\n            \"label\": \"Output\",\n            \"type\": \"options\",\n            \"description\": \"\",\n            \"options\": [\n              {\n                \"id\": \"faiss_0-output-retriever-Faiss|VectorStoreRetriever|BaseRetriever\",\n                \"name\": \"retriever\",\n                \"label\": \"Faiss Retriever\",\n                \"description\": \"\",\n                \"type\": \"Faiss | VectorStoreRetriever | BaseRetriever\"\n              },\n              {\n                \"id\": \"faiss_0-output-vectorStore-Faiss|SaveableVectorStore|VectorStore\",\n                \"name\": \"vectorStore\",\n                \"label\": \"Faiss Vector Store\",\n                \"description\": \"\",\n                \"type\": \"Faiss | SaveableVectorStore | VectorStore\"\n              }\n            ],\n            \"default\": \"retriever\"\n          }\n        ],\n        \"outputs\": {\n          \"output\": \"retriever\"\n        },\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 456,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 913.1694280026722,\n        \"y\": 214.85554371232524\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"recursiveCharacterTextSplitter_0\",\n      \"position\": {\n        \"x\": -71.14165453560494,\n        \"y\": 9.245022072447853\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"recursiveCharacterTextSplitter_0\",\n        \"label\": \"Recursive Character Text Splitter\",\n        \"version\": 2,\n        \"name\": \"recursiveCharacterTextSplitter\",\n        \"type\": \"RecursiveCharacterTextSplitter\",\n        \"baseClasses\": [\n          \"RecursiveCharacterTextSplitter\",\n          \"TextSplitter\",\n          \"BaseDocumentTransformer\",\n          \"Runnable\"\n        ],\n        \"category\": \"Text Splitters\",\n        \"description\": \"Split documents recursively by different characters - starting with \\\"\\\\n\\\\n\\\", then \\\"\\\\n\\\", then \\\" \\\"\",\n        \"inputParams\": [\n          {\n            \"label\": \"Chunk Size\",\n            \"name\": \"chunkSize\",\n            \"type\": \"number\",\n            \"description\": \"Number of characters in each chunk. Default is 1000.\",\n            \"default\": 1000,\n            \"optional\": true,\n            \"id\": \"recursiveCharacterTextSplitter_0-input-chunkSize-number\"\n          },\n          {\n            \"label\": \"Chunk Overlap\",\n            \"name\": \"chunkOverlap\",\n            \"type\": \"number\",\n            \"description\": \"Number of characters to overlap between chunks. Default is 200.\",\n            \"default\": 200,\n            \"optional\": true,\n            \"id\": \"recursiveCharacterTextSplitter_0-input-chunkOverlap-number\"\n          },\n          {\n            \"label\": \"Custom Separators\",\n            \"name\": \"separators\",\n            \"type\": \"string\",\n            \"rows\": 4,\n            \"description\": \"Array of custom separators to determine when to split the text, will override the default separators\",\n            \"placeholder\": \"[\\\"|\\\", \\\"##\\\", \\\">\\\", \\\"-\\\"]\",\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"id\": \"recursiveCharacterTextSplitter_0-input-separators-string\"\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"chunkSize\": \"150\",\n          \"chunkOverlap\": \"50\",\n          \"separators\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"recursiveCharacterTextSplitter_0-output-recursiveCharacterTextSplitter-RecursiveCharacterTextSplitter|TextSplitter|BaseDocumentTransformer|Runnable\",\n            \"name\": \"recursiveCharacterTextSplitter\",\n            \"label\": \"RecursiveCharacterTextSplitter\",\n            \"description\": \"Split documents recursively by different characters - starting with \\\"\\\\n\\\\n\\\", then \\\"\\\\n\\\", then \\\" \\\"\",\n            \"type\": \"RecursiveCharacterTextSplitter | TextSplitter | BaseDocumentTransformer | Runnable\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 427,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": -71.14165453560494,\n        \"y\": 9.245022072447853\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"bufferMemory_0\",\n      \"position\": {\n        \"x\": 1312.073929836873,\n        \"y\": -291.26647000837363\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"bufferMemory_0\",\n        \"label\": \"Buffer Memory\",\n        \"version\": 2,\n        \"name\": \"bufferMemory\",\n        \"type\": \"BufferMemory\",\n        \"baseClasses\": [\n          \"BufferMemory\",\n          \"BaseChatMemory\",\n          \"BaseMemory\"\n        ],\n        \"category\": \"Memory\",\n        \"description\": \"Retrieve chat messages stored in database\",\n        \"inputParams\": [\n          {\n            \"label\": \"Session Id\",\n            \"name\": \"sessionId\",\n            \"type\": \"string\",\n            \"description\": \"If not specified, a random id will be used. Learn <a target=\\\"_blank\\\" href=\\\"https://docs.flowiseai.com/memory#ui-and-embedded-chat\\\">more</a>\",\n            \"default\": \"\",\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"id\": \"bufferMemory_0-input-sessionId-string\"\n          },\n          {\n            \"label\": \"Memory Key\",\n            \"name\": \"memoryKey\",\n            \"type\": \"string\",\n            \"default\": \"chat_history\",\n            \"additionalParams\": true,\n            \"id\": \"bufferMemory_0-input-memoryKey-string\"\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"sessionId\": \"\",\n          \"memoryKey\": \"chat_history\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"bufferMemory_0-output-bufferMemory-BufferMemory|BaseChatMemory|BaseMemory\",\n            \"name\": \"bufferMemory\",\n            \"label\": \"BufferMemory\",\n            \"description\": \"Retrieve chat messages stored in database\",\n            \"type\": \"BufferMemory | BaseChatMemory | BaseMemory\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 250,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 1312.073929836873,\n        \"y\": -291.26647000837363\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"conversationalRetrievalQAChain_0\",\n      \"position\": {\n        \"x\": 1572.6783013037793,\n        \"y\": 159.46579950343676\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"conversationalRetrievalQAChain_0\",\n        \"label\": \"Conversational Retrieval QA Chain\",\n        \"version\": 3,\n        \"name\": \"conversationalRetrievalQAChain\",\n        \"type\": \"ConversationalRetrievalQAChain\",\n        \"baseClasses\": [\n          \"ConversationalRetrievalQAChain\",\n          \"BaseChain\",\n          \"Runnable\"\n        ],\n        \"category\": \"Chains\",\n        \"description\": \"Document QA - built on RetrievalQAChain to provide a chat history component\",\n        \"inputParams\": [\n          {\n            \"label\": \"Return Source Documents\",\n            \"name\": \"returnSourceDocuments\",\n            \"type\": \"boolean\",\n            \"optional\": true,\n            \"id\": \"conversationalRetrievalQAChain_0-input-returnSourceDocuments-boolean\"\n          },\n          {\n            \"label\": \"Rephrase Prompt\",\n            \"name\": \"rephrasePrompt\",\n            \"type\": \"string\",\n            \"description\": \"Using previous chat history, rephrase question into a standalone question\",\n            \"warning\": \"Prompt must include input variables: {chat_history} and {question}\",\n            \"rows\": 4,\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"default\": \"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone Question:\",\n            \"id\": \"conversationalRetrievalQAChain_0-input-rephrasePrompt-string\"\n          },\n          {\n            \"label\": \"Response Prompt\",\n            \"name\": \"responsePrompt\",\n            \"type\": \"string\",\n            \"description\": \"Taking the rephrased question, search for answer from the provided context\",\n            \"warning\": \"Prompt must include input variable: {context}\",\n            \"rows\": 4,\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"default\": \"I want you to act as a document that I am having a conversation with. Your name is \\\"AI Assistant\\\". Using the provided context, answer the user's question to the best of your ability using the resources provided.\\nIf there is nothing in the context relevant to the question at hand, just say \\\"Hmm, I'm not sure\\\" and stop after that. Refuse to answer any question not about the info. Never break character.\\n------------\\n{context}\\n------------\\nREMEMBER: If there is no relevant information within the context, just say \\\"Hmm, I'm not sure\\\". Don't try to make up an answer. Never break character.\",\n            \"id\": \"conversationalRetrievalQAChain_0-input-responsePrompt-string\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Chat Model\",\n            \"name\": \"model\",\n            \"type\": \"BaseChatModel\",\n            \"id\": \"conversationalRetrievalQAChain_0-input-model-BaseChatModel\"\n          },\n          {\n            \"label\": \"Vector Store Retriever\",\n            \"name\": \"vectorStoreRetriever\",\n            \"type\": \"BaseRetriever\",\n            \"id\": \"conversationalRetrievalQAChain_0-input-vectorStoreRetriever-BaseRetriever\"\n          },\n          {\n            \"label\": \"Memory\",\n            \"name\": \"memory\",\n            \"type\": \"BaseMemory\",\n            \"optional\": true,\n            \"description\": \"If left empty, a default BufferMemory will be used\",\n            \"id\": \"conversationalRetrievalQAChain_0-input-memory-BaseMemory\"\n          },\n          {\n            \"label\": \"Input Moderation\",\n            \"description\": \"Detect text that could generate harmful output and prevent it from being sent to the language model\",\n            \"name\": \"inputModeration\",\n            \"type\": \"Moderation\",\n            \"optional\": true,\n            \"list\": true,\n            \"id\": \"conversationalRetrievalQAChain_0-input-inputModeration-Moderation\"\n          }\n        ],\n        \"inputs\": {\n          \"model\": \"{{chatCohere_0.data.instance}}\",\n          \"vectorStoreRetriever\": \"{{faiss_0.data.instance}}\",\n          \"memory\": \"{{bufferMemory_0.data.instance}}\",\n          \"returnSourceDocuments\": true,\n          \"rephrasePrompt\": \"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone Question:\",\n          \"responsePrompt\": \"I want you to act as a document that I am having a conversation with. Your name is \\\"AI Assistant\\\". Using the provided context, answer the user's question to the best of your ability using the resources provided.\\nIf there is nothing in the context relevant to the question at hand, just say \\\"Hmm, I'm not sure\\\" and stop after that. Refuse to answer any question not about the info. Never break character.\\n------------\\n{context}\\n------------\\nREMEMBER: If there is no relevant information within the context, just say \\\"Hmm, I'm not sure\\\". Don't try to make up an answer. Never break character.\",\n          \"inputModeration\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"conversationalRetrievalQAChain_0-output-conversationalRetrievalQAChain-ConversationalRetrievalQAChain|BaseChain|Runnable\",\n            \"name\": \"conversationalRetrievalQAChain\",\n            \"label\": \"ConversationalRetrievalQAChain\",\n            \"description\": \"Document QA - built on RetrievalQAChain to provide a chat history component\",\n            \"type\": \"ConversationalRetrievalQAChain | BaseChain | Runnable\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 529,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 1572.6783013037793,\n        \"y\": 159.46579950343676\n      },\n      \"dragging\": false\n    }\n  ],\n  \"edges\": [\n    {\n      \"source\": \"cohereEmbeddings_0\",\n      \"sourceHandle\": \"cohereEmbeddings_0-output-cohereEmbeddings-CohereEmbeddings|Embeddings\",\n      \"target\": \"faiss_0\",\n      \"targetHandle\": \"faiss_0-input-embeddings-Embeddings\",\n      \"type\": \"buttonedge\",\n      \"id\": \"cohereEmbeddings_0-cohereEmbeddings_0-output-cohereEmbeddings-CohereEmbeddings|Embeddings-faiss_0-faiss_0-input-embeddings-Embeddings\"\n    },\n    {\n      \"source\": \"pdfFile_0\",\n      \"sourceHandle\": \"pdfFile_0-output-pdfFile-Document\",\n      \"target\": \"faiss_0\",\n      \"targetHandle\": \"faiss_0-input-document-Document\",\n      \"type\": \"buttonedge\",\n      \"id\": \"pdfFile_0-pdfFile_0-output-pdfFile-Document-faiss_0-faiss_0-input-document-Document\"\n    },\n    {\n      \"source\": \"recursiveCharacterTextSplitter_0\",\n      \"sourceHandle\": \"recursiveCharacterTextSplitter_0-output-recursiveCharacterTextSplitter-RecursiveCharacterTextSplitter|TextSplitter|BaseDocumentTransformer|Runnable\",\n      \"target\": \"pdfFile_0\",\n      \"targetHandle\": \"pdfFile_0-input-textSplitter-TextSplitter\",\n      \"type\": \"buttonedge\",\n      \"id\": \"recursiveCharacterTextSplitter_0-recursiveCharacterTextSplitter_0-output-recursiveCharacterTextSplitter-RecursiveCharacterTextSplitter|TextSplitter|BaseDocumentTransformer|Runnable-pdfFile_0-pdfFile_0-input-textSplitter-TextSplitter\"\n    },\n    {\n      \"source\": \"chatCohere_0\",\n      \"sourceHandle\": \"chatCohere_0-output-chatCohere-ChatCohere|BaseChatModel|BaseLanguageModel|Runnable\",\n      \"target\": \"conversationalRetrievalQAChain_0\",\n      \"targetHandle\": \"conversationalRetrievalQAChain_0-input-model-BaseChatModel\",\n      \"type\": \"buttonedge\",\n      \"id\": \"chatCohere_0-chatCohere_0-output-chatCohere-ChatCohere|BaseChatModel|BaseLanguageModel|Runnable-conversationalRetrievalQAChain_0-conversationalRetrievalQAChain_0-input-model-BaseChatModel\"\n    },\n    {\n      \"source\": \"faiss_0\",\n      \"sourceHandle\": \"faiss_0-output-retriever-Faiss|VectorStoreRetriever|BaseRetriever\",\n      \"target\": \"conversationalRetrievalQAChain_0\",\n      \"targetHandle\": \"conversationalRetrievalQAChain_0-input-vectorStoreRetriever-BaseRetriever\",\n      \"type\": \"buttonedge\",\n      \"id\": \"faiss_0-faiss_0-output-retriever-Faiss|VectorStoreRetriever|BaseRetriever-conversationalRetrievalQAChain_0-conversationalRetrievalQAChain_0-input-vectorStoreRetriever-BaseRetriever\"\n    },\n    {\n      \"source\": \"bufferMemory_0\",\n      \"sourceHandle\": \"bufferMemory_0-output-bufferMemory-BufferMemory|BaseChatMemory|BaseMemory\",\n      \"target\": \"conversationalRetrievalQAChain_0\",\n      \"targetHandle\": \"conversationalRetrievalQAChain_0-input-memory-BaseMemory\",\n      \"type\": \"buttonedge\",\n      \"id\": \"bufferMemory_0-bufferMemory_0-output-bufferMemory-BufferMemory|BaseChatMemory|BaseMemory-conversationalRetrievalQAChain_0-conversationalRetrievalQAChain_0-input-memory-BaseMemory\"\n    }\n  ]\n}",
      "type": "CHATFLOW"
    },
    {
      "id": "f031f470-33c8-4620-8557-2722a043f762",
      "name": "grok+llama",
      "flowData": "{\n  \"nodes\": [\n    {\n      \"id\": \"groqChat_0\",\n      \"position\": {\n        \"x\": 537.9587263279853,\n        \"y\": -135.886011176785\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"groqChat_0\",\n        \"label\": \"GroqChat\",\n        \"version\": 3,\n        \"name\": \"groqChat\",\n        \"type\": \"GroqChat\",\n        \"baseClasses\": [\n          \"GroqChat\",\n          \"BaseChatModel\",\n          \"BaseLanguageModel\",\n          \"Runnable\"\n        ],\n        \"category\": \"Chat Models\",\n        \"description\": \"Wrapper around Groq API with LPU Inference Engine\",\n        \"inputParams\": [\n          {\n            \"label\": \"Connect Credential\",\n            \"name\": \"credential\",\n            \"type\": \"credential\",\n            \"credentialNames\": [\n              \"groqApi\"\n            ],\n            \"optional\": true,\n            \"id\": \"groqChat_0-input-credential-credential\"\n          },\n          {\n            \"label\": \"Model Name\",\n            \"name\": \"modelName\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"placeholder\": \"llama3-70b-8192\",\n            \"id\": \"groqChat_0-input-modelName-asyncOptions\"\n          },\n          {\n            \"label\": \"Temperature\",\n            \"name\": \"temperature\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"default\": 0.9,\n            \"optional\": true,\n            \"id\": \"groqChat_0-input-temperature-number\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Cache\",\n            \"name\": \"cache\",\n            \"type\": \"BaseCache\",\n            \"optional\": true,\n            \"id\": \"groqChat_0-input-cache-BaseCache\"\n          }\n        ],\n        \"inputs\": {\n          \"cache\": \"\",\n          \"modelName\": \"llama-3.2-3b-preview\",\n          \"temperature\": \"0.2\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"groqChat_0-output-groqChat-GroqChat|BaseChatModel|BaseLanguageModel|Runnable\",\n            \"name\": \"groqChat\",\n            \"label\": \"GroqChat\",\n            \"description\": \"Wrapper around Groq API with LPU Inference Engine\",\n            \"type\": \"GroqChat | BaseChatModel | BaseLanguageModel | Runnable\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 519,\n      \"selected\": false,\n      \"dragging\": false,\n      \"positionAbsolute\": {\n        \"x\": 537.9587263279853,\n        \"y\": -135.886011176785\n      }\n    },\n    {\n      \"id\": \"ollama_0\",\n      \"position\": {\n        \"x\": 1368.8171609309877,\n        \"y\": 34.01899639575146\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"ollama_0\",\n        \"label\": \"Ollama\",\n        \"version\": 2,\n        \"name\": \"ollama\",\n        \"type\": \"Ollama\",\n        \"baseClasses\": [\n          \"Ollama\",\n          \"LLM\",\n          \"BaseLLM\",\n          \"BaseLanguageModel\",\n          \"Runnable\"\n        ],\n        \"category\": \"LLMs\",\n        \"description\": \"Wrapper around open source large language models on Ollama\",\n        \"inputParams\": [\n          {\n            \"label\": \"Base URL\",\n            \"name\": \"baseUrl\",\n            \"type\": \"string\",\n            \"default\": \"http://localhost:11434\",\n            \"id\": \"ollama_0-input-baseUrl-string\"\n          },\n          {\n            \"label\": \"Model Name\",\n            \"name\": \"modelName\",\n            \"type\": \"string\",\n            \"placeholder\": \"llama2\",\n            \"id\": \"ollama_0-input-modelName-string\"\n          },\n          {\n            \"label\": \"Temperature\",\n            \"name\": \"temperature\",\n            \"type\": \"number\",\n            \"description\": \"The temperature of the model. Increasing the temperature will make the model answer more creatively. (Default: 0.8). Refer to <a target=\\\"_blank\\\" href=\\\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\\\">docs</a> for more details\",\n            \"step\": 0.1,\n            \"default\": 0.9,\n            \"optional\": true,\n            \"id\": \"ollama_0-input-temperature-number\"\n          },\n          {\n            \"label\": \"Top P\",\n            \"name\": \"topP\",\n            \"type\": \"number\",\n            \"description\": \"Works together with top-k. A higher value (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. (Default: 0.9). Refer to <a target=\\\"_blank\\\" href=\\\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\\\">docs</a> for more details\",\n            \"step\": 0.1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"ollama_0-input-topP-number\"\n          },\n          {\n            \"label\": \"Top K\",\n            \"name\": \"topK\",\n            \"type\": \"number\",\n            \"description\": \"Reduces the probability of generating nonsense. A higher value (e.g. 100) will give more diverse answers, while a lower value (e.g. 10) will be more conservative. (Default: 40). Refer to <a target=\\\"_blank\\\" href=\\\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\\\">docs</a> for more details\",\n            \"step\": 1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"ollama_0-input-topK-number\"\n          },\n          {\n            \"label\": \"Mirostat\",\n            \"name\": \"mirostat\",\n            \"type\": \"number\",\n            \"description\": \"Enable Mirostat sampling for controlling perplexity. (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0). Refer to <a target=\\\"_blank\\\" href=\\\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\\\">docs</a> for more details\",\n            \"step\": 1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"ollama_0-input-mirostat-number\"\n          },\n          {\n            \"label\": \"Mirostat ETA\",\n            \"name\": \"mirostatEta\",\n            \"type\": \"number\",\n            \"description\": \"Influences how quickly the algorithm responds to feedback from the generated text. A lower learning rate will result in slower adjustments, while a higher learning rate will make the algorithm more responsive. (Default: 0.1) Refer to <a target=\\\"_blank\\\" href=\\\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\\\">docs</a> for more details\",\n            \"step\": 0.1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"ollama_0-input-mirostatEta-number\"\n          },\n          {\n            \"label\": \"Mirostat TAU\",\n            \"name\": \"mirostatTau\",\n            \"type\": \"number\",\n            \"description\": \"Controls the balance between coherence and diversity of the output. A lower value will result in more focused and coherent text. (Default: 5.0) Refer to <a target=\\\"_blank\\\" href=\\\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\\\">docs</a> for more details\",\n            \"step\": 0.1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"ollama_0-input-mirostatTau-number\"\n          },\n          {\n            \"label\": \"Context Window Size\",\n            \"name\": \"numCtx\",\n            \"type\": \"number\",\n            \"description\": \"Sets the size of the context window used to generate the next token. (Default: 2048) Refer to <a target=\\\"_blank\\\" href=\\\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\\\">docs</a> for more details\",\n            \"step\": 1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"ollama_0-input-numCtx-number\"\n          },\n          {\n            \"label\": \"Number of GQA groups\",\n            \"name\": \"numGqa\",\n            \"type\": \"number\",\n            \"description\": \"The number of GQA groups in the transformer layer. Required for some models, for example it is 8 for llama2:70b. Refer to <a target=\\\"_blank\\\" href=\\\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\\\">docs</a> for more details\",\n            \"step\": 1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"ollama_0-input-numGqa-number\"\n          },\n          {\n            \"label\": \"Number of GPU\",\n            \"name\": \"numGpu\",\n            \"type\": \"number\",\n            \"description\": \"The number of layers to send to the GPU(s). On macOS it defaults to 1 to enable metal support, 0 to disable. Refer to <a target=\\\"_blank\\\" href=\\\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\\\">docs</a> for more details\",\n            \"step\": 1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"ollama_0-input-numGpu-number\"\n          },\n          {\n            \"label\": \"Number of Thread\",\n            \"name\": \"numThread\",\n            \"type\": \"number\",\n            \"description\": \"Sets the number of threads to use during computation. By default, Ollama will detect this for optimal performance. It is recommended to set this value to the number of physical CPU cores your system has (as opposed to the logical number of cores). Refer to <a target=\\\"_blank\\\" href=\\\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\\\">docs</a> for more details\",\n            \"step\": 1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"ollama_0-input-numThread-number\"\n          },\n          {\n            \"label\": \"Repeat Last N\",\n            \"name\": \"repeatLastN\",\n            \"type\": \"number\",\n            \"description\": \"Sets how far back for the model to look back to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx). Refer to <a target=\\\"_blank\\\" href=\\\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\\\">docs</a> for more details\",\n            \"step\": 1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"ollama_0-input-repeatLastN-number\"\n          },\n          {\n            \"label\": \"Repeat Penalty\",\n            \"name\": \"repeatPenalty\",\n            \"type\": \"number\",\n            \"description\": \"Sets how strongly to penalize repetitions. A higher value (e.g., 1.5) will penalize repetitions more strongly, while a lower value (e.g., 0.9) will be more lenient. (Default: 1.1). Refer to <a target=\\\"_blank\\\" href=\\\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\\\">docs</a> for more details\",\n            \"step\": 0.1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"ollama_0-input-repeatPenalty-number\"\n          },\n          {\n            \"label\": \"Stop Sequence\",\n            \"name\": \"stop\",\n            \"type\": \"string\",\n            \"rows\": 4,\n            \"placeholder\": \"AI assistant:\",\n            \"description\": \"Sets the stop sequences to use. Use comma to seperate different sequences. Refer to <a target=\\\"_blank\\\" href=\\\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\\\">docs</a> for more details\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"ollama_0-input-stop-string\"\n          },\n          {\n            \"label\": \"Tail Free Sampling\",\n            \"name\": \"tfsZ\",\n            \"type\": \"number\",\n            \"description\": \"Tail free sampling is used to reduce the impact of less probable tokens from the output. A higher value (e.g., 2.0) will reduce the impact more, while a value of 1.0 disables this setting. (Default: 1). Refer to <a target=\\\"_blank\\\" href=\\\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\\\">docs</a> for more details\",\n            \"step\": 0.1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"ollama_0-input-tfsZ-number\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Cache\",\n            \"name\": \"cache\",\n            \"type\": \"BaseCache\",\n            \"optional\": true,\n            \"id\": \"ollama_0-input-cache-BaseCache\"\n          }\n        ],\n        \"inputs\": {\n          \"cache\": \"\",\n          \"baseUrl\": \"http://localhost:11434\",\n          \"modelName\": \"\",\n          \"temperature\": 0.9,\n          \"topP\": \"\",\n          \"topK\": \"\",\n          \"mirostat\": \"\",\n          \"mirostatEta\": \"\",\n          \"mirostatTau\": \"\",\n          \"numCtx\": \"\",\n          \"numGqa\": \"\",\n          \"numGpu\": \"\",\n          \"numThread\": \"\",\n          \"repeatLastN\": \"\",\n          \"repeatPenalty\": \"\",\n          \"stop\": \"\",\n          \"tfsZ\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"ollama_0-output-ollama-Ollama|LLM|BaseLLM|BaseLanguageModel|Runnable\",\n            \"name\": \"ollama\",\n            \"label\": \"Ollama\",\n            \"description\": \"Wrapper around open source large language models on Ollama\",\n            \"type\": \"Ollama | LLM | BaseLLM | BaseLanguageModel | Runnable\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 577,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 1368.8171609309877,\n        \"y\": 34.01899639575146\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"voyageAIEmbeddings_0\",\n      \"position\": {\n        \"x\": -474.03672636892577,\n        \"y\": -8.234365703505162\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"voyageAIEmbeddings_0\",\n        \"label\": \"VoyageAI Embeddings\",\n        \"version\": 2,\n        \"name\": \"voyageAIEmbeddings\",\n        \"type\": \"VoyageAIEmbeddings\",\n        \"baseClasses\": [\n          \"VoyageAIEmbeddings\",\n          \"Embeddings\"\n        ],\n        \"category\": \"Embeddings\",\n        \"description\": \"Voyage AI API to generate embeddings for a given text\",\n        \"inputParams\": [\n          {\n            \"label\": \"Connect Credential\",\n            \"name\": \"credential\",\n            \"type\": \"credential\",\n            \"credentialNames\": [\n              \"voyageAIApi\"\n            ],\n            \"id\": \"voyageAIEmbeddings_0-input-credential-credential\"\n          },\n          {\n            \"label\": \"Model Name\",\n            \"name\": \"modelName\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"default\": \"voyage-2\",\n            \"id\": \"voyageAIEmbeddings_0-input-modelName-asyncOptions\"\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"modelName\": \"voyage-2\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"voyageAIEmbeddings_0-output-voyageAIEmbeddings-VoyageAIEmbeddings|Embeddings\",\n            \"name\": \"voyageAIEmbeddings\",\n            \"label\": \"VoyageAIEmbeddings\",\n            \"description\": \"Voyage AI API to generate embeddings for a given text\",\n            \"type\": \"VoyageAIEmbeddings | Embeddings\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 370,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": -474.03672636892577,\n        \"y\": -8.234365703505162\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"recursiveCharacterTextSplitter_0\",\n      \"position\": {\n        \"x\": -852.7594120161409,\n        \"y\": -296.42262793276734\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"recursiveCharacterTextSplitter_0\",\n        \"label\": \"Recursive Character Text Splitter\",\n        \"version\": 2,\n        \"name\": \"recursiveCharacterTextSplitter\",\n        \"type\": \"RecursiveCharacterTextSplitter\",\n        \"baseClasses\": [\n          \"RecursiveCharacterTextSplitter\",\n          \"TextSplitter\",\n          \"BaseDocumentTransformer\",\n          \"Runnable\"\n        ],\n        \"category\": \"Text Splitters\",\n        \"description\": \"Split documents recursively by different characters - starting with \\\"\\\\n\\\\n\\\", then \\\"\\\\n\\\", then \\\" \\\"\",\n        \"inputParams\": [\n          {\n            \"label\": \"Chunk Size\",\n            \"name\": \"chunkSize\",\n            \"type\": \"number\",\n            \"description\": \"Number of characters in each chunk. Default is 1000.\",\n            \"default\": 1000,\n            \"optional\": true,\n            \"id\": \"recursiveCharacterTextSplitter_0-input-chunkSize-number\"\n          },\n          {\n            \"label\": \"Chunk Overlap\",\n            \"name\": \"chunkOverlap\",\n            \"type\": \"number\",\n            \"description\": \"Number of characters to overlap between chunks. Default is 200.\",\n            \"default\": 200,\n            \"optional\": true,\n            \"id\": \"recursiveCharacterTextSplitter_0-input-chunkOverlap-number\"\n          },\n          {\n            \"label\": \"Custom Separators\",\n            \"name\": \"separators\",\n            \"type\": \"string\",\n            \"rows\": 4,\n            \"description\": \"Array of custom separators to determine when to split the text, will override the default separators\",\n            \"placeholder\": \"[\\\"|\\\", \\\"##\\\", \\\">\\\", \\\"-\\\"]\",\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"id\": \"recursiveCharacterTextSplitter_0-input-separators-string\"\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"chunkSize\": \"200\",\n          \"chunkOverlap\": \"50\",\n          \"separators\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"recursiveCharacterTextSplitter_0-output-recursiveCharacterTextSplitter-RecursiveCharacterTextSplitter|TextSplitter|BaseDocumentTransformer|Runnable\",\n            \"name\": \"recursiveCharacterTextSplitter\",\n            \"label\": \"RecursiveCharacterTextSplitter\",\n            \"description\": \"Split documents recursively by different characters - starting with \\\"\\\\n\\\\n\\\", then \\\"\\\\n\\\", then \\\" \\\"\",\n            \"type\": \"RecursiveCharacterTextSplitter | TextSplitter | BaseDocumentTransformer | Runnable\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 427,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": -852.7594120161409,\n        \"y\": -296.42262793276734\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"toolAgent_0\",\n      \"position\": {\n        \"x\": 908.8994838628555,\n        \"y\": -345.4249792208967\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"toolAgent_0\",\n        \"label\": \"Tool Agent\",\n        \"version\": 2,\n        \"name\": \"toolAgent\",\n        \"type\": \"AgentExecutor\",\n        \"baseClasses\": [\n          \"AgentExecutor\",\n          \"BaseChain\",\n          \"Runnable\"\n        ],\n        \"category\": \"Agents\",\n        \"description\": \"Agent that uses Function Calling to pick the tools and args to call\",\n        \"inputParams\": [\n          {\n            \"label\": \"System Message\",\n            \"name\": \"systemMessage\",\n            \"type\": \"string\",\n            \"default\": \"You are a helpful AI assistant.\",\n            \"description\": \"If Chat Prompt Template is provided, this will be ignored\",\n            \"rows\": 4,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"toolAgent_0-input-systemMessage-string\"\n          },\n          {\n            \"label\": \"Max Iterations\",\n            \"name\": \"maxIterations\",\n            \"type\": \"number\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"toolAgent_0-input-maxIterations-number\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Tools\",\n            \"name\": \"tools\",\n            \"type\": \"Tool\",\n            \"list\": true,\n            \"id\": \"toolAgent_0-input-tools-Tool\"\n          },\n          {\n            \"label\": \"Memory\",\n            \"name\": \"memory\",\n            \"type\": \"BaseChatMemory\",\n            \"id\": \"toolAgent_0-input-memory-BaseChatMemory\"\n          },\n          {\n            \"label\": \"Tool Calling Chat Model\",\n            \"name\": \"model\",\n            \"type\": \"BaseChatModel\",\n            \"description\": \"Only compatible with models that are capable of function calling: ChatOpenAI, ChatMistral, ChatAnthropic, ChatGoogleGenerativeAI, ChatVertexAI, GroqChat\",\n            \"id\": \"toolAgent_0-input-model-BaseChatModel\"\n          },\n          {\n            \"label\": \"Chat Prompt Template\",\n            \"name\": \"chatPromptTemplate\",\n            \"type\": \"ChatPromptTemplate\",\n            \"description\": \"Override existing prompt with Chat Prompt Template. Human Message must includes {input} variable\",\n            \"optional\": true,\n            \"id\": \"toolAgent_0-input-chatPromptTemplate-ChatPromptTemplate\"\n          },\n          {\n            \"label\": \"Input Moderation\",\n            \"description\": \"Detect text that could generate harmful output and prevent it from being sent to the language model\",\n            \"name\": \"inputModeration\",\n            \"type\": \"Moderation\",\n            \"optional\": true,\n            \"list\": true,\n            \"id\": \"toolAgent_0-input-inputModeration-Moderation\"\n          }\n        ],\n        \"inputs\": {\n          \"tools\": [\n            \"{{retrieverTool_0.data.instance}}\"\n          ],\n          \"memory\": \"{{bufferMemory_0.data.instance}}\",\n          \"model\": \"{{groqChat_0.data.instance}}\",\n          \"chatPromptTemplate\": \"\",\n          \"systemMessage\": \"You are a helpful AI assistant.\",\n          \"inputModeration\": \"\",\n          \"maxIterations\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"toolAgent_0-output-toolAgent-AgentExecutor|BaseChain|Runnable\",\n            \"name\": \"toolAgent\",\n            \"label\": \"AgentExecutor\",\n            \"description\": \"Agent that uses Function Calling to pick the tools and args to call\",\n            \"type\": \"AgentExecutor | BaseChain | Runnable\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 483,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 908.8994838628555,\n        \"y\": -345.4249792208967\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"bufferMemory_0\",\n      \"position\": {\n        \"x\": 528.8654996263605,\n        \"y\": -424.39532863216294\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"bufferMemory_0\",\n        \"label\": \"Buffer Memory\",\n        \"version\": 2,\n        \"name\": \"bufferMemory\",\n        \"type\": \"BufferMemory\",\n        \"baseClasses\": [\n          \"BufferMemory\",\n          \"BaseChatMemory\",\n          \"BaseMemory\"\n        ],\n        \"category\": \"Memory\",\n        \"description\": \"Retrieve chat messages stored in database\",\n        \"inputParams\": [\n          {\n            \"label\": \"Session Id\",\n            \"name\": \"sessionId\",\n            \"type\": \"string\",\n            \"description\": \"If not specified, a random id will be used. Learn <a target=\\\"_blank\\\" href=\\\"https://docs.flowiseai.com/memory#ui-and-embedded-chat\\\">more</a>\",\n            \"default\": \"\",\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"id\": \"bufferMemory_0-input-sessionId-string\"\n          },\n          {\n            \"label\": \"Memory Key\",\n            \"name\": \"memoryKey\",\n            \"type\": \"string\",\n            \"default\": \"chat_history\",\n            \"additionalParams\": true,\n            \"id\": \"bufferMemory_0-input-memoryKey-string\"\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"sessionId\": \"\",\n          \"memoryKey\": \"chat_history\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"bufferMemory_0-output-bufferMemory-BufferMemory|BaseChatMemory|BaseMemory\",\n            \"name\": \"bufferMemory\",\n            \"label\": \"BufferMemory\",\n            \"description\": \"Retrieve chat messages stored in database\",\n            \"type\": \"BufferMemory | BaseChatMemory | BaseMemory\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 250,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 528.8654996263605,\n        \"y\": -424.39532863216294\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"docxFile_0\",\n      \"position\": {\n        \"x\": -498.16084802538484,\n        \"y\": -465.4129839843411\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"docxFile_0\",\n        \"label\": \"Docx File\",\n        \"version\": 1,\n        \"name\": \"docxFile\",\n        \"type\": \"Document\",\n        \"baseClasses\": [\n          \"Document\"\n        ],\n        \"category\": \"Document Loaders\",\n        \"description\": \"Load data from DOCX files\",\n        \"inputParams\": [\n          {\n            \"label\": \"Docx File\",\n            \"name\": \"docxFile\",\n            \"type\": \"file\",\n            \"fileType\": \".docx\",\n            \"id\": \"docxFile_0-input-docxFile-file\"\n          },\n          {\n            \"label\": \"Additional Metadata\",\n            \"name\": \"metadata\",\n            \"type\": \"json\",\n            \"description\": \"Additional metadata to be added to the extracted documents\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"docxFile_0-input-metadata-json\"\n          },\n          {\n            \"label\": \"Omit Metadata Keys\",\n            \"name\": \"omitMetadataKeys\",\n            \"type\": \"string\",\n            \"rows\": 4,\n            \"description\": \"Each document loader comes with a default set of metadata keys that are extracted from the document. You can use this field to omit some of the default metadata keys. The value should be a list of keys, seperated by comma. Use * to omit all metadata keys execept the ones you specify in the Additional Metadata field\",\n            \"placeholder\": \"key1, key2, key3.nestedKey1\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"docxFile_0-input-omitMetadataKeys-string\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Text Splitter\",\n            \"name\": \"textSplitter\",\n            \"type\": \"TextSplitter\",\n            \"optional\": true,\n            \"id\": \"docxFile_0-input-textSplitter-TextSplitter\"\n          }\n        ],\n        \"inputs\": {\n          \"textSplitter\": \"{{recursiveCharacterTextSplitter_0.data.instance}}\",\n          \"metadata\": \"\",\n          \"omitMetadataKeys\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"docxFile_0-output-docxFile-Document\",\n            \"name\": \"docxFile\",\n            \"label\": \"Document\",\n            \"description\": \"Load data from DOCX files\",\n            \"type\": \"Document\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 410,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": -498.16084802538484,\n        \"y\": -465.4129839843411\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"faiss_0\",\n      \"position\": {\n        \"x\": -143.4779216934812,\n        \"y\": -416.56918453185494\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"faiss_0\",\n        \"label\": \"Faiss\",\n        \"version\": 1,\n        \"name\": \"faiss\",\n        \"type\": \"Faiss\",\n        \"baseClasses\": [\n          \"Faiss\",\n          \"VectorStoreRetriever\",\n          \"BaseRetriever\"\n        ],\n        \"category\": \"Vector Stores\",\n        \"description\": \"Upsert embedded data and perform similarity search upon query using Faiss library from Meta\",\n        \"inputParams\": [\n          {\n            \"label\": \"Base Path to load\",\n            \"name\": \"basePath\",\n            \"description\": \"Path to load faiss.index file\",\n            \"placeholder\": \"C:\\\\Users\\\\User\\\\Desktop\",\n            \"type\": \"string\",\n            \"id\": \"faiss_0-input-basePath-string\"\n          },\n          {\n            \"label\": \"Top K\",\n            \"name\": \"topK\",\n            \"description\": \"Number of top results to fetch. Default to 4\",\n            \"placeholder\": \"4\",\n            \"type\": \"number\",\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"id\": \"faiss_0-input-topK-number\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Document\",\n            \"name\": \"document\",\n            \"type\": \"Document\",\n            \"list\": true,\n            \"optional\": true,\n            \"id\": \"faiss_0-input-document-Document\"\n          },\n          {\n            \"label\": \"Embeddings\",\n            \"name\": \"embeddings\",\n            \"type\": \"Embeddings\",\n            \"id\": \"faiss_0-input-embeddings-Embeddings\"\n          }\n        ],\n        \"inputs\": {\n          \"document\": [\n            \"{{docxFile_0.data.instance}}\"\n          ],\n          \"embeddings\": \"{{voyageAIEmbeddings_0.data.instance}}\",\n          \"basePath\": \"/opt/render/.flowise/1177\",\n          \"topK\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"name\": \"output\",\n            \"label\": \"Output\",\n            \"type\": \"options\",\n            \"description\": \"\",\n            \"options\": [\n              {\n                \"id\": \"faiss_0-output-retriever-Faiss|VectorStoreRetriever|BaseRetriever\",\n                \"name\": \"retriever\",\n                \"label\": \"Faiss Retriever\",\n                \"description\": \"\",\n                \"type\": \"Faiss | VectorStoreRetriever | BaseRetriever\"\n              },\n              {\n                \"id\": \"faiss_0-output-vectorStore-Faiss|SaveableVectorStore|VectorStore\",\n                \"name\": \"vectorStore\",\n                \"label\": \"Faiss Vector Store\",\n                \"description\": \"\",\n                \"type\": \"Faiss | SaveableVectorStore | VectorStore\"\n              }\n            ],\n            \"default\": \"retriever\"\n          }\n        ],\n        \"outputs\": {\n          \"output\": \"retriever\"\n        },\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 456,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": -143.4779216934812,\n        \"y\": -416.56918453185494\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"retrieverTool_0\",\n      \"position\": {\n        \"x\": 204.54833182164043,\n        \"y\": -399.9178877515032\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"retrieverTool_0\",\n        \"label\": \"Retriever Tool\",\n        \"version\": 2,\n        \"name\": \"retrieverTool\",\n        \"type\": \"RetrieverTool\",\n        \"baseClasses\": [\n          \"RetrieverTool\",\n          \"DynamicTool\",\n          \"Tool\",\n          \"StructuredTool\",\n          \"Runnable\"\n        ],\n        \"category\": \"Tools\",\n        \"description\": \"Use a retriever as allowed tool for agent\",\n        \"inputParams\": [\n          {\n            \"label\": \"Retriever Name\",\n            \"name\": \"name\",\n            \"type\": \"string\",\n            \"placeholder\": \"search_state_of_union\",\n            \"id\": \"retrieverTool_0-input-name-string\"\n          },\n          {\n            \"label\": \"Retriever Description\",\n            \"name\": \"description\",\n            \"type\": \"string\",\n            \"description\": \"When should agent uses to retrieve documents\",\n            \"rows\": 3,\n            \"placeholder\": \"Searches and returns documents regarding the state-of-the-union.\",\n            \"id\": \"retrieverTool_0-input-description-string\"\n          },\n          {\n            \"label\": \"Return Source Documents\",\n            \"name\": \"returnSourceDocuments\",\n            \"type\": \"boolean\",\n            \"optional\": true,\n            \"id\": \"retrieverTool_0-input-returnSourceDocuments-boolean\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Retriever\",\n            \"name\": \"retriever\",\n            \"type\": \"BaseRetriever\",\n            \"id\": \"retrieverTool_0-input-retriever-BaseRetriever\"\n          }\n        ],\n        \"inputs\": {\n          \"name\": \"\",\n          \"description\": \"\",\n          \"retriever\": \"{{faiss_0.data.instance}}\",\n          \"returnSourceDocuments\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"retrieverTool_0-output-retrieverTool-RetrieverTool|DynamicTool|Tool|StructuredTool|Runnable\",\n            \"name\": \"retrieverTool\",\n            \"label\": \"RetrieverTool\",\n            \"description\": \"Use a retriever as allowed tool for agent\",\n            \"type\": \"RetrieverTool | DynamicTool | Tool | StructuredTool | Runnable\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 601,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 204.54833182164043,\n        \"y\": -399.9178877515032\n      },\n      \"dragging\": false\n    }\n  ],\n  \"edges\": [\n    {\n      \"source\": \"groqChat_0\",\n      \"sourceHandle\": \"groqChat_0-output-groqChat-GroqChat|BaseChatModel|BaseLanguageModel|Runnable\",\n      \"target\": \"toolAgent_0\",\n      \"targetHandle\": \"toolAgent_0-input-model-BaseChatModel\",\n      \"type\": \"buttonedge\",\n      \"id\": \"groqChat_0-groqChat_0-output-groqChat-GroqChat|BaseChatModel|BaseLanguageModel|Runnable-toolAgent_0-toolAgent_0-input-model-BaseChatModel\"\n    },\n    {\n      \"source\": \"bufferMemory_0\",\n      \"sourceHandle\": \"bufferMemory_0-output-bufferMemory-BufferMemory|BaseChatMemory|BaseMemory\",\n      \"target\": \"toolAgent_0\",\n      \"targetHandle\": \"toolAgent_0-input-memory-BaseChatMemory\",\n      \"type\": \"buttonedge\",\n      \"id\": \"bufferMemory_0-bufferMemory_0-output-bufferMemory-BufferMemory|BaseChatMemory|BaseMemory-toolAgent_0-toolAgent_0-input-memory-BaseChatMemory\"\n    },\n    {\n      \"source\": \"recursiveCharacterTextSplitter_0\",\n      \"sourceHandle\": \"recursiveCharacterTextSplitter_0-output-recursiveCharacterTextSplitter-RecursiveCharacterTextSplitter|TextSplitter|BaseDocumentTransformer|Runnable\",\n      \"target\": \"docxFile_0\",\n      \"targetHandle\": \"docxFile_0-input-textSplitter-TextSplitter\",\n      \"type\": \"buttonedge\",\n      \"id\": \"recursiveCharacterTextSplitter_0-recursiveCharacterTextSplitter_0-output-recursiveCharacterTextSplitter-RecursiveCharacterTextSplitter|TextSplitter|BaseDocumentTransformer|Runnable-docxFile_0-docxFile_0-input-textSplitter-TextSplitter\"\n    },\n    {\n      \"source\": \"docxFile_0\",\n      \"sourceHandle\": \"docxFile_0-output-docxFile-Document\",\n      \"target\": \"faiss_0\",\n      \"targetHandle\": \"faiss_0-input-document-Document\",\n      \"type\": \"buttonedge\",\n      \"id\": \"docxFile_0-docxFile_0-output-docxFile-Document-faiss_0-faiss_0-input-document-Document\"\n    },\n    {\n      \"source\": \"voyageAIEmbeddings_0\",\n      \"sourceHandle\": \"voyageAIEmbeddings_0-output-voyageAIEmbeddings-VoyageAIEmbeddings|Embeddings\",\n      \"target\": \"faiss_0\",\n      \"targetHandle\": \"faiss_0-input-embeddings-Embeddings\",\n      \"type\": \"buttonedge\",\n      \"id\": \"voyageAIEmbeddings_0-voyageAIEmbeddings_0-output-voyageAIEmbeddings-VoyageAIEmbeddings|Embeddings-faiss_0-faiss_0-input-embeddings-Embeddings\"\n    },\n    {\n      \"source\": \"faiss_0\",\n      \"sourceHandle\": \"faiss_0-output-retriever-Faiss|VectorStoreRetriever|BaseRetriever\",\n      \"target\": \"retrieverTool_0\",\n      \"targetHandle\": \"retrieverTool_0-input-retriever-BaseRetriever\",\n      \"type\": \"buttonedge\",\n      \"id\": \"faiss_0-faiss_0-output-retriever-Faiss|VectorStoreRetriever|BaseRetriever-retrieverTool_0-retrieverTool_0-input-retriever-BaseRetriever\"\n    },\n    {\n      \"source\": \"retrieverTool_0\",\n      \"sourceHandle\": \"retrieverTool_0-output-retrieverTool-RetrieverTool|DynamicTool|Tool|StructuredTool|Runnable\",\n      \"target\": \"toolAgent_0\",\n      \"targetHandle\": \"toolAgent_0-input-tools-Tool\",\n      \"type\": \"buttonedge\",\n      \"id\": \"retrieverTool_0-retrieverTool_0-output-retrieverTool-RetrieverTool|DynamicTool|Tool|StructuredTool|Runnable-toolAgent_0-toolAgent_0-input-tools-Tool\"\n    }\n  ]\n}",
      "type": "CHATFLOW"
    },
    {
      "id": "405c5314-3e25-4319-951d-3a1718051e82",
      "name": "chatbotapp",
      "flowData": "{\n  \"nodes\": [\n    {\n      \"id\": \"bufferMemory_0\",\n      \"position\": {\n        \"x\": 644.649501951249,\n        \"y\": 357.62609002028546\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"bufferMemory_0\",\n        \"label\": \"Buffer Memory\",\n        \"version\": 2,\n        \"name\": \"bufferMemory\",\n        \"type\": \"BufferMemory\",\n        \"baseClasses\": [\n          \"BufferMemory\",\n          \"BaseChatMemory\",\n          \"BaseMemory\"\n        ],\n        \"category\": \"Memory\",\n        \"description\": \"Retrieve chat messages stored in database\",\n        \"inputParams\": [\n          {\n            \"label\": \"Session Id\",\n            \"name\": \"sessionId\",\n            \"type\": \"string\",\n            \"description\": \"If not specified, a random id will be used. Learn <a target=\\\"_blank\\\" href=\\\"https://docs.flowiseai.com/memory#ui-and-embedded-chat\\\">more</a>\",\n            \"default\": \"\",\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"id\": \"bufferMemory_0-input-sessionId-string\"\n          },\n          {\n            \"label\": \"Memory Key\",\n            \"name\": \"memoryKey\",\n            \"type\": \"string\",\n            \"default\": \"chat_history\",\n            \"additionalParams\": true,\n            \"id\": \"bufferMemory_0-input-memoryKey-string\"\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"sessionId\": \"\",\n          \"memoryKey\": \"chat_history\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"bufferMemory_0-output-bufferMemory-BufferMemory|BaseChatMemory|BaseMemory\",\n            \"name\": \"bufferMemory\",\n            \"label\": \"BufferMemory\",\n            \"description\": \"Retrieve chat messages stored in database\",\n            \"type\": \"BufferMemory | BaseChatMemory | BaseMemory\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 252,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 644.649501951249,\n        \"y\": 357.62609002028546\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"conversationChain_0\",\n      \"position\": {\n        \"x\": 1602.8712027040751,\n        \"y\": 207.2382538310188\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"conversationChain_0\",\n        \"label\": \"Conversation Chain\",\n        \"version\": 3,\n        \"name\": \"conversationChain\",\n        \"type\": \"ConversationChain\",\n        \"baseClasses\": [\n          \"ConversationChain\",\n          \"LLMChain\",\n          \"BaseChain\",\n          \"Runnable\"\n        ],\n        \"category\": \"Chains\",\n        \"description\": \"Chat models specific conversational chain with memory\",\n        \"inputParams\": [\n          {\n            \"label\": \"System Message\",\n            \"name\": \"systemMessagePrompt\",\n            \"type\": \"string\",\n            \"rows\": 4,\n            \"description\": \"If Chat Prompt Template is provided, this will be ignored\",\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"default\": \"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\",\n            \"placeholder\": \"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\",\n            \"id\": \"conversationChain_0-input-systemMessagePrompt-string\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Chat Model\",\n            \"name\": \"model\",\n            \"type\": \"BaseChatModel\",\n            \"id\": \"conversationChain_0-input-model-BaseChatModel\"\n          },\n          {\n            \"label\": \"Memory\",\n            \"name\": \"memory\",\n            \"type\": \"BaseMemory\",\n            \"id\": \"conversationChain_0-input-memory-BaseMemory\"\n          },\n          {\n            \"label\": \"Chat Prompt Template\",\n            \"name\": \"chatPromptTemplate\",\n            \"type\": \"ChatPromptTemplate\",\n            \"description\": \"Override existing prompt with Chat Prompt Template. Human Message must includes {input} variable\",\n            \"optional\": true,\n            \"id\": \"conversationChain_0-input-chatPromptTemplate-ChatPromptTemplate\"\n          },\n          {\n            \"label\": \"Input Moderation\",\n            \"description\": \"Detect text that could generate harmful output and prevent it from being sent to the language model\",\n            \"name\": \"inputModeration\",\n            \"type\": \"Moderation\",\n            \"optional\": true,\n            \"list\": true,\n            \"id\": \"conversationChain_0-input-inputModeration-Moderation\"\n          }\n        ],\n        \"inputs\": {\n          \"model\": \"{{chatGoogleGenerativeAI_0.data.instance}}\",\n          \"memory\": \"{{bufferMemory_0.data.instance}}\",\n          \"chatPromptTemplate\": \"\",\n          \"inputModeration\": \"\",\n          \"systemMessagePrompt\": \"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"conversationChain_0-output-conversationChain-ConversationChain|LLMChain|BaseChain|Runnable\",\n            \"name\": \"conversationChain\",\n            \"label\": \"ConversationChain\",\n            \"description\": \"Chat models specific conversational chain with memory\",\n            \"type\": \"ConversationChain | LLMChain | BaseChain | Runnable\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 434,\n      \"positionAbsolute\": {\n        \"x\": 1602.8712027040751,\n        \"y\": 207.2382538310188\n      },\n      \"selected\": false,\n      \"dragging\": false\n    },\n    {\n      \"id\": \"chatGoogleGenerativeAI_0\",\n      \"position\": {\n        \"x\": 1039.5084061718198,\n        \"y\": -486.0979688537318\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"chatGoogleGenerativeAI_0\",\n        \"label\": \"ChatGoogleGenerativeAI\",\n        \"version\": 3,\n        \"name\": \"chatGoogleGenerativeAI\",\n        \"type\": \"ChatGoogleGenerativeAI\",\n        \"baseClasses\": [\n          \"ChatGoogleGenerativeAI\",\n          \"LangchainChatGoogleGenerativeAI\",\n          \"BaseChatModel\",\n          \"BaseLanguageModel\",\n          \"Runnable\"\n        ],\n        \"category\": \"Chat Models\",\n        \"description\": \"Wrapper around Google Gemini large language models that use the Chat endpoint\",\n        \"inputParams\": [\n          {\n            \"label\": \"Connect Credential\",\n            \"name\": \"credential\",\n            \"type\": \"credential\",\n            \"credentialNames\": [\n              \"googleGenerativeAI\"\n            ],\n            \"optional\": false,\n            \"description\": \"Google Generative AI credential.\",\n            \"id\": \"chatGoogleGenerativeAI_0-input-credential-credential\"\n          },\n          {\n            \"label\": \"Model Name\",\n            \"name\": \"modelName\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"default\": \"gemini-1.5-flash-latest\",\n            \"id\": \"chatGoogleGenerativeAI_0-input-modelName-asyncOptions\"\n          },\n          {\n            \"label\": \"Custom Model Name\",\n            \"name\": \"customModelName\",\n            \"type\": \"string\",\n            \"placeholder\": \"gemini-1.5-pro-exp-0801\",\n            \"description\": \"Custom model name to use. If provided, it will override the model selected\",\n            \"additionalParams\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-customModelName-string\"\n          },\n          {\n            \"label\": \"Temperature\",\n            \"name\": \"temperature\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"default\": 0.9,\n            \"optional\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-temperature-number\"\n          },\n          {\n            \"label\": \"Streaming\",\n            \"name\": \"streaming\",\n            \"type\": \"boolean\",\n            \"default\": true,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-streaming-boolean\"\n          },\n          {\n            \"label\": \"Max Output Tokens\",\n            \"name\": \"maxOutputTokens\",\n            \"type\": \"number\",\n            \"step\": 1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-maxOutputTokens-number\"\n          },\n          {\n            \"label\": \"Top Probability\",\n            \"name\": \"topP\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-topP-number\"\n          },\n          {\n            \"label\": \"Top Next Highest Probability Tokens\",\n            \"name\": \"topK\",\n            \"type\": \"number\",\n            \"description\": \"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive\",\n            \"step\": 1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-topK-number\"\n          },\n          {\n            \"label\": \"Harm Category\",\n            \"name\": \"harmCategory\",\n            \"type\": \"multiOptions\",\n            \"description\": \"Refer to <a target=\\\"_blank\\\" href=\\\"https://cloud.google.com/vertex-ai/docs/generative-ai/multimodal/configure-safety-attributes#safety_attribute_definitions\\\">official guide</a> on how to use Harm Category\",\n            \"options\": [\n              {\n                \"label\": \"Dangerous\",\n                \"name\": \"HARM_CATEGORY_DANGEROUS_CONTENT\"\n              },\n              {\n                \"label\": \"Harassment\",\n                \"name\": \"HARM_CATEGORY_HARASSMENT\"\n              },\n              {\n                \"label\": \"Hate Speech\",\n                \"name\": \"HARM_CATEGORY_HATE_SPEECH\"\n              },\n              {\n                \"label\": \"Sexually Explicit\",\n                \"name\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\"\n              }\n            ],\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-harmCategory-multiOptions\"\n          },\n          {\n            \"label\": \"Harm Block Threshold\",\n            \"name\": \"harmBlockThreshold\",\n            \"type\": \"multiOptions\",\n            \"description\": \"Refer to <a target=\\\"_blank\\\" href=\\\"https://cloud.google.com/vertex-ai/docs/generative-ai/multimodal/configure-safety-attributes#safety_setting_thresholds\\\">official guide</a> on how to use Harm Block Threshold\",\n            \"options\": [\n              {\n                \"label\": \"Low and Above\",\n                \"name\": \"BLOCK_LOW_AND_ABOVE\"\n              },\n              {\n                \"label\": \"Medium and Above\",\n                \"name\": \"BLOCK_MEDIUM_AND_ABOVE\"\n              },\n              {\n                \"label\": \"None\",\n                \"name\": \"BLOCK_NONE\"\n              },\n              {\n                \"label\": \"Only High\",\n                \"name\": \"BLOCK_ONLY_HIGH\"\n              },\n              {\n                \"label\": \"Threshold Unspecified\",\n                \"name\": \"HARM_BLOCK_THRESHOLD_UNSPECIFIED\"\n              }\n            ],\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-harmBlockThreshold-multiOptions\"\n          },\n          {\n            \"label\": \"Allow Image Uploads\",\n            \"name\": \"allowImageUploads\",\n            \"type\": \"boolean\",\n            \"description\": \"Allow image input. Refer to the <a href=\\\"https://docs.flowiseai.com/using-flowise/uploads#image\\\" target=\\\"_blank\\\">docs</a> for more details.\",\n            \"default\": false,\n            \"optional\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-allowImageUploads-boolean\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Cache\",\n            \"name\": \"cache\",\n            \"type\": \"BaseCache\",\n            \"optional\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-cache-BaseCache\"\n          }\n        ],\n        \"inputs\": {\n          \"cache\": \"\",\n          \"modelName\": \"gemini-2.0-flash-001\",\n          \"customModelName\": \"\",\n          \"temperature\": 0.9,\n          \"streaming\": true,\n          \"maxOutputTokens\": \"\",\n          \"topP\": \"\",\n          \"topK\": \"\",\n          \"harmCategory\": \"\",\n          \"harmBlockThreshold\": \"\",\n          \"allowImageUploads\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"chatGoogleGenerativeAI_0-output-chatGoogleGenerativeAI-ChatGoogleGenerativeAI|LangchainChatGoogleGenerativeAI|BaseChatModel|BaseLanguageModel|Runnable\",\n            \"name\": \"chatGoogleGenerativeAI\",\n            \"label\": \"ChatGoogleGenerativeAI\",\n            \"description\": \"Wrapper around Google Gemini large language models that use the Chat endpoint\",\n            \"type\": \"ChatGoogleGenerativeAI | LangchainChatGoogleGenerativeAI | BaseChatModel | BaseLanguageModel | Runnable\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 669,\n      \"positionAbsolute\": {\n        \"x\": 1039.5084061718198,\n        \"y\": -486.0979688537318\n      },\n      \"selected\": false,\n      \"dragging\": false\n    }\n  ],\n  \"edges\": [\n    {\n      \"source\": \"bufferMemory_0\",\n      \"sourceHandle\": \"bufferMemory_0-output-bufferMemory-BufferMemory|BaseChatMemory|BaseMemory\",\n      \"target\": \"conversationChain_0\",\n      \"targetHandle\": \"conversationChain_0-input-memory-BaseMemory\",\n      \"type\": \"buttonedge\",\n      \"id\": \"bufferMemory_0-bufferMemory_0-output-bufferMemory-BufferMemory|BaseChatMemory|BaseMemory-conversationChain_0-conversationChain_0-input-memory-BaseMemory\"\n    },\n    {\n      \"source\": \"chatGoogleGenerativeAI_0\",\n      \"sourceHandle\": \"chatGoogleGenerativeAI_0-output-chatGoogleGenerativeAI-ChatGoogleGenerativeAI|LangchainChatGoogleGenerativeAI|BaseChatModel|BaseLanguageModel|Runnable\",\n      \"target\": \"conversationChain_0\",\n      \"targetHandle\": \"conversationChain_0-input-model-BaseChatModel\",\n      \"type\": \"buttonedge\",\n      \"id\": \"chatGoogleGenerativeAI_0-chatGoogleGenerativeAI_0-output-chatGoogleGenerativeAI-ChatGoogleGenerativeAI|LangchainChatGoogleGenerativeAI|BaseChatModel|BaseLanguageModel|Runnable-conversationChain_0-conversationChain_0-input-model-BaseChatModel\"\n    }\n  ]\n}",
      "type": "CHATFLOW"
    },
    {
      "id": "4bc57609-9d16-4d01-994f-9ff48eb0f59d",
      "name": "pdfretriverusingmistralai",
      "flowData": "{\n  \"nodes\": [\n    {\n      \"width\": 300,\n      \"height\": 421,\n      \"id\": \"openAIEmbeddings_0\",\n      \"position\": {\n        \"x\": 598.4691381182121,\n        \"y\": 614.7222554103136\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"openAIEmbeddings_0\",\n        \"label\": \"OpenAI Embeddings\",\n        \"version\": 4,\n        \"name\": \"openAIEmbeddings\",\n        \"type\": \"OpenAIEmbeddings\",\n        \"baseClasses\": [\n          \"OpenAIEmbeddings\",\n          \"Embeddings\"\n        ],\n        \"category\": \"Embeddings\",\n        \"description\": \"OpenAI API to generate embeddings for a given text\",\n        \"inputParams\": [\n          {\n            \"label\": \"Connect Credential\",\n            \"name\": \"credential\",\n            \"type\": \"credential\",\n            \"credentialNames\": [\n              \"openAIApi\"\n            ],\n            \"id\": \"openAIEmbeddings_0-input-credential-credential\"\n          },\n          {\n            \"label\": \"Model Name\",\n            \"name\": \"modelName\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"default\": \"text-embedding-ada-002\",\n            \"id\": \"openAIEmbeddings_0-input-modelName-asyncOptions\"\n          },\n          {\n            \"label\": \"Strip New Lines\",\n            \"name\": \"stripNewLines\",\n            \"type\": \"boolean\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"openAIEmbeddings_0-input-stripNewLines-boolean\"\n          },\n          {\n            \"label\": \"Batch Size\",\n            \"name\": \"batchSize\",\n            \"type\": \"number\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"openAIEmbeddings_0-input-batchSize-number\"\n          },\n          {\n            \"label\": \"Timeout\",\n            \"name\": \"timeout\",\n            \"type\": \"number\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"openAIEmbeddings_0-input-timeout-number\"\n          },\n          {\n            \"label\": \"BasePath\",\n            \"name\": \"basepath\",\n            \"type\": \"string\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"openAIEmbeddings_0-input-basepath-string\"\n          },\n          {\n            \"label\": \"Dimensions\",\n            \"name\": \"dimensions\",\n            \"type\": \"number\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"openAIEmbeddings_0-input-dimensions-number\"\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"modelName\": \"text-embedding-ada-002\",\n          \"stripNewLines\": \"\",\n          \"batchSize\": \"\",\n          \"timeout\": \"\",\n          \"basepath\": \"\",\n          \"dimensions\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"openAIEmbeddings_0-output-openAIEmbeddings-OpenAIEmbeddings|Embeddings\",\n            \"name\": \"openAIEmbeddings\",\n            \"label\": \"OpenAIEmbeddings\",\n            \"description\": \"OpenAI API to generate embeddings for a given text\",\n            \"type\": \"OpenAIEmbeddings | Embeddings\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 598.4691381182121,\n        \"y\": 614.7222554103136\n      },\n      \"dragging\": false\n    },\n    {\n      \"width\": 300,\n      \"height\": 427,\n      \"id\": \"recursiveCharacterTextSplitter_0\",\n      \"position\": {\n        \"x\": 124.11835209314779,\n        \"y\": 151.81643824918933\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"recursiveCharacterTextSplitter_0\",\n        \"label\": \"Recursive Character Text Splitter\",\n        \"version\": 2,\n        \"name\": \"recursiveCharacterTextSplitter\",\n        \"type\": \"RecursiveCharacterTextSplitter\",\n        \"baseClasses\": [\n          \"RecursiveCharacterTextSplitter\",\n          \"TextSplitter\"\n        ],\n        \"category\": \"Text Splitters\",\n        \"description\": \"Split documents recursively by different characters - starting with \\\"\\\\n\\\\n\\\", then \\\"\\\\n\\\", then \\\" \\\"\",\n        \"inputParams\": [\n          {\n            \"label\": \"Chunk Size\",\n            \"name\": \"chunkSize\",\n            \"type\": \"number\",\n            \"default\": 1000,\n            \"optional\": true,\n            \"id\": \"recursiveCharacterTextSplitter_0-input-chunkSize-number\"\n          },\n          {\n            \"label\": \"Chunk Overlap\",\n            \"name\": \"chunkOverlap\",\n            \"type\": \"number\",\n            \"optional\": true,\n            \"id\": \"recursiveCharacterTextSplitter_0-input-chunkOverlap-number\"\n          },\n          {\n            \"label\": \"Custom Separators\",\n            \"name\": \"separators\",\n            \"type\": \"string\",\n            \"rows\": 4,\n            \"description\": \"Array of custom separators to determine when to split the text, will override the default separators\",\n            \"placeholder\": \"[\\\"|\\\", \\\"##\\\", \\\">\\\", \\\"-\\\"]\",\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"id\": \"recursiveCharacterTextSplitter_0-input-separators-string\"\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"chunkSize\": \"500\",\n          \"chunkOverlap\": \"150\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"recursiveCharacterTextSplitter_0-output-recursiveCharacterTextSplitter-RecursiveCharacterTextSplitter|TextSplitter\",\n            \"name\": \"recursiveCharacterTextSplitter\",\n            \"label\": \"RecursiveCharacterTextSplitter\",\n            \"type\": \"RecursiveCharacterTextSplitter | TextSplitter\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 124.11835209314779,\n        \"y\": 151.81643824918933\n      },\n      \"dragging\": false\n    },\n    {\n      \"width\": 300,\n      \"height\": 529,\n      \"id\": \"conversationalRetrievalQAChain_0\",\n      \"position\": {\n        \"x\": 1558.6564094656787,\n        \"y\": 386.60217819991124\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"conversationalRetrievalQAChain_0\",\n        \"label\": \"Conversational Retrieval QA Chain\",\n        \"version\": 3,\n        \"name\": \"conversationalRetrievalQAChain\",\n        \"type\": \"ConversationalRetrievalQAChain\",\n        \"baseClasses\": [\n          \"ConversationalRetrievalQAChain\",\n          \"BaseChain\",\n          \"Runnable\"\n        ],\n        \"category\": \"Chains\",\n        \"description\": \"Document QA - built on RetrievalQAChain to provide a chat history component\",\n        \"inputParams\": [\n          {\n            \"label\": \"Return Source Documents\",\n            \"name\": \"returnSourceDocuments\",\n            \"type\": \"boolean\",\n            \"optional\": true,\n            \"id\": \"conversationalRetrievalQAChain_0-input-returnSourceDocuments-boolean\"\n          },\n          {\n            \"label\": \"Rephrase Prompt\",\n            \"name\": \"rephrasePrompt\",\n            \"type\": \"string\",\n            \"description\": \"Using previous chat history, rephrase question into a standalone question\",\n            \"warning\": \"Prompt must include input variables: {chat_history} and {question}\",\n            \"rows\": 4,\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"default\": \"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone Question:\",\n            \"id\": \"conversationalRetrievalQAChain_0-input-rephrasePrompt-string\"\n          },\n          {\n            \"label\": \"Response Prompt\",\n            \"name\": \"responsePrompt\",\n            \"type\": \"string\",\n            \"description\": \"Taking the rephrased question, search for answer from the provided context\",\n            \"warning\": \"Prompt must include input variable: {context}\",\n            \"rows\": 4,\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"default\": \"You are a helpful assistant. Using the provided context, answer the user's question to the best of your ability using the resources provided.\\nIf there is nothing in the context relevant to the question at hand, just say \\\"Hmm, I'm not sure.\\\" Don't try to make up an answer.\\n------------\\n{context}\\n------------\\nREMEMBER: If there is no relevant information within the context, just say \\\"Hmm, I'm not sure.\\\" Don't try to make up an answer.\",\n            \"id\": \"conversationalRetrievalQAChain_0-input-responsePrompt-string\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Chat Model\",\n            \"name\": \"model\",\n            \"type\": \"BaseChatModel\",\n            \"id\": \"conversationalRetrievalQAChain_0-input-model-BaseChatModel\"\n          },\n          {\n            \"label\": \"Vector Store Retriever\",\n            \"name\": \"vectorStoreRetriever\",\n            \"type\": \"BaseRetriever\",\n            \"id\": \"conversationalRetrievalQAChain_0-input-vectorStoreRetriever-BaseRetriever\"\n          },\n          {\n            \"label\": \"Memory\",\n            \"name\": \"memory\",\n            \"type\": \"BaseMemory\",\n            \"optional\": true,\n            \"description\": \"If left empty, a default BufferMemory will be used\",\n            \"id\": \"conversationalRetrievalQAChain_0-input-memory-BaseMemory\"\n          },\n          {\n            \"label\": \"Input Moderation\",\n            \"description\": \"Detect text that could generate harmful output and prevent it from being sent to the language model\",\n            \"name\": \"inputModeration\",\n            \"type\": \"Moderation\",\n            \"optional\": true,\n            \"list\": true,\n            \"id\": \"conversationalRetrievalQAChain_0-input-inputModeration-Moderation\"\n          }\n        ],\n        \"inputs\": {\n          \"inputModeration\": \"\",\n          \"model\": \"{{chatMistralAI_0.data.instance}}\",\n          \"vectorStoreRetriever\": \"{{faiss_0.data.instance}}\",\n          \"memory\": \"{{bufferMemory_0.data.instance}}\",\n          \"rephrasePrompt\": \"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone Question:\",\n          \"responsePrompt\": \"You are a helpful assistant. Using the provided context, answer the user's question to the best of your ability using the resources provided.\\nIf there is nothing in the context relevant to the question at hand, just say \\\"Hmm, I'm not sure.\\\" Don't try to make up an answer.\\n------------\\n{context}\\n------------\\nREMEMBER: If there is no relevant information within the context, just say \\\"Hmm, I'm not sure.\\\" Don't try to make up an answer.\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"conversationalRetrievalQAChain_0-output-conversationalRetrievalQAChain-ConversationalRetrievalQAChain|BaseChain|Runnable\",\n            \"name\": \"conversationalRetrievalQAChain\",\n            \"label\": \"ConversationalRetrievalQAChain\",\n            \"type\": \"ConversationalRetrievalQAChain | BaseChain | Runnable\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"positionAbsolute\": {\n        \"x\": 1558.6564094656787,\n        \"y\": 386.60217819991124\n      },\n      \"selected\": false,\n      \"dragging\": false\n    },\n    {\n      \"id\": \"faiss_0\",\n      \"position\": {\n        \"x\": 1026.272061488701,\n        \"y\": 540.7157860306309\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"faiss_0\",\n        \"label\": \"Faiss\",\n        \"version\": 1,\n        \"name\": \"faiss\",\n        \"type\": \"Faiss\",\n        \"baseClasses\": [\n          \"Faiss\",\n          \"VectorStoreRetriever\",\n          \"BaseRetriever\"\n        ],\n        \"category\": \"Vector Stores\",\n        \"description\": \"Upsert embedded data and perform similarity search upon query using Faiss library from Meta\",\n        \"inputParams\": [\n          {\n            \"label\": \"Base Path to load\",\n            \"name\": \"basePath\",\n            \"description\": \"Path to load faiss.index file\",\n            \"placeholder\": \"C:\\\\Users\\\\User\\\\Desktop\",\n            \"type\": \"string\",\n            \"id\": \"faiss_0-input-basePath-string\"\n          },\n          {\n            \"label\": \"Top K\",\n            \"name\": \"topK\",\n            \"description\": \"Number of top results to fetch. Default to 4\",\n            \"placeholder\": \"4\",\n            \"type\": \"number\",\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"id\": \"faiss_0-input-topK-number\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Document\",\n            \"name\": \"document\",\n            \"type\": \"Document\",\n            \"list\": true,\n            \"optional\": true,\n            \"id\": \"faiss_0-input-document-Document\"\n          },\n          {\n            \"label\": \"Embeddings\",\n            \"name\": \"embeddings\",\n            \"type\": \"Embeddings\",\n            \"id\": \"faiss_0-input-embeddings-Embeddings\"\n          }\n        ],\n        \"inputs\": {\n          \"document\": [\n            \"{{documentStore_0.data.instance}}\",\n            \"{{pdfFile_0.data.instance}}\"\n          ],\n          \"embeddings\": \"{{openAIEmbeddings_0.data.instance}}\",\n          \"basePath\": \"/opt/render/.flowise/1177\",\n          \"topK\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"name\": \"output\",\n            \"label\": \"Output\",\n            \"type\": \"options\",\n            \"description\": \"\",\n            \"options\": [\n              {\n                \"id\": \"faiss_0-output-retriever-Faiss|VectorStoreRetriever|BaseRetriever\",\n                \"name\": \"retriever\",\n                \"label\": \"Faiss Retriever\",\n                \"description\": \"\",\n                \"type\": \"Faiss | VectorStoreRetriever | BaseRetriever\"\n              },\n              {\n                \"id\": \"faiss_0-output-vectorStore-Faiss|SaveableVectorStore|VectorStore\",\n                \"name\": \"vectorStore\",\n                \"label\": \"Faiss Vector Store\",\n                \"description\": \"\",\n                \"type\": \"Faiss | SaveableVectorStore | VectorStore\"\n              }\n            ],\n            \"default\": \"retriever\"\n          }\n        ],\n        \"outputs\": {\n          \"output\": \"retriever\"\n        },\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 456,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 1026.272061488701,\n        \"y\": 540.7157860306309\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"documentStore_0\",\n      \"position\": {\n        \"x\": 826.5653750371686,\n        \"y\": -229.4786988814253\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"documentStore_0\",\n        \"label\": \"Document Store\",\n        \"version\": 1,\n        \"name\": \"documentStore\",\n        \"type\": \"Document\",\n        \"baseClasses\": [\n          \"Document\"\n        ],\n        \"category\": \"Document Loaders\",\n        \"description\": \"Load data from pre-configured document stores\",\n        \"inputParams\": [\n          {\n            \"label\": \"Select Store\",\n            \"name\": \"selectedStore\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listStores\",\n            \"id\": \"documentStore_0-input-selectedStore-asyncOptions\"\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"selectedStore\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"name\": \"output\",\n            \"label\": \"Output\",\n            \"type\": \"options\",\n            \"description\": \"Array of document objects containing metadata and pageContent\",\n            \"options\": [\n              {\n                \"id\": \"documentStore_0-output-document-Document|json\",\n                \"name\": \"document\",\n                \"label\": \"Document\",\n                \"description\": \"Array of document objects containing metadata and pageContent\",\n                \"type\": \"Document | json\"\n              },\n              {\n                \"id\": \"documentStore_0-output-text-string|json\",\n                \"name\": \"text\",\n                \"label\": \"Text\",\n                \"description\": \"Concatenated string from pageContent of documents\",\n                \"type\": \"string | json\"\n              }\n            ],\n            \"default\": \"document\"\n          }\n        ],\n        \"outputs\": {\n          \"output\": \"document\"\n        },\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 310,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 826.5653750371686,\n        \"y\": -229.4786988814253\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"chatMistralAI_0\",\n      \"position\": {\n        \"x\": 1185.9624817228073,\n        \"y\": -60.75719138037451\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"chatMistralAI_0\",\n        \"label\": \"ChatMistralAI\",\n        \"version\": 3,\n        \"name\": \"chatMistralAI\",\n        \"type\": \"ChatMistralAI\",\n        \"baseClasses\": [\n          \"ChatMistralAI\",\n          \"BaseChatModel\",\n          \"BaseLanguageModel\",\n          \"Runnable\"\n        ],\n        \"category\": \"Chat Models\",\n        \"description\": \"Wrapper around Mistral large language models that use the Chat endpoint\",\n        \"inputParams\": [\n          {\n            \"label\": \"Connect Credential\",\n            \"name\": \"credential\",\n            \"type\": \"credential\",\n            \"credentialNames\": [\n              \"mistralAIApi\"\n            ],\n            \"id\": \"chatMistralAI_0-input-credential-credential\"\n          },\n          {\n            \"label\": \"Model Name\",\n            \"name\": \"modelName\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"default\": \"mistral-tiny\",\n            \"id\": \"chatMistralAI_0-input-modelName-asyncOptions\"\n          },\n          {\n            \"label\": \"Temperature\",\n            \"name\": \"temperature\",\n            \"type\": \"number\",\n            \"description\": \"What sampling temperature to use, between 0.0 and 1.0. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\",\n            \"step\": 0.1,\n            \"default\": 0.9,\n            \"optional\": true,\n            \"id\": \"chatMistralAI_0-input-temperature-number\"\n          },\n          {\n            \"label\": \"Max Output Tokens\",\n            \"name\": \"maxOutputTokens\",\n            \"type\": \"number\",\n            \"description\": \"The maximum number of tokens to generate in the completion.\",\n            \"step\": 1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatMistralAI_0-input-maxOutputTokens-number\"\n          },\n          {\n            \"label\": \"Top Probability\",\n            \"name\": \"topP\",\n            \"type\": \"number\",\n            \"description\": \"Nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\",\n            \"step\": 0.1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatMistralAI_0-input-topP-number\"\n          },\n          {\n            \"label\": \"Random Seed\",\n            \"name\": \"randomSeed\",\n            \"type\": \"number\",\n            \"description\": \"The seed to use for random sampling. If set, different calls will generate deterministic results.\",\n            \"step\": 1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatMistralAI_0-input-randomSeed-number\"\n          },\n          {\n            \"label\": \"Safe Mode\",\n            \"name\": \"safeMode\",\n            \"type\": \"boolean\",\n            \"description\": \"Whether to inject a safety prompt before all conversations.\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatMistralAI_0-input-safeMode-boolean\"\n          },\n          {\n            \"label\": \"Override Endpoint\",\n            \"name\": \"overrideEndpoint\",\n            \"type\": \"string\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatMistralAI_0-input-overrideEndpoint-string\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Cache\",\n            \"name\": \"cache\",\n            \"type\": \"BaseCache\",\n            \"optional\": true,\n            \"id\": \"chatMistralAI_0-input-cache-BaseCache\"\n          }\n        ],\n        \"inputs\": {\n          \"cache\": \"\",\n          \"modelName\": \"open-mistral-7b\",\n          \"temperature\": \"0.7\",\n          \"maxOutputTokens\": \"\",\n          \"topP\": \"\",\n          \"randomSeed\": \"\",\n          \"safeMode\": \"\",\n          \"overrideEndpoint\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"chatMistralAI_0-output-chatMistralAI-ChatMistralAI|BaseChatModel|BaseLanguageModel|Runnable\",\n            \"name\": \"chatMistralAI\",\n            \"label\": \"ChatMistralAI\",\n            \"description\": \"Wrapper around Mistral large language models that use the Chat endpoint\",\n            \"type\": \"ChatMistralAI | BaseChatModel | BaseLanguageModel | Runnable\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 571,\n      \"positionAbsolute\": {\n        \"x\": 1185.9624817228073,\n        \"y\": -60.75719138037451\n      },\n      \"selected\": false,\n      \"dragging\": false\n    },\n    {\n      \"id\": \"bufferMemory_0\",\n      \"position\": {\n        \"x\": 1511.4347003706323,\n        \"y\": 45.4357477285582\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"bufferMemory_0\",\n        \"label\": \"Buffer Memory\",\n        \"version\": 2,\n        \"name\": \"bufferMemory\",\n        \"type\": \"BufferMemory\",\n        \"baseClasses\": [\n          \"BufferMemory\",\n          \"BaseChatMemory\",\n          \"BaseMemory\"\n        ],\n        \"category\": \"Memory\",\n        \"description\": \"Retrieve chat messages stored in database\",\n        \"inputParams\": [\n          {\n            \"label\": \"Session Id\",\n            \"name\": \"sessionId\",\n            \"type\": \"string\",\n            \"description\": \"If not specified, a random id will be used. Learn <a target=\\\"_blank\\\" href=\\\"https://docs.flowiseai.com/memory#ui-and-embedded-chat\\\">more</a>\",\n            \"default\": \"\",\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"id\": \"bufferMemory_0-input-sessionId-string\"\n          },\n          {\n            \"label\": \"Memory Key\",\n            \"name\": \"memoryKey\",\n            \"type\": \"string\",\n            \"default\": \"chat_history\",\n            \"additionalParams\": true,\n            \"id\": \"bufferMemory_0-input-memoryKey-string\"\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"sessionId\": \"\",\n          \"memoryKey\": \"chat_history\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"bufferMemory_0-output-bufferMemory-BufferMemory|BaseChatMemory|BaseMemory\",\n            \"name\": \"bufferMemory\",\n            \"label\": \"BufferMemory\",\n            \"description\": \"Retrieve chat messages stored in database\",\n            \"type\": \"BufferMemory | BaseChatMemory | BaseMemory\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 250,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 1511.4347003706323,\n        \"y\": 45.4357477285582\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"pdfFile_0\",\n      \"position\": {\n        \"x\": 489.91103540268375,\n        \"y\": 14.39872090193586\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"pdfFile_0\",\n        \"label\": \"Pdf File\",\n        \"version\": 1,\n        \"name\": \"pdfFile\",\n        \"type\": \"Document\",\n        \"baseClasses\": [\n          \"Document\"\n        ],\n        \"category\": \"Document Loaders\",\n        \"description\": \"Load data from PDF files\",\n        \"inputParams\": [\n          {\n            \"label\": \"Pdf File\",\n            \"name\": \"pdfFile\",\n            \"type\": \"file\",\n            \"fileType\": \".pdf\",\n            \"id\": \"pdfFile_0-input-pdfFile-file\"\n          },\n          {\n            \"label\": \"Usage\",\n            \"name\": \"usage\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"One document per page\",\n                \"name\": \"perPage\"\n              },\n              {\n                \"label\": \"One document per file\",\n                \"name\": \"perFile\"\n              }\n            ],\n            \"default\": \"perPage\",\n            \"id\": \"pdfFile_0-input-usage-options\"\n          },\n          {\n            \"label\": \"Use Legacy Build\",\n            \"name\": \"legacyBuild\",\n            \"type\": \"boolean\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"pdfFile_0-input-legacyBuild-boolean\"\n          },\n          {\n            \"label\": \"Additional Metadata\",\n            \"name\": \"metadata\",\n            \"type\": \"json\",\n            \"description\": \"Additional metadata to be added to the extracted documents\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"pdfFile_0-input-metadata-json\"\n          },\n          {\n            \"label\": \"Omit Metadata Keys\",\n            \"name\": \"omitMetadataKeys\",\n            \"type\": \"string\",\n            \"rows\": 4,\n            \"description\": \"Each document loader comes with a default set of metadata keys that are extracted from the document. You can use this field to omit some of the default metadata keys. The value should be a list of keys, seperated by comma. Use * to omit all metadata keys execept the ones you specify in the Additional Metadata field\",\n            \"placeholder\": \"key1, key2, key3.nestedKey1\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"pdfFile_0-input-omitMetadataKeys-string\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Text Splitter\",\n            \"name\": \"textSplitter\",\n            \"type\": \"TextSplitter\",\n            \"optional\": true,\n            \"id\": \"pdfFile_0-input-textSplitter-TextSplitter\"\n          }\n        ],\n        \"inputs\": {\n          \"textSplitter\": \"{{recursiveCharacterTextSplitter_0.data.instance}}\",\n          \"usage\": \"perPage\",\n          \"legacyBuild\": \"\",\n          \"metadata\": \"\",\n          \"omitMetadataKeys\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"pdfFile_0-output-pdfFile-Document\",\n            \"name\": \"pdfFile\",\n            \"label\": \"Document\",\n            \"description\": \"Load data from PDF files\",\n            \"type\": \"Document\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 524,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 489.91103540268375,\n        \"y\": 14.39872090193586\n      },\n      \"dragging\": false\n    }\n  ],\n  \"edges\": [\n    {\n      \"source\": \"openAIEmbeddings_0\",\n      \"sourceHandle\": \"openAIEmbeddings_0-output-openAIEmbeddings-OpenAIEmbeddings|Embeddings\",\n      \"target\": \"faiss_0\",\n      \"targetHandle\": \"faiss_0-input-embeddings-Embeddings\",\n      \"type\": \"buttonedge\",\n      \"id\": \"openAIEmbeddings_0-openAIEmbeddings_0-output-openAIEmbeddings-OpenAIEmbeddings|Embeddings-faiss_0-faiss_0-input-embeddings-Embeddings\"\n    },\n    {\n      \"source\": \"documentStore_0\",\n      \"sourceHandle\": \"documentStore_0-output-document-Document|json\",\n      \"target\": \"faiss_0\",\n      \"targetHandle\": \"faiss_0-input-document-Document\",\n      \"type\": \"buttonedge\",\n      \"id\": \"documentStore_0-documentStore_0-output-document-Document|json-faiss_0-faiss_0-input-document-Document\"\n    },\n    {\n      \"source\": \"faiss_0\",\n      \"sourceHandle\": \"faiss_0-output-retriever-Faiss|VectorStoreRetriever|BaseRetriever\",\n      \"target\": \"conversationalRetrievalQAChain_0\",\n      \"targetHandle\": \"conversationalRetrievalQAChain_0-input-vectorStoreRetriever-BaseRetriever\",\n      \"type\": \"buttonedge\",\n      \"id\": \"faiss_0-faiss_0-output-retriever-Faiss|VectorStoreRetriever|BaseRetriever-conversationalRetrievalQAChain_0-conversationalRetrievalQAChain_0-input-vectorStoreRetriever-BaseRetriever\"\n    },\n    {\n      \"source\": \"chatMistralAI_0\",\n      \"sourceHandle\": \"chatMistralAI_0-output-chatMistralAI-ChatMistralAI|BaseChatModel|BaseLanguageModel|Runnable\",\n      \"target\": \"conversationalRetrievalQAChain_0\",\n      \"targetHandle\": \"conversationalRetrievalQAChain_0-input-model-BaseChatModel\",\n      \"type\": \"buttonedge\",\n      \"id\": \"chatMistralAI_0-chatMistralAI_0-output-chatMistralAI-ChatMistralAI|BaseChatModel|BaseLanguageModel|Runnable-conversationalRetrievalQAChain_0-conversationalRetrievalQAChain_0-input-model-BaseChatModel\"\n    },\n    {\n      \"source\": \"bufferMemory_0\",\n      \"sourceHandle\": \"bufferMemory_0-output-bufferMemory-BufferMemory|BaseChatMemory|BaseMemory\",\n      \"target\": \"conversationalRetrievalQAChain_0\",\n      \"targetHandle\": \"conversationalRetrievalQAChain_0-input-memory-BaseMemory\",\n      \"type\": \"buttonedge\",\n      \"id\": \"bufferMemory_0-bufferMemory_0-output-bufferMemory-BufferMemory|BaseChatMemory|BaseMemory-conversationalRetrievalQAChain_0-conversationalRetrievalQAChain_0-input-memory-BaseMemory\"\n    },\n    {\n      \"source\": \"recursiveCharacterTextSplitter_0\",\n      \"sourceHandle\": \"recursiveCharacterTextSplitter_0-output-recursiveCharacterTextSplitter-RecursiveCharacterTextSplitter|TextSplitter\",\n      \"target\": \"pdfFile_0\",\n      \"targetHandle\": \"pdfFile_0-input-textSplitter-TextSplitter\",\n      \"type\": \"buttonedge\",\n      \"id\": \"recursiveCharacterTextSplitter_0-recursiveCharacterTextSplitter_0-output-recursiveCharacterTextSplitter-RecursiveCharacterTextSplitter|TextSplitter-pdfFile_0-pdfFile_0-input-textSplitter-TextSplitter\"\n    },\n    {\n      \"source\": \"pdfFile_0\",\n      \"sourceHandle\": \"pdfFile_0-output-pdfFile-Document\",\n      \"target\": \"faiss_0\",\n      \"targetHandle\": \"faiss_0-input-document-Document\",\n      \"type\": \"buttonedge\",\n      \"id\": \"pdfFile_0-pdfFile_0-output-pdfFile-Document-faiss_0-faiss_0-input-document-Document\"\n    }\n  ]\n}",
      "type": "CHATFLOW"
    },
    {
      "id": "73c50ae8-16ec-44d0-bbbf-ed2dbcd199db",
      "name": "databaseretriver",
      "flowData": "{\n  \"nodes\": [\n    {\n      \"id\": \"customFunction_0\",\n      \"position\": {\n        \"x\": 815.8106338990393,\n        \"y\": -1586.9699386384095\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"customFunction_0\",\n        \"label\": \"Custom JS Function\",\n        \"version\": 3,\n        \"name\": \"customFunction\",\n        \"type\": \"CustomFunction\",\n        \"baseClasses\": [\n          \"CustomFunction\",\n          \"Utilities\"\n        ],\n        \"tags\": [\n          \"Utilities\"\n        ],\n        \"category\": \"Utilities\",\n        \"description\": \"Execute custom javascript function\",\n        \"inputParams\": [\n          {\n            \"label\": \"Input Variables\",\n            \"name\": \"functionInputVariables\",\n            \"description\": \"Input variables can be used in the function with prefix $. For example: $var\",\n            \"type\": \"json\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"list\": true,\n            \"id\": \"customFunction_0-input-functionInputVariables-json\"\n          },\n          {\n            \"label\": \"Function Name\",\n            \"name\": \"functionName\",\n            \"type\": \"string\",\n            \"optional\": true,\n            \"placeholder\": \"My Function\",\n            \"id\": \"customFunction_0-input-functionName-string\"\n          },\n          {\n            \"label\": \"Javascript Function\",\n            \"name\": \"javascriptFunction\",\n            \"type\": \"code\",\n            \"id\": \"customFunction_0-input-javascriptFunction-code\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Additional Tools\",\n            \"description\": \"Tools can be used in the function with $tools.{tool_name}.invoke(args)\",\n            \"name\": \"tools\",\n            \"type\": \"Tool\",\n            \"list\": true,\n            \"optional\": true,\n            \"id\": \"customFunction_0-input-tools-Tool\"\n          }\n        ],\n        \"inputs\": {\n          \"functionInputVariables\": \"\",\n          \"functionName\": \"\",\n          \"tools\": \"\",\n          \"javascriptFunction\": \"const HOST = 'localhost';\\nconst USER = 'postgres';\\nconst PASSWORD = '0912';\\nconst DATABASE = 'newdata';\\nconst TABLE = 'newdata';\\nconst { Pool } = require('pg'); // Import the pg module for PostgreSQL\\n\\nlet sqlSchemaPrompt;\\n\\nasync function getSQLPrompt() {\\n  try {\\n    const pool = new Pool({\\n      host: HOST,\\n      user: USER,\\n      password: PASSWORD,\\n      database: DATABASE,\\n    });\\n\\n    // Get schema info\\n    const schemaQuery = `SELECT column_name, data_type, is_nullable \\n                         FROM information_schema.columns \\n                         WHERE table_name = $1`;\\n\\n    const schemaResult = await pool.query(schemaQuery, [TABLE]);\\n    const createColumns = [];\\n    const columnNames = [];\\n\\n    for (const schemaData of schemaResult.rows) {\\n      columnNames.push(schemaData.column_name);\\n      createColumns.push(`${schemaData.column_name} ${schemaData.data_type} ${schemaData.is_nullable === 'NO' ? 'NOT NULL' : ''}`);\\n    }\\n\\n    const sqlCreateTableQuery = `CREATE TABLE ${TABLE} (${createColumns.join(', ')})`;\\n    const sqlSelectTableQuery = `SELECT * FROM ${TABLE} LIMIT 3`;\\n\\n    // Get first 3 rows\\n    const rowsResult = await pool.query(sqlSelectTableQuery);\\n    const allValues = rowsResult.rows.map(row => Object.values(row).join(' '));\\n\\n    sqlSchemaPrompt = sqlCreateTableQuery + '\\\\n' + sqlSelectTableQuery + '\\\\n' + columnNames.join(' ') + '\\\\n' + allValues.join('\\\\n');\\n\\n    await pool.end(); // Close the connection pool\\n  } catch (e) {\\n    console.error(e);\\n    throw e;\\n  }\\n}\\n\\nasync function main() {\\n  await getSQLPrompt();\\n}\\n\\nmain().then(() => console.log(sqlSchemaPrompt)).catch(console.error);\\n\"\n        },\n        \"outputAnchors\": [\n          {\n            \"name\": \"output\",\n            \"label\": \"Output\",\n            \"type\": \"options\",\n            \"description\": \"\",\n            \"options\": [\n              {\n                \"id\": \"customFunction_0-output-output-string|number|boolean|json|array\",\n                \"name\": \"output\",\n                \"label\": \"Output\",\n                \"description\": \"\",\n                \"type\": \"string | number | boolean | json | array\"\n              },\n              {\n                \"id\": \"customFunction_0-output-EndingNode-CustomFunction\",\n                \"name\": \"EndingNode\",\n                \"label\": \"Ending Node\",\n                \"description\": \"\",\n                \"type\": \"CustomFunction\"\n              }\n            ],\n            \"default\": \"output\"\n          }\n        ],\n        \"outputs\": {\n          \"output\": \"output\"\n        },\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 723,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 815.8106338990393,\n        \"y\": -1586.9699386384095\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"chatOpenAI_0\",\n      \"position\": {\n        \"x\": 1139.6518116443945,\n        \"y\": -1935.2947005945705\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"chatOpenAI_0\",\n        \"label\": \"ChatOpenAI\",\n        \"version\": 7,\n        \"name\": \"chatOpenAI\",\n        \"type\": \"ChatOpenAI\",\n        \"baseClasses\": [\n          \"ChatOpenAI\",\n          \"BaseChatModel\",\n          \"BaseLanguageModel\",\n          \"Runnable\"\n        ],\n        \"category\": \"Chat Models\",\n        \"description\": \"Wrapper around OpenAI large language models that use the Chat endpoint\",\n        \"inputParams\": [\n          {\n            \"label\": \"Connect Credential\",\n            \"name\": \"credential\",\n            \"type\": \"credential\",\n            \"credentialNames\": [\n              \"openAIApi\"\n            ],\n            \"id\": \"chatOpenAI_0-input-credential-credential\"\n          },\n          {\n            \"label\": \"Model Name\",\n            \"name\": \"modelName\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"default\": \"gpt-3.5-turbo\",\n            \"id\": \"chatOpenAI_0-input-modelName-asyncOptions\"\n          },\n          {\n            \"label\": \"Temperature\",\n            \"name\": \"temperature\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"default\": 0.9,\n            \"optional\": true,\n            \"id\": \"chatOpenAI_0-input-temperature-number\"\n          },\n          {\n            \"label\": \"Max Tokens\",\n            \"name\": \"maxTokens\",\n            \"type\": \"number\",\n            \"step\": 1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_0-input-maxTokens-number\"\n          },\n          {\n            \"label\": \"Top Probability\",\n            \"name\": \"topP\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_0-input-topP-number\"\n          },\n          {\n            \"label\": \"Frequency Penalty\",\n            \"name\": \"frequencyPenalty\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_0-input-frequencyPenalty-number\"\n          },\n          {\n            \"label\": \"Presence Penalty\",\n            \"name\": \"presencePenalty\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_0-input-presencePenalty-number\"\n          },\n          {\n            \"label\": \"Timeout\",\n            \"name\": \"timeout\",\n            \"type\": \"number\",\n            \"step\": 1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_0-input-timeout-number\"\n          },\n          {\n            \"label\": \"BasePath\",\n            \"name\": \"basepath\",\n            \"type\": \"string\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_0-input-basepath-string\"\n          },\n          {\n            \"label\": \"Proxy Url\",\n            \"name\": \"proxyUrl\",\n            \"type\": \"string\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_0-input-proxyUrl-string\"\n          },\n          {\n            \"label\": \"Stop Sequence\",\n            \"name\": \"stopSequence\",\n            \"type\": \"string\",\n            \"rows\": 4,\n            \"optional\": true,\n            \"description\": \"List of stop words to use when generating. Use comma to separate multiple stop words.\",\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_0-input-stopSequence-string\"\n          },\n          {\n            \"label\": \"BaseOptions\",\n            \"name\": \"baseOptions\",\n            \"type\": \"json\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_0-input-baseOptions-json\"\n          },\n          {\n            \"label\": \"Allow Image Uploads\",\n            \"name\": \"allowImageUploads\",\n            \"type\": \"boolean\",\n            \"description\": \"Automatically uses gpt-4-vision-preview when image is being uploaded from chat. Only works with LLMChain, Conversation Chain, ReAct Agent, Conversational Agent, Tool Agent\",\n            \"default\": false,\n            \"optional\": true,\n            \"id\": \"chatOpenAI_0-input-allowImageUploads-boolean\"\n          },\n          {\n            \"label\": \"Image Resolution\",\n            \"description\": \"This parameter controls the resolution in which the model views the image.\",\n            \"name\": \"imageResolution\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"Low\",\n                \"name\": \"low\"\n              },\n              {\n                \"label\": \"High\",\n                \"name\": \"high\"\n              },\n              {\n                \"label\": \"Auto\",\n                \"name\": \"auto\"\n              }\n            ],\n            \"default\": \"low\",\n            \"optional\": false,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_0-input-imageResolution-options\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Cache\",\n            \"name\": \"cache\",\n            \"type\": \"BaseCache\",\n            \"optional\": true,\n            \"id\": \"chatOpenAI_0-input-cache-BaseCache\"\n          }\n        ],\n        \"inputs\": {\n          \"cache\": \"\",\n          \"modelName\": \"gpt-3.5-turbo\",\n          \"temperature\": \"0.7\",\n          \"maxTokens\": \"\",\n          \"topP\": \"\",\n          \"frequencyPenalty\": \"\",\n          \"presencePenalty\": \"\",\n          \"timeout\": \"\",\n          \"basepath\": \"\",\n          \"proxyUrl\": \"\",\n          \"stopSequence\": \"\",\n          \"baseOptions\": \"\",\n          \"allowImageUploads\": \"\",\n          \"imageResolution\": \"low\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"chatOpenAI_0-output-chatOpenAI-ChatOpenAI|BaseChatModel|BaseLanguageModel|Runnable\",\n            \"name\": \"chatOpenAI\",\n            \"label\": \"ChatOpenAI\",\n            \"description\": \"Wrapper around OpenAI large language models that use the Chat endpoint\",\n            \"type\": \"ChatOpenAI | BaseChatModel | BaseLanguageModel | Runnable\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 668,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 1139.6518116443945,\n        \"y\": -1935.2947005945705\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"llmChain_0\",\n      \"position\": {\n        \"x\": 1661.2911278200088,\n        \"y\": -1461.1712392874351\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"llmChain_0\",\n        \"label\": \"LLM Chain\",\n        \"version\": 3,\n        \"name\": \"llmChain\",\n        \"type\": \"LLMChain\",\n        \"baseClasses\": [\n          \"LLMChain\",\n          \"BaseChain\",\n          \"Runnable\"\n        ],\n        \"category\": \"Chains\",\n        \"description\": \"Chain to run queries against LLMs\",\n        \"inputParams\": [\n          {\n            \"label\": \"Chain Name\",\n            \"name\": \"chainName\",\n            \"type\": \"string\",\n            \"placeholder\": \"Name Your Chain\",\n            \"optional\": true,\n            \"id\": \"llmChain_0-input-chainName-string\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Language Model\",\n            \"name\": \"model\",\n            \"type\": \"BaseLanguageModel\",\n            \"id\": \"llmChain_0-input-model-BaseLanguageModel\"\n          },\n          {\n            \"label\": \"Prompt\",\n            \"name\": \"prompt\",\n            \"type\": \"BasePromptTemplate\",\n            \"id\": \"llmChain_0-input-prompt-BasePromptTemplate\"\n          },\n          {\n            \"label\": \"Output Parser\",\n            \"name\": \"outputParser\",\n            \"type\": \"BaseLLMOutputParser\",\n            \"optional\": true,\n            \"id\": \"llmChain_0-input-outputParser-BaseLLMOutputParser\"\n          },\n          {\n            \"label\": \"Input Moderation\",\n            \"description\": \"Detect text that could generate harmful output and prevent it from being sent to the language model\",\n            \"name\": \"inputModeration\",\n            \"type\": \"Moderation\",\n            \"optional\": true,\n            \"list\": true,\n            \"id\": \"llmChain_0-input-inputModeration-Moderation\"\n          }\n        ],\n        \"inputs\": {\n          \"model\": \"{{chatOpenAI_0.data.instance}}\",\n          \"prompt\": \"{{promptTemplate_4.data.instance}}\",\n          \"outputParser\": \"\",\n          \"inputModeration\": \"\",\n          \"chainName\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"name\": \"output\",\n            \"label\": \"Output\",\n            \"type\": \"options\",\n            \"description\": \"\",\n            \"options\": [\n              {\n                \"id\": \"llmChain_0-output-llmChain-LLMChain|BaseChain|Runnable\",\n                \"name\": \"llmChain\",\n                \"label\": \"LLM Chain\",\n                \"description\": \"\",\n                \"type\": \"LLMChain | BaseChain | Runnable\"\n              },\n              {\n                \"id\": \"llmChain_0-output-outputPrediction-string|json\",\n                \"name\": \"outputPrediction\",\n                \"label\": \"Output Prediction\",\n                \"description\": \"\",\n                \"type\": \"string | json\"\n              }\n            ],\n            \"default\": \"llmChain\"\n          }\n        ],\n        \"outputs\": {\n          \"output\": \"llmChain\"\n        },\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 506,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 1661.2911278200088,\n        \"y\": -1461.1712392874351\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"ifElseFunction_0\",\n      \"position\": {\n        \"x\": 367.89910000015686,\n        \"y\": -1558.5029771198094\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"ifElseFunction_0\",\n        \"label\": \"IfElse Function\",\n        \"version\": 2,\n        \"name\": \"ifElseFunction\",\n        \"type\": \"IfElseFunction\",\n        \"baseClasses\": [\n          \"IfElseFunction\",\n          \"Utilities\"\n        ],\n        \"tags\": [\n          \"Utilities\"\n        ],\n        \"category\": \"Utilities\",\n        \"description\": \"Split flows based on If Else javascript functions\",\n        \"inputParams\": [\n          {\n            \"label\": \"Input Variables\",\n            \"name\": \"functionInputVariables\",\n            \"description\": \"Input variables can be used in the function with prefix $. For example: $var\",\n            \"type\": \"json\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"list\": true,\n            \"id\": \"ifElseFunction_0-input-functionInputVariables-json\"\n          },\n          {\n            \"label\": \"IfElse Name\",\n            \"name\": \"functionName\",\n            \"type\": \"string\",\n            \"optional\": true,\n            \"placeholder\": \"If Condition Match\",\n            \"id\": \"ifElseFunction_0-input-functionName-string\"\n          },\n          {\n            \"label\": \"If Function\",\n            \"name\": \"ifFunction\",\n            \"description\": \"Function must return a value\",\n            \"type\": \"code\",\n            \"rows\": 2,\n            \"default\": \"if (\\\"hello\\\" == \\\"hello\\\") {\\n    return true;\\n}\",\n            \"id\": \"ifElseFunction_0-input-ifFunction-code\"\n          },\n          {\n            \"label\": \"Else Function\",\n            \"name\": \"elseFunction\",\n            \"description\": \"Function must return a value\",\n            \"type\": \"code\",\n            \"rows\": 2,\n            \"default\": \"return false;\",\n            \"id\": \"ifElseFunction_0-input-elseFunction-code\"\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"functionInputVariables\": \"\",\n          \"functionName\": \"\",\n          \"ifFunction\": \"// Ensure $sqlQuery is defined and not null\\nconst sqlQuery = $sqlQuery && $sqlQuery.trim ? $sqlQuery.trim() : '';\\n\\nconst regex = /SELECT\\\\s.*?(?:\\\\n|$)/gi;\\n\\n// Extracting the SQL part\\nconst matches = sqlQuery.match(regex);\\nconst cleanSql = matches ? matches[0].trim() : \\\"\\\";\\n\\n// Check if the query includes SELECT and WHERE for PostgreSQL\\nif (cleanSql.toUpperCase().includes(\\\"SELECT\\\") && cleanSql.toUpperCase().includes(\\\"WHERE\\\")) {\\n    return cleanSql;\\n}\\n\",\n          \"elseFunction\": \"return $sqlQuery;\"\n        },\n        \"outputAnchors\": [\n          {\n            \"name\": \"output\",\n            \"label\": \"Output\",\n            \"type\": \"options\",\n            \"description\": \"\",\n            \"options\": [\n              {\n                \"id\": \"ifElseFunction_0-output-returnTrue-string|number|boolean|json|array\",\n                \"name\": \"returnTrue\",\n                \"label\": \"True\",\n                \"description\": \"\",\n                \"type\": \"string | number | boolean | json | array\",\n                \"isAnchor\": true\n              },\n              {\n                \"id\": \"ifElseFunction_0-output-returnFalse-string|number|boolean|json|array\",\n                \"name\": \"returnFalse\",\n                \"label\": \"False\",\n                \"description\": \"\",\n                \"type\": \"string | number | boolean | json | array\",\n                \"isAnchor\": true\n              }\n            ],\n            \"default\": \"returnTrue\"\n          }\n        ],\n        \"outputs\": {\n          \"output\": \"returnTrue\"\n        },\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 764,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 367.89910000015686,\n        \"y\": -1558.5029771198094\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"customFunction_1\",\n      \"position\": {\n        \"x\": -855.0387778308864,\n        \"y\": -1503.6724441072079\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"customFunction_1\",\n        \"label\": \"Custom JS Function\",\n        \"version\": 3,\n        \"name\": \"customFunction\",\n        \"type\": \"CustomFunction\",\n        \"baseClasses\": [\n          \"CustomFunction\",\n          \"Utilities\"\n        ],\n        \"tags\": [\n          \"Utilities\"\n        ],\n        \"category\": \"Utilities\",\n        \"description\": \"Execute custom javascript function\",\n        \"inputParams\": [\n          {\n            \"label\": \"Input Variables\",\n            \"name\": \"functionInputVariables\",\n            \"description\": \"Input variables can be used in the function with prefix $. For example: $var\",\n            \"type\": \"json\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"list\": true,\n            \"id\": \"customFunction_1-input-functionInputVariables-json\"\n          },\n          {\n            \"label\": \"Function Name\",\n            \"name\": \"functionName\",\n            \"type\": \"string\",\n            \"optional\": true,\n            \"placeholder\": \"My Function\",\n            \"id\": \"customFunction_1-input-functionName-string\"\n          },\n          {\n            \"label\": \"Javascript Function\",\n            \"name\": \"javascriptFunction\",\n            \"type\": \"code\",\n            \"id\": \"customFunction_1-input-javascriptFunction-code\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Additional Tools\",\n            \"description\": \"Tools can be used in the function with $tools.{tool_name}.invoke(args)\",\n            \"name\": \"tools\",\n            \"type\": \"Tool\",\n            \"list\": true,\n            \"optional\": true,\n            \"id\": \"customFunction_1-input-tools-Tool\"\n          }\n        ],\n        \"inputs\": {\n          \"functionInputVariables\": \"\",\n          \"functionName\": \"\",\n          \"tools\": \"\",\n          \"javascriptFunction\": \"const HOST = 'localhost';\\nconst USER = 'postgres';\\nconst PASSWORD = '0912';\\nconst DATABASE = 'newdata';\\nconst TABLE = 'newdata';\\nconst { Pool } = require('pg'); // Import the pg module for PostgreSQL\\n\\nlet sqlSchemaPrompt;\\n\\nasync function getSQLPrompt() {\\n  try {\\n    const pool = new Pool({\\n      host: HOST,\\n      user: USER,\\n      password: PASSWORD,\\n      database: DATABASE,\\n    });\\n\\n    // Get schema info\\n    const schemaQuery = `SELECT column_name, data_type, is_nullable \\n                         FROM information_schema.columns \\n                         WHERE table_name = $1`;\\n\\n    const schemaResult = await pool.query(schemaQuery, [TABLE]);\\n    const createColumns = [];\\n    const columnNames = [];\\n\\n    // Loop through schema result and build the CREATE TABLE statement\\n    for (const schemaData of schemaResult.rows) {\\n      columnNames.push(schemaData.column_name);\\n      createColumns.push(`${schemaData.column_name} ${schemaData.data_type} ${schemaData.is_nullable === 'NO' ? 'NOT NULL' : ''}`);\\n    }\\n\\n    // Build the CREATE TABLE SQL query\\n    const sqlCreateTableQuery = `CREATE TABLE ${TABLE} (${createColumns.join(', ')})`;\\n\\n    // Select first 3 rows from the table\\n    const sqlSelectTableQuery = `SELECT * FROM ${TABLE} `;\\n\\n    // Get first 3 rows from the table\\n    const rowsResult = await pool.query(sqlSelectTableQuery);\\n    const allValues = rowsResult.rows.map(row => Object.values(row).join(' '));\\n\\n    // Combine schema info, queries, and values\\n    sqlSchemaPrompt = sqlCreateTableQuery + '\\\\n' + sqlSelectTableQuery + '\\\\n' + columnNames.join(' ') + '\\\\n' + allValues.join('\\\\n');\\n\\n    await pool.end(); // Close the connection pool\\n  } catch (e) {\\n    console.error(e);\\n    throw e;\\n  }\\n}\\n\\nasync function main() {\\n  await getSQLPrompt();  // Fetch schema and data\\n}\\n\\nmain().then(() => console.log(sqlSchemaPrompt)).catch(console.error);\\n\"\n        },\n        \"outputAnchors\": [\n          {\n            \"name\": \"output\",\n            \"label\": \"Output\",\n            \"type\": \"options\",\n            \"description\": \"\",\n            \"options\": [\n              {\n                \"id\": \"customFunction_1-output-output-string|number|boolean|json|array\",\n                \"name\": \"output\",\n                \"label\": \"Output\",\n                \"description\": \"\",\n                \"type\": \"string | number | boolean | json | array\"\n              },\n              {\n                \"id\": \"customFunction_1-output-EndingNode-CustomFunction\",\n                \"name\": \"EndingNode\",\n                \"label\": \"Ending Node\",\n                \"description\": \"\",\n                \"type\": \"CustomFunction\"\n              }\n            ],\n            \"default\": \"output\"\n          }\n        ],\n        \"outputs\": {\n          \"output\": \"output\"\n        },\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 723,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": -855.0387778308864,\n        \"y\": -1503.6724441072079\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"chatOpenAI_1\",\n      \"position\": {\n        \"x\": -418.2408770778424,\n        \"y\": -1555.1083268726193\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"chatOpenAI_1\",\n        \"label\": \"ChatOpenAI\",\n        \"version\": 7,\n        \"name\": \"chatOpenAI\",\n        \"type\": \"ChatOpenAI\",\n        \"baseClasses\": [\n          \"ChatOpenAI\",\n          \"BaseChatModel\",\n          \"BaseLanguageModel\",\n          \"Runnable\"\n        ],\n        \"category\": \"Chat Models\",\n        \"description\": \"Wrapper around OpenAI large language models that use the Chat endpoint\",\n        \"inputParams\": [\n          {\n            \"label\": \"Connect Credential\",\n            \"name\": \"credential\",\n            \"type\": \"credential\",\n            \"credentialNames\": [\n              \"openAIApi\"\n            ],\n            \"id\": \"chatOpenAI_1-input-credential-credential\"\n          },\n          {\n            \"label\": \"Model Name\",\n            \"name\": \"modelName\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"default\": \"gpt-3.5-turbo\",\n            \"id\": \"chatOpenAI_1-input-modelName-asyncOptions\"\n          },\n          {\n            \"label\": \"Temperature\",\n            \"name\": \"temperature\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"default\": 0.9,\n            \"optional\": true,\n            \"id\": \"chatOpenAI_1-input-temperature-number\"\n          },\n          {\n            \"label\": \"Max Tokens\",\n            \"name\": \"maxTokens\",\n            \"type\": \"number\",\n            \"step\": 1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_1-input-maxTokens-number\"\n          },\n          {\n            \"label\": \"Top Probability\",\n            \"name\": \"topP\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_1-input-topP-number\"\n          },\n          {\n            \"label\": \"Frequency Penalty\",\n            \"name\": \"frequencyPenalty\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_1-input-frequencyPenalty-number\"\n          },\n          {\n            \"label\": \"Presence Penalty\",\n            \"name\": \"presencePenalty\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_1-input-presencePenalty-number\"\n          },\n          {\n            \"label\": \"Timeout\",\n            \"name\": \"timeout\",\n            \"type\": \"number\",\n            \"step\": 1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_1-input-timeout-number\"\n          },\n          {\n            \"label\": \"BasePath\",\n            \"name\": \"basepath\",\n            \"type\": \"string\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_1-input-basepath-string\"\n          },\n          {\n            \"label\": \"Proxy Url\",\n            \"name\": \"proxyUrl\",\n            \"type\": \"string\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_1-input-proxyUrl-string\"\n          },\n          {\n            \"label\": \"Stop Sequence\",\n            \"name\": \"stopSequence\",\n            \"type\": \"string\",\n            \"rows\": 4,\n            \"optional\": true,\n            \"description\": \"List of stop words to use when generating. Use comma to separate multiple stop words.\",\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_1-input-stopSequence-string\"\n          },\n          {\n            \"label\": \"BaseOptions\",\n            \"name\": \"baseOptions\",\n            \"type\": \"json\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_1-input-baseOptions-json\"\n          },\n          {\n            \"label\": \"Allow Image Uploads\",\n            \"name\": \"allowImageUploads\",\n            \"type\": \"boolean\",\n            \"description\": \"Automatically uses gpt-4-vision-preview when image is being uploaded from chat. Only works with LLMChain, Conversation Chain, ReAct Agent, Conversational Agent, Tool Agent\",\n            \"default\": false,\n            \"optional\": true,\n            \"id\": \"chatOpenAI_1-input-allowImageUploads-boolean\"\n          },\n          {\n            \"label\": \"Image Resolution\",\n            \"description\": \"This parameter controls the resolution in which the model views the image.\",\n            \"name\": \"imageResolution\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"Low\",\n                \"name\": \"low\"\n              },\n              {\n                \"label\": \"High\",\n                \"name\": \"high\"\n              },\n              {\n                \"label\": \"Auto\",\n                \"name\": \"auto\"\n              }\n            ],\n            \"default\": \"low\",\n            \"optional\": false,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_1-input-imageResolution-options\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Cache\",\n            \"name\": \"cache\",\n            \"type\": \"BaseCache\",\n            \"optional\": true,\n            \"id\": \"chatOpenAI_1-input-cache-BaseCache\"\n          }\n        ],\n        \"inputs\": {\n          \"cache\": \"\",\n          \"modelName\": \"gpt-3.5-turbo\",\n          \"temperature\": \"0.7\",\n          \"maxTokens\": \"\",\n          \"topP\": \"\",\n          \"frequencyPenalty\": \"\",\n          \"presencePenalty\": \"\",\n          \"timeout\": \"\",\n          \"basepath\": \"\",\n          \"proxyUrl\": \"\",\n          \"stopSequence\": \"\",\n          \"baseOptions\": \"\",\n          \"allowImageUploads\": \"\",\n          \"imageResolution\": \"low\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"chatOpenAI_1-output-chatOpenAI-ChatOpenAI|BaseChatModel|BaseLanguageModel|Runnable\",\n            \"name\": \"chatOpenAI\",\n            \"label\": \"ChatOpenAI\",\n            \"description\": \"Wrapper around OpenAI large language models that use the Chat endpoint\",\n            \"type\": \"ChatOpenAI | BaseChatModel | BaseLanguageModel | Runnable\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 668,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": -418.2408770778424,\n        \"y\": -1555.1083268726193\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"llmChain_1\",\n      \"position\": {\n        \"x\": -20.791200171381718,\n        \"y\": -1585.292554108034\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"llmChain_1\",\n        \"label\": \"LLM Chain\",\n        \"version\": 3,\n        \"name\": \"llmChain\",\n        \"type\": \"LLMChain\",\n        \"baseClasses\": [\n          \"LLMChain\",\n          \"BaseChain\",\n          \"Runnable\"\n        ],\n        \"category\": \"Chains\",\n        \"description\": \"Chain to run queries against LLMs\",\n        \"inputParams\": [\n          {\n            \"label\": \"Chain Name\",\n            \"name\": \"chainName\",\n            \"type\": \"string\",\n            \"placeholder\": \"Name Your Chain\",\n            \"optional\": true,\n            \"id\": \"llmChain_1-input-chainName-string\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Language Model\",\n            \"name\": \"model\",\n            \"type\": \"BaseLanguageModel\",\n            \"id\": \"llmChain_1-input-model-BaseLanguageModel\"\n          },\n          {\n            \"label\": \"Prompt\",\n            \"name\": \"prompt\",\n            \"type\": \"BasePromptTemplate\",\n            \"id\": \"llmChain_1-input-prompt-BasePromptTemplate\"\n          },\n          {\n            \"label\": \"Output Parser\",\n            \"name\": \"outputParser\",\n            \"type\": \"BaseLLMOutputParser\",\n            \"optional\": true,\n            \"id\": \"llmChain_1-input-outputParser-BaseLLMOutputParser\"\n          },\n          {\n            \"label\": \"Input Moderation\",\n            \"description\": \"Detect text that could generate harmful output and prevent it from being sent to the language model\",\n            \"name\": \"inputModeration\",\n            \"type\": \"Moderation\",\n            \"optional\": true,\n            \"list\": true,\n            \"id\": \"llmChain_1-input-inputModeration-Moderation\"\n          }\n        ],\n        \"inputs\": {\n          \"model\": \"{{chatOpenAI_1.data.instance}}\",\n          \"prompt\": \"{{promptTemplate_1.data.instance}}\",\n          \"outputParser\": \"\",\n          \"inputModeration\": \"\",\n          \"chainName\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"name\": \"output\",\n            \"label\": \"Output\",\n            \"type\": \"options\",\n            \"description\": \"\",\n            \"options\": [\n              {\n                \"id\": \"llmChain_1-output-llmChain-LLMChain|BaseChain|Runnable\",\n                \"name\": \"llmChain\",\n                \"label\": \"LLM Chain\",\n                \"description\": \"\",\n                \"type\": \"LLMChain | BaseChain | Runnable\"\n              },\n              {\n                \"id\": \"llmChain_1-output-outputPrediction-string|json\",\n                \"name\": \"outputPrediction\",\n                \"label\": \"Output Prediction\",\n                \"description\": \"\",\n                \"type\": \"string | json\"\n              }\n            ],\n            \"default\": \"llmChain\"\n          }\n        ],\n        \"outputs\": {\n          \"output\": \"outputPrediction\"\n        },\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 506,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": -20.791200171381718,\n        \"y\": -1585.292554108034\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"promptTemplate_1\",\n      \"position\": {\n        \"x\": -395.72333221813,\n        \"y\": -822.9496528372629\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"promptTemplate_1\",\n        \"label\": \"Prompt Template\",\n        \"version\": 1,\n        \"name\": \"promptTemplate\",\n        \"type\": \"PromptTemplate\",\n        \"baseClasses\": [\n          \"PromptTemplate\",\n          \"BaseStringPromptTemplate\",\n          \"BasePromptTemplate\",\n          \"Runnable\"\n        ],\n        \"category\": \"Prompts\",\n        \"description\": \"Schema to represent a basic prompt for an LLM\",\n        \"inputParams\": [\n          {\n            \"label\": \"Template\",\n            \"name\": \"template\",\n            \"type\": \"string\",\n            \"rows\": 4,\n            \"placeholder\": \"What is a good name for a company that makes {product}?\",\n            \"id\": \"promptTemplate_1-input-template-string\"\n          },\n          {\n            \"label\": \"Format Prompt Values\",\n            \"name\": \"promptValues\",\n            \"type\": \"json\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"list\": true,\n            \"id\": \"promptTemplate_1-input-promptValues-json\"\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"template\": \"Based on the provided postgresql  table schema and question below, return a SQL SELECT ALL query that would answer the user's question. For example: SELECT * FROM table WHERE id = '1'.\\n------------\\nSCHEMA: {schema}\\n------------\\nQUESTION: {question}\\n------------\\nSQL QUERY:\",\n          \"promptValues\": \"{\\\"schema\\\":\\\"{{customFunction_0.data.instance}}\\\",\\\"question\\\":\\\"{{question}}\\\"}\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"promptTemplate_1-output-promptTemplate-PromptTemplate|BaseStringPromptTemplate|BasePromptTemplate|Runnable\",\n            \"name\": \"promptTemplate\",\n            \"label\": \"PromptTemplate\",\n            \"description\": \"Schema to represent a basic prompt for an LLM\",\n            \"type\": \"PromptTemplate | BaseStringPromptTemplate | BasePromptTemplate | Runnable\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 509,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": -395.72333221813,\n        \"y\": -822.9496528372629\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"llmChain_2\",\n      \"position\": {\n        \"x\": 1303.092374471539,\n        \"y\": -642.3580063367729\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"llmChain_2\",\n        \"label\": \"LLM Chain\",\n        \"version\": 3,\n        \"name\": \"llmChain\",\n        \"type\": \"LLMChain\",\n        \"baseClasses\": [\n          \"LLMChain\",\n          \"BaseChain\",\n          \"Runnable\"\n        ],\n        \"category\": \"Chains\",\n        \"description\": \"Chain to run queries against LLMs\",\n        \"inputParams\": [\n          {\n            \"label\": \"Chain Name\",\n            \"name\": \"chainName\",\n            \"type\": \"string\",\n            \"placeholder\": \"Name Your Chain\",\n            \"optional\": true,\n            \"id\": \"llmChain_2-input-chainName-string\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Language Model\",\n            \"name\": \"model\",\n            \"type\": \"BaseLanguageModel\",\n            \"id\": \"llmChain_2-input-model-BaseLanguageModel\"\n          },\n          {\n            \"label\": \"Prompt\",\n            \"name\": \"prompt\",\n            \"type\": \"BasePromptTemplate\",\n            \"id\": \"llmChain_2-input-prompt-BasePromptTemplate\"\n          },\n          {\n            \"label\": \"Output Parser\",\n            \"name\": \"outputParser\",\n            \"type\": \"BaseLLMOutputParser\",\n            \"optional\": true,\n            \"id\": \"llmChain_2-input-outputParser-BaseLLMOutputParser\"\n          },\n          {\n            \"label\": \"Input Moderation\",\n            \"description\": \"Detect text that could generate harmful output and prevent it from being sent to the language model\",\n            \"name\": \"inputModeration\",\n            \"type\": \"Moderation\",\n            \"optional\": true,\n            \"list\": true,\n            \"id\": \"llmChain_2-input-inputModeration-Moderation\"\n          }\n        ],\n        \"inputs\": {\n          \"model\": \"\",\n          \"prompt\": \"{{promptTemplate_2.data.instance}}\",\n          \"outputParser\": \"\",\n          \"inputModeration\": \"\",\n          \"chainName\": \"Fallback chain\"\n        },\n        \"outputAnchors\": [\n          {\n            \"name\": \"output\",\n            \"label\": \"Output\",\n            \"type\": \"options\",\n            \"description\": \"\",\n            \"options\": [\n              {\n                \"id\": \"llmChain_2-output-llmChain-LLMChain|BaseChain|Runnable\",\n                \"name\": \"llmChain\",\n                \"label\": \"LLM Chain\",\n                \"description\": \"\",\n                \"type\": \"LLMChain | BaseChain | Runnable\"\n              },\n              {\n                \"id\": \"llmChain_2-output-outputPrediction-string|json\",\n                \"name\": \"outputPrediction\",\n                \"label\": \"Output Prediction\",\n                \"description\": \"\",\n                \"type\": \"string | json\"\n              }\n            ],\n            \"default\": \"llmChain\"\n          }\n        ],\n        \"outputs\": {\n          \"output\": \"llmChain\"\n        },\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 506,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 1303.092374471539,\n        \"y\": -642.3580063367729\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"promptTemplate_2\",\n      \"position\": {\n        \"x\": 869.7952222419366,\n        \"y\": -756.4302333273861\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"promptTemplate_2\",\n        \"label\": \"Prompt Template\",\n        \"version\": 1,\n        \"name\": \"promptTemplate\",\n        \"type\": \"PromptTemplate\",\n        \"baseClasses\": [\n          \"PromptTemplate\",\n          \"BaseStringPromptTemplate\",\n          \"BasePromptTemplate\",\n          \"Runnable\"\n        ],\n        \"category\": \"Prompts\",\n        \"description\": \"Schema to represent a basic prompt for an LLM\",\n        \"inputParams\": [\n          {\n            \"label\": \"Template\",\n            \"name\": \"template\",\n            \"type\": \"string\",\n            \"rows\": 4,\n            \"placeholder\": \"What is a good name for a company that makes {product}?\",\n            \"id\": \"promptTemplate_2-input-template-string\"\n          },\n          {\n            \"label\": \"Format Prompt Values\",\n            \"name\": \"promptValues\",\n            \"type\": \"json\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"list\": true,\n            \"id\": \"promptTemplate_2-input-promptValues-json\"\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"template\": \"politely say i am not able to answer the query\",\n          \"promptValues\": \"{\\\"schema\\\":\\\"{{customFunction_0.data.instance}}\\\",\\\"question\\\":\\\"{{question}}\\\"}\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"promptTemplate_2-output-promptTemplate-PromptTemplate|BaseStringPromptTemplate|BasePromptTemplate|Runnable\",\n            \"name\": \"promptTemplate\",\n            \"label\": \"PromptTemplate\",\n            \"description\": \"Schema to represent a basic prompt for an LLM\",\n            \"type\": \"PromptTemplate | BaseStringPromptTemplate | BasePromptTemplate | Runnable\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 509,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 869.7952222419366,\n        \"y\": -756.4302333273861\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"promptTemplate_4\",\n      \"position\": {\n        \"x\": 1201.5587822935686,\n        \"y\": -1227.1641815986081\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"promptTemplate_4\",\n        \"label\": \"Prompt Template\",\n        \"version\": 1,\n        \"name\": \"promptTemplate\",\n        \"type\": \"PromptTemplate\",\n        \"baseClasses\": [\n          \"PromptTemplate\",\n          \"BaseStringPromptTemplate\",\n          \"BasePromptTemplate\",\n          \"Runnable\"\n        ],\n        \"category\": \"Prompts\",\n        \"description\": \"Schema to represent a basic prompt for an LLM\",\n        \"inputParams\": [\n          {\n            \"label\": \"Template\",\n            \"name\": \"template\",\n            \"type\": \"string\",\n            \"rows\": 4,\n            \"placeholder\": \"What is a good name for a company that makes {product}?\",\n            \"id\": \"promptTemplate_4-input-template-string\"\n          },\n          {\n            \"label\": \"Format Prompt Values\",\n            \"name\": \"promptValues\",\n            \"type\": \"json\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"list\": true,\n            \"id\": \"promptTemplate_4-input-promptValues-json\"\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"template\": \"Based on the question, and SQL response, write a natural language response, be details as possible:\\n------------\\nQUESTION: {question}\\n------------\\nSQL RESPONSE: {sqlResponse}\\n------------\\nNATURAL LANGUAGE RESPONSE:\",\n          \"promptValues\": \"{\\\"schema\\\":\\\"{{customFunction_0.data.instance}}\\\",\\\"question\\\":\\\"{{question}}\\\"}\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"promptTemplate_4-output-promptTemplate-PromptTemplate|BaseStringPromptTemplate|BasePromptTemplate|Runnable\",\n            \"name\": \"promptTemplate\",\n            \"label\": \"PromptTemplate\",\n            \"description\": \"Schema to represent a basic prompt for an LLM\",\n            \"type\": \"PromptTemplate | BaseStringPromptTemplate | BasePromptTemplate | Runnable\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 509,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 1201.5587822935686,\n        \"y\": -1227.1641815986081\n      },\n      \"dragging\": false\n    }\n  ],\n  \"edges\": [\n    {\n      \"source\": \"chatOpenAI_0\",\n      \"sourceHandle\": \"chatOpenAI_0-output-chatOpenAI-ChatOpenAI|BaseChatModel|BaseLanguageModel|Runnable\",\n      \"target\": \"llmChain_0\",\n      \"targetHandle\": \"llmChain_0-input-model-BaseLanguageModel\",\n      \"type\": \"buttonedge\",\n      \"id\": \"chatOpenAI_0-chatOpenAI_0-output-chatOpenAI-ChatOpenAI|BaseChatModel|BaseLanguageModel|Runnable-llmChain_0-llmChain_0-input-model-BaseLanguageModel\"\n    },\n    {\n      \"source\": \"chatOpenAI_1\",\n      \"sourceHandle\": \"chatOpenAI_1-output-chatOpenAI-ChatOpenAI|BaseChatModel|BaseLanguageModel|Runnable\",\n      \"target\": \"llmChain_1\",\n      \"targetHandle\": \"llmChain_1-input-model-BaseLanguageModel\",\n      \"type\": \"buttonedge\",\n      \"id\": \"chatOpenAI_1-chatOpenAI_1-output-chatOpenAI-ChatOpenAI|BaseChatModel|BaseLanguageModel|Runnable-llmChain_1-llmChain_1-input-model-BaseLanguageModel\"\n    },\n    {\n      \"source\": \"promptTemplate_1\",\n      \"sourceHandle\": \"promptTemplate_1-output-promptTemplate-PromptTemplate|BaseStringPromptTemplate|BasePromptTemplate|Runnable\",\n      \"target\": \"llmChain_1\",\n      \"targetHandle\": \"llmChain_1-input-prompt-BasePromptTemplate\",\n      \"type\": \"buttonedge\",\n      \"id\": \"promptTemplate_1-promptTemplate_1-output-promptTemplate-PromptTemplate|BaseStringPromptTemplate|BasePromptTemplate|Runnable-llmChain_1-llmChain_1-input-prompt-BasePromptTemplate\"\n    },\n    {\n      \"source\": \"llmChain_1\",\n      \"sourceHandle\": \"llmChain_1-output-outputPrediction-string|json\",\n      \"target\": \"ifElseFunction_0\",\n      \"targetHandle\": \"ifElseFunction_0-input-functionInputVariables-json\",\n      \"type\": \"buttonedge\",\n      \"id\": \"llmChain_1-llmChain_1-output-outputPrediction-string|json-ifElseFunction_0-ifElseFunction_0-input-functionInputVariables-json\"\n    },\n    {\n      \"source\": \"customFunction_1\",\n      \"sourceHandle\": \"customFunction_1-output-output-string|number|boolean|json|array\",\n      \"target\": \"promptTemplate_1\",\n      \"targetHandle\": \"promptTemplate_1-input-promptValues-json\",\n      \"type\": \"buttonedge\",\n      \"id\": \"customFunction_1-customFunction_1-output-output-string|number|boolean|json|array-promptTemplate_1-promptTemplate_1-input-promptValues-json\"\n    },\n    {\n      \"source\": \"ifElseFunction_0\",\n      \"sourceHandle\": \"ifElseFunction_0-output-returnTrue-string|number|boolean|json|array\",\n      \"target\": \"customFunction_0\",\n      \"targetHandle\": \"customFunction_0-input-functionInputVariables-json\",\n      \"type\": \"buttonedge\",\n      \"id\": \"ifElseFunction_0-ifElseFunction_0-output-returnTrue-string|number|boolean|json|array-customFunction_0-customFunction_0-input-functionInputVariables-json\"\n    },\n    {\n      \"source\": \"ifElseFunction_0\",\n      \"sourceHandle\": \"ifElseFunction_0-output-returnFalse-string|number|boolean|json|array\",\n      \"target\": \"promptTemplate_2\",\n      \"targetHandle\": \"promptTemplate_2-input-promptValues-json\",\n      \"type\": \"buttonedge\",\n      \"id\": \"ifElseFunction_0-ifElseFunction_0-output-returnFalse-string|number|boolean|json|array-promptTemplate_2-promptTemplate_2-input-promptValues-json\"\n    },\n    {\n      \"source\": \"promptTemplate_2\",\n      \"sourceHandle\": \"promptTemplate_2-output-promptTemplate-PromptTemplate|BaseStringPromptTemplate|BasePromptTemplate|Runnable\",\n      \"target\": \"llmChain_2\",\n      \"targetHandle\": \"llmChain_2-input-prompt-BasePromptTemplate\",\n      \"type\": \"buttonedge\",\n      \"id\": \"promptTemplate_2-promptTemplate_2-output-promptTemplate-PromptTemplate|BaseStringPromptTemplate|BasePromptTemplate|Runnable-llmChain_2-llmChain_2-input-prompt-BasePromptTemplate\"\n    },\n    {\n      \"source\": \"promptTemplate_4\",\n      \"sourceHandle\": \"promptTemplate_4-output-promptTemplate-PromptTemplate|BaseStringPromptTemplate|BasePromptTemplate|Runnable\",\n      \"target\": \"llmChain_0\",\n      \"targetHandle\": \"llmChain_0-input-prompt-BasePromptTemplate\",\n      \"type\": \"buttonedge\",\n      \"id\": \"promptTemplate_4-promptTemplate_4-output-promptTemplate-PromptTemplate|BaseStringPromptTemplate|BasePromptTemplate|Runnable-llmChain_0-llmChain_0-input-prompt-BasePromptTemplate\"\n    },\n    {\n      \"source\": \"customFunction_0\",\n      \"sourceHandle\": \"customFunction_0-output-output-string|number|boolean|json|array\",\n      \"target\": \"promptTemplate_4\",\n      \"targetHandle\": \"promptTemplate_4-input-promptValues-json\",\n      \"type\": \"buttonedge\",\n      \"id\": \"customFunction_0-customFunction_0-output-output-string|number|boolean|json|array-promptTemplate_4-promptTemplate_4-input-promptValues-json\"\n    }\n  ]\n}",
      "type": "CHATFLOW"
    },
    {
      "id": "804ac3ba-08af-46c3-9827-b081a7edb679",
      "name": "gujaratdocument (1)",
      "flowData": "{\n  \"nodes\": [\n    {\n      \"id\": \"openAIEmbeddings_0\",\n      \"position\": {\n        \"x\": 787.9382208398894,\n        \"y\": -68.23346919758643\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"openAIEmbeddings_0\",\n        \"label\": \"OpenAI Embeddings\",\n        \"version\": 4,\n        \"name\": \"openAIEmbeddings\",\n        \"type\": \"OpenAIEmbeddings\",\n        \"baseClasses\": [\n          \"OpenAIEmbeddings\",\n          \"Embeddings\"\n        ],\n        \"category\": \"Embeddings\",\n        \"description\": \"OpenAI API to generate embeddings for a given text\",\n        \"inputParams\": [\n          {\n            \"label\": \"Connect Credential\",\n            \"name\": \"credential\",\n            \"type\": \"credential\",\n            \"credentialNames\": [\n              \"openAIApi\"\n            ],\n            \"id\": \"openAIEmbeddings_0-input-credential-credential\"\n          },\n          {\n            \"label\": \"Model Name\",\n            \"name\": \"modelName\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"default\": \"text-embedding-ada-002\",\n            \"id\": \"openAIEmbeddings_0-input-modelName-asyncOptions\"\n          },\n          {\n            \"label\": \"Strip New Lines\",\n            \"name\": \"stripNewLines\",\n            \"type\": \"boolean\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"openAIEmbeddings_0-input-stripNewLines-boolean\"\n          },\n          {\n            \"label\": \"Batch Size\",\n            \"name\": \"batchSize\",\n            \"type\": \"number\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"openAIEmbeddings_0-input-batchSize-number\"\n          },\n          {\n            \"label\": \"Timeout\",\n            \"name\": \"timeout\",\n            \"type\": \"number\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"openAIEmbeddings_0-input-timeout-number\"\n          },\n          {\n            \"label\": \"BasePath\",\n            \"name\": \"basepath\",\n            \"type\": \"string\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"openAIEmbeddings_0-input-basepath-string\"\n          },\n          {\n            \"label\": \"Dimensions\",\n            \"name\": \"dimensions\",\n            \"type\": \"number\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"openAIEmbeddings_0-input-dimensions-number\"\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"modelName\": \"text-embedding-3-small\",\n          \"stripNewLines\": \"\",\n          \"batchSize\": \"\",\n          \"timeout\": \"\",\n          \"basepath\": \"\",\n          \"dimensions\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"openAIEmbeddings_0-output-openAIEmbeddings-OpenAIEmbeddings|Embeddings\",\n            \"name\": \"openAIEmbeddings\",\n            \"label\": \"OpenAIEmbeddings\",\n            \"description\": \"OpenAI API to generate embeddings for a given text\",\n            \"type\": \"OpenAIEmbeddings | Embeddings\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 423,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 787.9382208398894,\n        \"y\": -68.23346919758643\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"pdfFile_0\",\n      \"position\": {\n        \"x\": 787.634023360763,\n        \"y\": -684.0661302681481\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"pdfFile_0\",\n        \"label\": \"Pdf File\",\n        \"version\": 1,\n        \"name\": \"pdfFile\",\n        \"type\": \"Document\",\n        \"baseClasses\": [\n          \"Document\"\n        ],\n        \"category\": \"Document Loaders\",\n        \"description\": \"Load data from PDF files\",\n        \"inputParams\": [\n          {\n            \"label\": \"Pdf File\",\n            \"name\": \"pdfFile\",\n            \"type\": \"file\",\n            \"fileType\": \".pdf\",\n            \"id\": \"pdfFile_0-input-pdfFile-file\"\n          },\n          {\n            \"label\": \"Usage\",\n            \"name\": \"usage\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"One document per page\",\n                \"name\": \"perPage\"\n              },\n              {\n                \"label\": \"One document per file\",\n                \"name\": \"perFile\"\n              }\n            ],\n            \"default\": \"perPage\",\n            \"id\": \"pdfFile_0-input-usage-options\"\n          },\n          {\n            \"label\": \"Use Legacy Build\",\n            \"name\": \"legacyBuild\",\n            \"type\": \"boolean\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"pdfFile_0-input-legacyBuild-boolean\"\n          },\n          {\n            \"label\": \"Additional Metadata\",\n            \"name\": \"metadata\",\n            \"type\": \"json\",\n            \"description\": \"Additional metadata to be added to the extracted documents\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"pdfFile_0-input-metadata-json\"\n          },\n          {\n            \"label\": \"Omit Metadata Keys\",\n            \"name\": \"omitMetadataKeys\",\n            \"type\": \"string\",\n            \"rows\": 4,\n            \"description\": \"Each document loader comes with a default set of metadata keys that are extracted from the document. You can use this field to omit some of the default metadata keys. The value should be a list of keys, seperated by comma. Use * to omit all metadata keys execept the ones you specify in the Additional Metadata field\",\n            \"placeholder\": \"key1, key2, key3.nestedKey1\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"pdfFile_0-input-omitMetadataKeys-string\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Text Splitter\",\n            \"name\": \"textSplitter\",\n            \"type\": \"TextSplitter\",\n            \"optional\": true,\n            \"id\": \"pdfFile_0-input-textSplitter-TextSplitter\"\n          }\n        ],\n        \"inputs\": {\n          \"textSplitter\": \"{{tokenTextSplitter_0.data.instance}}\",\n          \"usage\": \"perFile\",\n          \"legacyBuild\": \"\",\n          \"metadata\": \"\",\n          \"omitMetadataKeys\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"pdfFile_0-output-pdfFile-Document\",\n            \"name\": \"pdfFile\",\n            \"label\": \"Document\",\n            \"description\": \"Load data from PDF files\",\n            \"type\": \"Document\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 507,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 787.634023360763,\n        \"y\": -684.0661302681481\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"tokenTextSplitter_0\",\n      \"position\": {\n        \"x\": 278.11347700836836,\n        \"y\": -235.12078789520618\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"tokenTextSplitter_0\",\n        \"label\": \"Token Text Splitter\",\n        \"version\": 1,\n        \"name\": \"tokenTextSplitter\",\n        \"type\": \"TokenTextSplitter\",\n        \"baseClasses\": [\n          \"TokenTextSplitter\",\n          \"TextSplitter\",\n          \"BaseDocumentTransformer\",\n          \"Runnable\"\n        ],\n        \"category\": \"Text Splitters\",\n        \"description\": \"Splits a raw text string by first converting the text into BPE tokens, then split these tokens into chunks and convert the tokens within a single chunk back into text.\",\n        \"inputParams\": [\n          {\n            \"label\": \"Encoding Name\",\n            \"name\": \"encodingName\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"gpt2\",\n                \"name\": \"gpt2\"\n              },\n              {\n                \"label\": \"r50k_base\",\n                \"name\": \"r50k_base\"\n              },\n              {\n                \"label\": \"p50k_base\",\n                \"name\": \"p50k_base\"\n              },\n              {\n                \"label\": \"p50k_edit\",\n                \"name\": \"p50k_edit\"\n              },\n              {\n                \"label\": \"cl100k_base\",\n                \"name\": \"cl100k_base\"\n              }\n            ],\n            \"default\": \"gpt2\",\n            \"id\": \"tokenTextSplitter_0-input-encodingName-options\"\n          },\n          {\n            \"label\": \"Chunk Size\",\n            \"name\": \"chunkSize\",\n            \"type\": \"number\",\n            \"description\": \"Number of characters in each chunk. Default is 1000.\",\n            \"default\": 1000,\n            \"optional\": true,\n            \"id\": \"tokenTextSplitter_0-input-chunkSize-number\"\n          },\n          {\n            \"label\": \"Chunk Overlap\",\n            \"name\": \"chunkOverlap\",\n            \"type\": \"number\",\n            \"description\": \"Number of characters to overlap between chunks. Default is 200.\",\n            \"default\": 200,\n            \"optional\": true,\n            \"id\": \"tokenTextSplitter_0-input-chunkOverlap-number\"\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"encodingName\": \"gpt2\",\n          \"chunkSize\": \"150\",\n          \"chunkOverlap\": \"50\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"tokenTextSplitter_0-output-tokenTextSplitter-TokenTextSplitter|TextSplitter|BaseDocumentTransformer|Runnable\",\n            \"name\": \"tokenTextSplitter\",\n            \"label\": \"TokenTextSplitter\",\n            \"description\": \"Splits a raw text string by first converting the text into BPE tokens, then split these tokens into chunks and convert the tokens within a single chunk back into text.\",\n            \"type\": \"TokenTextSplitter | TextSplitter | BaseDocumentTransformer | Runnable\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 472,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 278.11347700836836,\n        \"y\": -235.12078789520618\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"conversationalRetrievalQAChain_0\",\n      \"position\": {\n        \"x\": 1796.0622601449247,\n        \"y\": -239.60433948755286\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"conversationalRetrievalQAChain_0\",\n        \"label\": \"Conversational Retrieval QA Chain\",\n        \"version\": 3,\n        \"name\": \"conversationalRetrievalQAChain\",\n        \"type\": \"ConversationalRetrievalQAChain\",\n        \"baseClasses\": [\n          \"ConversationalRetrievalQAChain\",\n          \"BaseChain\",\n          \"Runnable\"\n        ],\n        \"category\": \"Chains\",\n        \"description\": \"Document QA - built on RetrievalQAChain to provide a chat history component\",\n        \"inputParams\": [\n          {\n            \"label\": \"Return Source Documents\",\n            \"name\": \"returnSourceDocuments\",\n            \"type\": \"boolean\",\n            \"optional\": true,\n            \"id\": \"conversationalRetrievalQAChain_0-input-returnSourceDocuments-boolean\"\n          },\n          {\n            \"label\": \"Rephrase Prompt\",\n            \"name\": \"rephrasePrompt\",\n            \"type\": \"string\",\n            \"description\": \"Using previous chat history, rephrase question into a standalone question\",\n            \"warning\": \"Prompt must include input variables: {chat_history} and {question}\",\n            \"rows\": 4,\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"default\": \"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone Question:\",\n            \"id\": \"conversationalRetrievalQAChain_0-input-rephrasePrompt-string\"\n          },\n          {\n            \"label\": \"Response Prompt\",\n            \"name\": \"responsePrompt\",\n            \"type\": \"string\",\n            \"description\": \"Taking the rephrased question, search for answer from the provided context\",\n            \"warning\": \"Prompt must include input variable: {context}\",\n            \"rows\": 4,\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"default\": \"I want you to act as a document that I am having a conversation with. Your name is \\\"AI Assistant\\\". Using the provided context, answer the user's question to the best of your ability using the resources provided.\\nIf there is nothing in the context relevant to the question at hand, just say \\\"Hmm, I'm not sure\\\" and stop after that. Refuse to answer any question not about the info. Never break character.\\n------------\\n{context}\\n------------\\nREMEMBER: If there is no relevant information within the context, just say \\\"Hmm, I'm not sure\\\". Don't try to make up an answer. Never break character.\",\n            \"id\": \"conversationalRetrievalQAChain_0-input-responsePrompt-string\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Chat Model\",\n            \"name\": \"model\",\n            \"type\": \"BaseChatModel\",\n            \"id\": \"conversationalRetrievalQAChain_0-input-model-BaseChatModel\"\n          },\n          {\n            \"label\": \"Vector Store Retriever\",\n            \"name\": \"vectorStoreRetriever\",\n            \"type\": \"BaseRetriever\",\n            \"id\": \"conversationalRetrievalQAChain_0-input-vectorStoreRetriever-BaseRetriever\"\n          },\n          {\n            \"label\": \"Memory\",\n            \"name\": \"memory\",\n            \"type\": \"BaseMemory\",\n            \"optional\": true,\n            \"description\": \"If left empty, a default BufferMemory will be used\",\n            \"id\": \"conversationalRetrievalQAChain_0-input-memory-BaseMemory\"\n          },\n          {\n            \"label\": \"Input Moderation\",\n            \"description\": \"Detect text that could generate harmful output and prevent it from being sent to the language model\",\n            \"name\": \"inputModeration\",\n            \"type\": \"Moderation\",\n            \"optional\": true,\n            \"list\": true,\n            \"id\": \"conversationalRetrievalQAChain_0-input-inputModeration-Moderation\"\n          }\n        ],\n        \"inputs\": {\n          \"model\": \"{{chatOpenAI_0.data.instance}}\",\n          \"vectorStoreRetriever\": \"{{pinecone_0.data.instance}}\",\n          \"memory\": \"\",\n          \"returnSourceDocuments\": true,\n          \"rephrasePrompt\": \"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone Question:\",\n          \"responsePrompt\": \"I want you to act as a document that I am having a conversation with. Your name is \\\"AI Assistant\\\". Using the provided context, answer the user's question to the best of your ability using the resources provided.\\nIf there is nothing in the context relevant to the question at hand, just say \\\"Hmm, I'm not sure\\\" and stop after that. Refuse to answer any question not about the info. Never break character.\\n------------\\n{context}\\n------------\\nREMEMBER: If there is no relevant information within the context, just say \\\"Hmm, I'm not sure\\\". Don't try to make up an answer. Never break character.\",\n          \"inputModeration\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"conversationalRetrievalQAChain_0-output-conversationalRetrievalQAChain-ConversationalRetrievalQAChain|BaseChain|Runnable\",\n            \"name\": \"conversationalRetrievalQAChain\",\n            \"label\": \"ConversationalRetrievalQAChain\",\n            \"description\": \"Document QA - built on RetrievalQAChain to provide a chat history component\",\n            \"type\": \"ConversationalRetrievalQAChain | BaseChain | Runnable\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 530,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 1796.0622601449247,\n        \"y\": -239.60433948755286\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"chatOpenAI_0\",\n      \"position\": {\n        \"x\": 1224.291242400906,\n        \"y\": -751.6041744992549\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"chatOpenAI_0\",\n        \"label\": \"ChatOpenAI\",\n        \"version\": 7,\n        \"name\": \"chatOpenAI\",\n        \"type\": \"ChatOpenAI\",\n        \"baseClasses\": [\n          \"ChatOpenAI\",\n          \"BaseChatModel\",\n          \"BaseLanguageModel\",\n          \"Runnable\"\n        ],\n        \"category\": \"Chat Models\",\n        \"description\": \"Wrapper around OpenAI large language models that use the Chat endpoint\",\n        \"inputParams\": [\n          {\n            \"label\": \"Connect Credential\",\n            \"name\": \"credential\",\n            \"type\": \"credential\",\n            \"credentialNames\": [\n              \"openAIApi\"\n            ],\n            \"id\": \"chatOpenAI_0-input-credential-credential\"\n          },\n          {\n            \"label\": \"Model Name\",\n            \"name\": \"modelName\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"default\": \"gpt-3.5-turbo\",\n            \"id\": \"chatOpenAI_0-input-modelName-asyncOptions\"\n          },\n          {\n            \"label\": \"Temperature\",\n            \"name\": \"temperature\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"default\": 0.9,\n            \"optional\": true,\n            \"id\": \"chatOpenAI_0-input-temperature-number\"\n          },\n          {\n            \"label\": \"Max Tokens\",\n            \"name\": \"maxTokens\",\n            \"type\": \"number\",\n            \"step\": 1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_0-input-maxTokens-number\"\n          },\n          {\n            \"label\": \"Top Probability\",\n            \"name\": \"topP\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_0-input-topP-number\"\n          },\n          {\n            \"label\": \"Frequency Penalty\",\n            \"name\": \"frequencyPenalty\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_0-input-frequencyPenalty-number\"\n          },\n          {\n            \"label\": \"Presence Penalty\",\n            \"name\": \"presencePenalty\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_0-input-presencePenalty-number\"\n          },\n          {\n            \"label\": \"Timeout\",\n            \"name\": \"timeout\",\n            \"type\": \"number\",\n            \"step\": 1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_0-input-timeout-number\"\n          },\n          {\n            \"label\": \"BasePath\",\n            \"name\": \"basepath\",\n            \"type\": \"string\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_0-input-basepath-string\"\n          },\n          {\n            \"label\": \"Proxy Url\",\n            \"name\": \"proxyUrl\",\n            \"type\": \"string\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_0-input-proxyUrl-string\"\n          },\n          {\n            \"label\": \"Stop Sequence\",\n            \"name\": \"stopSequence\",\n            \"type\": \"string\",\n            \"rows\": 4,\n            \"optional\": true,\n            \"description\": \"List of stop words to use when generating. Use comma to separate multiple stop words.\",\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_0-input-stopSequence-string\"\n          },\n          {\n            \"label\": \"BaseOptions\",\n            \"name\": \"baseOptions\",\n            \"type\": \"json\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_0-input-baseOptions-json\"\n          },\n          {\n            \"label\": \"Allow Image Uploads\",\n            \"name\": \"allowImageUploads\",\n            \"type\": \"boolean\",\n            \"description\": \"Automatically uses gpt-4-vision-preview when image is being uploaded from chat. Only works with LLMChain, Conversation Chain, ReAct Agent, Conversational Agent, Tool Agent\",\n            \"default\": false,\n            \"optional\": true,\n            \"id\": \"chatOpenAI_0-input-allowImageUploads-boolean\"\n          },\n          {\n            \"label\": \"Image Resolution\",\n            \"description\": \"This parameter controls the resolution in which the model views the image.\",\n            \"name\": \"imageResolution\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"Low\",\n                \"name\": \"low\"\n              },\n              {\n                \"label\": \"High\",\n                \"name\": \"high\"\n              },\n              {\n                \"label\": \"Auto\",\n                \"name\": \"auto\"\n              }\n            ],\n            \"default\": \"low\",\n            \"optional\": false,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_0-input-imageResolution-options\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Cache\",\n            \"name\": \"cache\",\n            \"type\": \"BaseCache\",\n            \"optional\": true,\n            \"id\": \"chatOpenAI_0-input-cache-BaseCache\"\n          }\n        ],\n        \"inputs\": {\n          \"cache\": \"\",\n          \"modelName\": \"gpt-3.5-turbo\",\n          \"temperature\": \"0.7\",\n          \"maxTokens\": \"\",\n          \"topP\": \"\",\n          \"frequencyPenalty\": \"\",\n          \"presencePenalty\": \"\",\n          \"timeout\": \"\",\n          \"basepath\": \"\",\n          \"proxyUrl\": \"\",\n          \"stopSequence\": \"\",\n          \"baseOptions\": \"\",\n          \"allowImageUploads\": false,\n          \"imageResolution\": \"low\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"chatOpenAI_0-output-chatOpenAI-ChatOpenAI|BaseChatModel|BaseLanguageModel|Runnable\",\n            \"name\": \"chatOpenAI\",\n            \"label\": \"ChatOpenAI\",\n            \"description\": \"Wrapper around OpenAI large language models that use the Chat endpoint\",\n            \"type\": \"ChatOpenAI | BaseChatModel | BaseLanguageModel | Runnable\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 669,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 1224.291242400906,\n        \"y\": -751.6041744992549\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"pinecone_0\",\n      \"position\": {\n        \"x\": 1247.1111194208547,\n        \"y\": -22.4754058214736\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"pinecone_0\",\n        \"label\": \"Pinecone\",\n        \"version\": 5,\n        \"name\": \"pinecone\",\n        \"type\": \"Pinecone\",\n        \"baseClasses\": [\n          \"Pinecone\",\n          \"VectorStoreRetriever\",\n          \"BaseRetriever\"\n        ],\n        \"category\": \"Vector Stores\",\n        \"description\": \"Upsert embedded data and perform similarity or mmr search using Pinecone, a leading fully managed hosted vector database\",\n        \"inputParams\": [\n          {\n            \"label\": \"Connect Credential\",\n            \"name\": \"credential\",\n            \"type\": \"credential\",\n            \"credentialNames\": [\n              \"pineconeApi\"\n            ],\n            \"id\": \"pinecone_0-input-credential-credential\"\n          },\n          {\n            \"label\": \"Pinecone Index\",\n            \"name\": \"pineconeIndex\",\n            \"type\": \"string\",\n            \"id\": \"pinecone_0-input-pineconeIndex-string\"\n          },\n          {\n            \"label\": \"Pinecone Namespace\",\n            \"name\": \"pineconeNamespace\",\n            \"type\": \"string\",\n            \"placeholder\": \"my-first-namespace\",\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"id\": \"pinecone_0-input-pineconeNamespace-string\"\n          },\n          {\n            \"label\": \"File Upload\",\n            \"name\": \"fileUpload\",\n            \"description\": \"Allow file upload on the chat\",\n            \"hint\": {\n              \"label\": \"How to use\",\n              \"value\": \"\\n**File Upload**\\n\\nThis allows file upload on the chat. Uploaded files will be upserted on the fly to the vector store.\\n\\n**Note:**\\n- You can only turn on file upload for one vector store at a time.\\n- At least one Document Loader node should be connected to the document input.\\n- Document Loader should be file types like PDF, DOCX, TXT, etc.\\n\\n**How it works**\\n- Uploaded files will have the metadata updated with the chatId.\\n- This will allow the file to be associated with the chatId.\\n- When querying, metadata will be filtered by chatId to retrieve files associated with the chatId.\\n\"\n            },\n            \"type\": \"boolean\",\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"id\": \"pinecone_0-input-fileUpload-boolean\"\n          },\n          {\n            \"label\": \"Pinecone Text Key\",\n            \"name\": \"pineconeTextKey\",\n            \"description\": \"The key in the metadata for storing text. Default to `text`\",\n            \"type\": \"string\",\n            \"placeholder\": \"text\",\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"id\": \"pinecone_0-input-pineconeTextKey-string\"\n          },\n          {\n            \"label\": \"Pinecone Metadata Filter\",\n            \"name\": \"pineconeMetadataFilter\",\n            \"type\": \"json\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"pinecone_0-input-pineconeMetadataFilter-json\"\n          },\n          {\n            \"label\": \"Top K\",\n            \"name\": \"topK\",\n            \"description\": \"Number of top results to fetch. Default to 4\",\n            \"placeholder\": \"4\",\n            \"type\": \"number\",\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"id\": \"pinecone_0-input-topK-number\"\n          },\n          {\n            \"label\": \"Search Type\",\n            \"name\": \"searchType\",\n            \"type\": \"options\",\n            \"default\": \"similarity\",\n            \"options\": [\n              {\n                \"label\": \"Similarity\",\n                \"name\": \"similarity\"\n              },\n              {\n                \"label\": \"Max Marginal Relevance\",\n                \"name\": \"mmr\"\n              }\n            ],\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"id\": \"pinecone_0-input-searchType-options\"\n          },\n          {\n            \"label\": \"Fetch K (for MMR Search)\",\n            \"name\": \"fetchK\",\n            \"description\": \"Number of initial documents to fetch for MMR reranking. Default to 20. Used only when the search type is MMR\",\n            \"placeholder\": \"20\",\n            \"type\": \"number\",\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"id\": \"pinecone_0-input-fetchK-number\"\n          },\n          {\n            \"label\": \"Lambda (for MMR Search)\",\n            \"name\": \"lambda\",\n            \"description\": \"Number between 0 and 1 that determines the degree of diversity among the results, where 0 corresponds to maximum diversity and 1 to minimum diversity. Used only when the search type is MMR\",\n            \"placeholder\": \"0.5\",\n            \"type\": \"number\",\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"id\": \"pinecone_0-input-lambda-number\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Document\",\n            \"name\": \"document\",\n            \"type\": \"Document\",\n            \"list\": true,\n            \"optional\": true,\n            \"id\": \"pinecone_0-input-document-Document\"\n          },\n          {\n            \"label\": \"Embeddings\",\n            \"name\": \"embeddings\",\n            \"type\": \"Embeddings\",\n            \"id\": \"pinecone_0-input-embeddings-Embeddings\"\n          },\n          {\n            \"label\": \"Record Manager\",\n            \"name\": \"recordManager\",\n            \"type\": \"RecordManager\",\n            \"description\": \"Keep track of the record to prevent duplication\",\n            \"optional\": true,\n            \"id\": \"pinecone_0-input-recordManager-RecordManager\"\n          }\n        ],\n        \"inputs\": {\n          \"document\": [\n            \"{{pdfFile_0.data.instance}}\"\n          ],\n          \"embeddings\": \"{{openAIEmbeddings_0.data.instance}}\",\n          \"recordManager\": \"\",\n          \"pineconeIndex\": \"new\",\n          \"pineconeNamespace\": \"\",\n          \"fileUpload\": \"\",\n          \"pineconeTextKey\": \"\",\n          \"pineconeMetadataFilter\": \"\",\n          \"topK\": \"\",\n          \"searchType\": \"similarity\",\n          \"fetchK\": \"\",\n          \"lambda\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"name\": \"output\",\n            \"label\": \"Output\",\n            \"type\": \"options\",\n            \"description\": \"\",\n            \"options\": [\n              {\n                \"id\": \"pinecone_0-output-retriever-Pinecone|VectorStoreRetriever|BaseRetriever\",\n                \"name\": \"retriever\",\n                \"label\": \"Pinecone Retriever\",\n                \"description\": \"\",\n                \"type\": \"Pinecone | VectorStoreRetriever | BaseRetriever\"\n              },\n              {\n                \"id\": \"pinecone_0-output-vectorStore-Pinecone|VectorStore\",\n                \"name\": \"vectorStore\",\n                \"label\": \"Pinecone Vector Store\",\n                \"description\": \"\",\n                \"type\": \"Pinecone | VectorStore\"\n              }\n            ],\n            \"default\": \"retriever\"\n          }\n        ],\n        \"outputs\": {\n          \"output\": \"vectorStore\"\n        },\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 604,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 1247.1111194208547,\n        \"y\": -22.4754058214736\n      },\n      \"dragging\": false\n    }\n  ],\n  \"edges\": [\n    {\n      \"source\": \"tokenTextSplitter_0\",\n      \"sourceHandle\": \"tokenTextSplitter_0-output-tokenTextSplitter-TokenTextSplitter|TextSplitter|BaseDocumentTransformer|Runnable\",\n      \"target\": \"pdfFile_0\",\n      \"targetHandle\": \"pdfFile_0-input-textSplitter-TextSplitter\",\n      \"type\": \"buttonedge\",\n      \"id\": \"tokenTextSplitter_0-tokenTextSplitter_0-output-tokenTextSplitter-TokenTextSplitter|TextSplitter|BaseDocumentTransformer|Runnable-pdfFile_0-pdfFile_0-input-textSplitter-TextSplitter\"\n    },\n    {\n      \"source\": \"chatOpenAI_0\",\n      \"sourceHandle\": \"chatOpenAI_0-output-chatOpenAI-ChatOpenAI|BaseChatModel|BaseLanguageModel|Runnable\",\n      \"target\": \"conversationalRetrievalQAChain_0\",\n      \"targetHandle\": \"conversationalRetrievalQAChain_0-input-model-BaseChatModel\",\n      \"type\": \"buttonedge\",\n      \"id\": \"chatOpenAI_0-chatOpenAI_0-output-chatOpenAI-ChatOpenAI|BaseChatModel|BaseLanguageModel|Runnable-conversationalRetrievalQAChain_0-conversationalRetrievalQAChain_0-input-model-BaseChatModel\"\n    },\n    {\n      \"source\": \"openAIEmbeddings_0\",\n      \"sourceHandle\": \"openAIEmbeddings_0-output-openAIEmbeddings-OpenAIEmbeddings|Embeddings\",\n      \"target\": \"pinecone_0\",\n      \"targetHandle\": \"pinecone_0-input-embeddings-Embeddings\",\n      \"type\": \"buttonedge\",\n      \"id\": \"openAIEmbeddings_0-openAIEmbeddings_0-output-openAIEmbeddings-OpenAIEmbeddings|Embeddings-pinecone_0-pinecone_0-input-embeddings-Embeddings\"\n    },\n    {\n      \"source\": \"pdfFile_0\",\n      \"sourceHandle\": \"pdfFile_0-output-pdfFile-Document\",\n      \"target\": \"pinecone_0\",\n      \"targetHandle\": \"pinecone_0-input-document-Document\",\n      \"type\": \"buttonedge\",\n      \"id\": \"pdfFile_0-pdfFile_0-output-pdfFile-Document-pinecone_0-pinecone_0-input-document-Document\"\n    },\n    {\n      \"source\": \"pinecone_0\",\n      \"sourceHandle\": \"pinecone_0-output-retriever-Pinecone|VectorStoreRetriever|BaseRetriever\",\n      \"target\": \"conversationalRetrievalQAChain_0\",\n      \"targetHandle\": \"conversationalRetrievalQAChain_0-input-vectorStoreRetriever-BaseRetriever\",\n      \"type\": \"buttonedge\",\n      \"id\": \"pinecone_0-pinecone_0-output-retriever-Pinecone|VectorStoreRetriever|BaseRetriever-conversationalRetrievalQAChain_0-conversationalRetrievalQAChain_0-input-vectorStoreRetriever-BaseRetriever\"\n    }\n  ]\n}",
      "type": "CHATFLOW"
    },
    {
      "id": "89599953-351c-4db3-9824-0d5ea298f39c",
      "name": "multilingualbot (1)",
      "flowData": "{\n  \"nodes\": [\n    {\n      \"id\": \"cohereEmbeddings_0\",\n      \"position\": {\n        \"x\": 272.42916536964475,\n        \"y\": 257.3020678993243\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"cohereEmbeddings_0\",\n        \"label\": \"Cohere Embeddings\",\n        \"version\": 3,\n        \"name\": \"cohereEmbeddings\",\n        \"type\": \"CohereEmbeddings\",\n        \"baseClasses\": [\n          \"CohereEmbeddings\",\n          \"Embeddings\"\n        ],\n        \"category\": \"Embeddings\",\n        \"description\": \"Cohere API to generate embeddings for a given text\",\n        \"inputParams\": [\n          {\n            \"label\": \"Connect Credential\",\n            \"name\": \"credential\",\n            \"type\": \"credential\",\n            \"credentialNames\": [\n              \"cohereApi\"\n            ],\n            \"id\": \"cohereEmbeddings_0-input-credential-credential\"\n          },\n          {\n            \"label\": \"Model Name\",\n            \"name\": \"modelName\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"default\": \"embed-english-v2.0\",\n            \"id\": \"cohereEmbeddings_0-input-modelName-asyncOptions\"\n          },\n          {\n            \"label\": \"Type\",\n            \"name\": \"inputType\",\n            \"type\": \"options\",\n            \"description\": \"Specifies the type of input passed to the model. Required for embedding models v3 and higher. <a target=\\\"_blank\\\" href=\\\"https://docs.cohere.com/reference/embed\\\">Official Docs</a>\",\n            \"options\": [\n              {\n                \"label\": \"search_document\",\n                \"name\": \"search_document\",\n                \"description\": \"Use this to encode documents for embeddings that you store in a vector database for search use-cases\"\n              },\n              {\n                \"label\": \"search_query\",\n                \"name\": \"search_query\",\n                \"description\": \"Use this when you query your vector DB to find relevant documents.\"\n              },\n              {\n                \"label\": \"classification\",\n                \"name\": \"classification\",\n                \"description\": \"Use this when you use the embeddings as an input to a text classifier\"\n              },\n              {\n                \"label\": \"clustering\",\n                \"name\": \"clustering\",\n                \"description\": \"Use this when you want to cluster the embeddings.\"\n              }\n            ],\n            \"default\": \"search_query\",\n            \"optional\": true,\n            \"id\": \"cohereEmbeddings_0-input-inputType-options\"\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"modelName\": \"embed-multilingual-v3.0\",\n          \"inputType\": \"search_query\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"cohereEmbeddings_0-output-cohereEmbeddings-CohereEmbeddings|Embeddings\",\n            \"name\": \"cohereEmbeddings\",\n            \"label\": \"CohereEmbeddings\",\n            \"description\": \"Cohere API to generate embeddings for a given text\",\n            \"type\": \"CohereEmbeddings | Embeddings\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 467,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 272.42916536964475,\n        \"y\": 257.3020678993243\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"pdfFile_0\",\n      \"position\": {\n        \"x\": -812.3024173963345,\n        \"y\": 505.33000531696723\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"pdfFile_0\",\n        \"label\": \"Pdf File\",\n        \"version\": 2,\n        \"name\": \"pdfFile\",\n        \"type\": \"Document\",\n        \"baseClasses\": [\n          \"Document\"\n        ],\n        \"category\": \"Document Loaders\",\n        \"description\": \"Load data from PDF files\",\n        \"inputParams\": [\n          {\n            \"label\": \"Pdf File\",\n            \"name\": \"pdfFile\",\n            \"type\": \"file\",\n            \"fileType\": \".pdf\",\n            \"id\": \"pdfFile_0-input-pdfFile-file\"\n          },\n          {\n            \"label\": \"Usage\",\n            \"name\": \"usage\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"One document per page\",\n                \"name\": \"perPage\"\n              },\n              {\n                \"label\": \"One document per file\",\n                \"name\": \"perFile\"\n              }\n            ],\n            \"default\": \"perPage\",\n            \"id\": \"pdfFile_0-input-usage-options\"\n          },\n          {\n            \"label\": \"Use Legacy Build\",\n            \"name\": \"legacyBuild\",\n            \"type\": \"boolean\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"pdfFile_0-input-legacyBuild-boolean\"\n          },\n          {\n            \"label\": \"Additional Metadata\",\n            \"name\": \"metadata\",\n            \"type\": \"json\",\n            \"description\": \"Additional metadata to be added to the extracted documents\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"pdfFile_0-input-metadata-json\"\n          },\n          {\n            \"label\": \"Omit Metadata Keys\",\n            \"name\": \"omitMetadataKeys\",\n            \"type\": \"string\",\n            \"rows\": 4,\n            \"description\": \"Each document loader comes with a default set of metadata keys that are extracted from the document. You can use this field to omit some of the default metadata keys. The value should be a list of keys, seperated by comma. Use * to omit all metadata keys execept the ones you specify in the Additional Metadata field\",\n            \"placeholder\": \"key1, key2, key3.nestedKey1\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"pdfFile_0-input-omitMetadataKeys-string\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Text Splitter\",\n            \"name\": \"textSplitter\",\n            \"type\": \"TextSplitter\",\n            \"optional\": true,\n            \"id\": \"pdfFile_0-input-textSplitter-TextSplitter\"\n          }\n        ],\n        \"inputs\": {\n          \"textSplitter\": \"\",\n          \"usage\": \"perPage\",\n          \"legacyBuild\": \"\",\n          \"metadata\": \"\",\n          \"omitMetadataKeys\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"name\": \"output\",\n            \"label\": \"Output\",\n            \"type\": \"options\",\n            \"description\": \"Array of document objects containing metadata and pageContent\",\n            \"options\": [\n              {\n                \"id\": \"pdfFile_0-output-document-Document|json\",\n                \"name\": \"document\",\n                \"label\": \"Document\",\n                \"description\": \"Array of document objects containing metadata and pageContent\",\n                \"type\": \"Document | json\"\n              },\n              {\n                \"id\": \"pdfFile_0-output-text-string|json\",\n                \"name\": \"text\",\n                \"label\": \"Text\",\n                \"description\": \"Concatenated string from pageContent of documents\",\n                \"type\": \"string | json\"\n              }\n            ],\n            \"default\": \"document\"\n          }\n        ],\n        \"outputs\": {\n          \"output\": \"document\"\n        },\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 534,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": -812.3024173963345,\n        \"y\": 505.33000531696723\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"faiss_0\",\n      \"position\": {\n        \"x\": 2023.7754065998608,\n        \"y\": 457.4835324343404\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"faiss_0\",\n        \"label\": \"Faiss\",\n        \"version\": 1,\n        \"name\": \"faiss\",\n        \"type\": \"Faiss\",\n        \"baseClasses\": [\n          \"Faiss\",\n          \"VectorStoreRetriever\",\n          \"BaseRetriever\"\n        ],\n        \"category\": \"Vector Stores\",\n        \"description\": \"Upsert embedded data and perform similarity search upon query using Faiss library from Meta\",\n        \"inputParams\": [\n          {\n            \"label\": \"Base Path to load\",\n            \"name\": \"basePath\",\n            \"description\": \"Path to load faiss.index file\",\n            \"placeholder\": \"C:\\\\Users\\\\User\\\\Desktop\",\n            \"type\": \"string\",\n            \"id\": \"faiss_0-input-basePath-string\"\n          },\n          {\n            \"label\": \"Top K\",\n            \"name\": \"topK\",\n            \"description\": \"Number of top results to fetch. Default to 4\",\n            \"placeholder\": \"4\",\n            \"type\": \"number\",\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"id\": \"faiss_0-input-topK-number\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Document\",\n            \"name\": \"document\",\n            \"type\": \"Document\",\n            \"list\": true,\n            \"optional\": true,\n            \"id\": \"faiss_0-input-document-Document\"\n          },\n          {\n            \"label\": \"Embeddings\",\n            \"name\": \"embeddings\",\n            \"type\": \"Embeddings\",\n            \"id\": \"faiss_0-input-embeddings-Embeddings\"\n          }\n        ],\n        \"inputs\": {\n          \"document\": [],\n          \"embeddings\": \"\",\n          \"basePath\": \"/opt/render/.flowise/1177-child\",\n          \"topK\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"name\": \"output\",\n            \"label\": \"Output\",\n            \"type\": \"options\",\n            \"description\": \"\",\n            \"options\": [\n              {\n                \"id\": \"faiss_0-output-retriever-Faiss|VectorStoreRetriever|BaseRetriever\",\n                \"name\": \"retriever\",\n                \"label\": \"Faiss Retriever\",\n                \"description\": \"\",\n                \"type\": \"Faiss | VectorStoreRetriever | BaseRetriever\"\n              },\n              {\n                \"id\": \"faiss_0-output-vectorStore-Faiss|SaveableVectorStore|VectorStore\",\n                \"name\": \"vectorStore\",\n                \"label\": \"Faiss Vector Store\",\n                \"description\": \"\",\n                \"type\": \"Faiss | SaveableVectorStore | VectorStore\"\n              }\n            ],\n            \"default\": \"retriever\"\n          }\n        ],\n        \"outputs\": {\n          \"output\": \"retriever\"\n        },\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 458,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 2023.7754065998608,\n        \"y\": 457.4835324343404\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"recursiveCharacterTextSplitter_0\",\n      \"position\": {\n        \"x\": -377.1782629011914,\n        \"y\": 574.8945504673474\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"recursiveCharacterTextSplitter_0\",\n        \"label\": \"Recursive Character Text Splitter\",\n        \"version\": 2,\n        \"name\": \"recursiveCharacterTextSplitter\",\n        \"type\": \"RecursiveCharacterTextSplitter\",\n        \"baseClasses\": [\n          \"RecursiveCharacterTextSplitter\",\n          \"TextSplitter\",\n          \"BaseDocumentTransformer\",\n          \"Runnable\"\n        ],\n        \"category\": \"Text Splitters\",\n        \"description\": \"Split documents recursively by different characters - starting with \\\"\\\\n\\\\n\\\", then \\\"\\\\n\\\", then \\\" \\\"\",\n        \"inputParams\": [\n          {\n            \"label\": \"Chunk Size\",\n            \"name\": \"chunkSize\",\n            \"type\": \"number\",\n            \"description\": \"Number of characters in each chunk. Default is 1000.\",\n            \"default\": 1000,\n            \"optional\": true,\n            \"id\": \"recursiveCharacterTextSplitter_0-input-chunkSize-number\"\n          },\n          {\n            \"label\": \"Chunk Overlap\",\n            \"name\": \"chunkOverlap\",\n            \"type\": \"number\",\n            \"description\": \"Number of characters to overlap between chunks. Default is 200.\",\n            \"default\": 200,\n            \"optional\": true,\n            \"id\": \"recursiveCharacterTextSplitter_0-input-chunkOverlap-number\"\n          },\n          {\n            \"label\": \"Custom Separators\",\n            \"name\": \"separators\",\n            \"type\": \"string\",\n            \"rows\": 4,\n            \"description\": \"Array of custom separators to determine when to split the text, will override the default separators\",\n            \"placeholder\": \"[\\\"|\\\", \\\"##\\\", \\\">\\\", \\\"-\\\"]\",\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"id\": \"recursiveCharacterTextSplitter_0-input-separators-string\"\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"chunkSize\": \"150\",\n          \"chunkOverlap\": \"50\",\n          \"separators\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"recursiveCharacterTextSplitter_0-output-recursiveCharacterTextSplitter-RecursiveCharacterTextSplitter|TextSplitter|BaseDocumentTransformer|Runnable\",\n            \"name\": \"recursiveCharacterTextSplitter\",\n            \"label\": \"RecursiveCharacterTextSplitter\",\n            \"description\": \"Split documents recursively by different characters - starting with \\\"\\\\n\\\\n\\\", then \\\"\\\\n\\\", then \\\" \\\"\",\n            \"type\": \"RecursiveCharacterTextSplitter | TextSplitter | BaseDocumentTransformer | Runnable\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 429,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": -377.1782629011914,\n        \"y\": 574.8945504673474\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"bufferMemory_0\",\n      \"position\": {\n        \"x\": 1312.073929836873,\n        \"y\": -291.26647000837363\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"bufferMemory_0\",\n        \"label\": \"Buffer Memory\",\n        \"version\": 2,\n        \"name\": \"bufferMemory\",\n        \"type\": \"BufferMemory\",\n        \"baseClasses\": [\n          \"BufferMemory\",\n          \"BaseChatMemory\",\n          \"BaseMemory\"\n        ],\n        \"category\": \"Memory\",\n        \"description\": \"Retrieve chat messages stored in database\",\n        \"inputParams\": [\n          {\n            \"label\": \"Session Id\",\n            \"name\": \"sessionId\",\n            \"type\": \"string\",\n            \"description\": \"If not specified, a random id will be used. Learn <a target=\\\"_blank\\\" href=\\\"https://docs.flowiseai.com/memory#ui-and-embedded-chat\\\">more</a>\",\n            \"default\": \"\",\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"id\": \"bufferMemory_0-input-sessionId-string\"\n          },\n          {\n            \"label\": \"Memory Key\",\n            \"name\": \"memoryKey\",\n            \"type\": \"string\",\n            \"default\": \"chat_history\",\n            \"additionalParams\": true,\n            \"id\": \"bufferMemory_0-input-memoryKey-string\"\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"sessionId\": \"\",\n          \"memoryKey\": \"chat_history\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"bufferMemory_0-output-bufferMemory-BufferMemory|BaseChatMemory|BaseMemory\",\n            \"name\": \"bufferMemory\",\n            \"label\": \"BufferMemory\",\n            \"description\": \"Retrieve chat messages stored in database\",\n            \"type\": \"BufferMemory | BaseChatMemory | BaseMemory\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 252,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 1312.073929836873,\n        \"y\": -291.26647000837363\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"conversationalRetrievalQAChain_0\",\n      \"position\": {\n        \"x\": 1572.6783013037793,\n        \"y\": 159.46579950343676\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"conversationalRetrievalQAChain_0\",\n        \"label\": \"Conversational Retrieval QA Chain\",\n        \"version\": 3,\n        \"name\": \"conversationalRetrievalQAChain\",\n        \"type\": \"ConversationalRetrievalQAChain\",\n        \"baseClasses\": [\n          \"ConversationalRetrievalQAChain\",\n          \"BaseChain\",\n          \"Runnable\"\n        ],\n        \"category\": \"Chains\",\n        \"description\": \"Document QA - built on RetrievalQAChain to provide a chat history component\",\n        \"inputParams\": [\n          {\n            \"label\": \"Return Source Documents\",\n            \"name\": \"returnSourceDocuments\",\n            \"type\": \"boolean\",\n            \"optional\": true,\n            \"id\": \"conversationalRetrievalQAChain_0-input-returnSourceDocuments-boolean\"\n          },\n          {\n            \"label\": \"Rephrase Prompt\",\n            \"name\": \"rephrasePrompt\",\n            \"type\": \"string\",\n            \"description\": \"Using previous chat history, rephrase question into a standalone question\",\n            \"warning\": \"Prompt must include input variables: {chat_history} and {question}\",\n            \"rows\": 4,\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"default\": \"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone Question:\",\n            \"id\": \"conversationalRetrievalQAChain_0-input-rephrasePrompt-string\"\n          },\n          {\n            \"label\": \"Response Prompt\",\n            \"name\": \"responsePrompt\",\n            \"type\": \"string\",\n            \"description\": \"Taking the rephrased question, search for answer from the provided context\",\n            \"warning\": \"Prompt must include input variable: {context}\",\n            \"rows\": 4,\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"default\": \"I want you to act as a document that I am having a conversation with. Your name is \\\"AI Assistant\\\". Using the provided context, answer the user's question to the best of your ability using the resources provided.\\nIf there is nothing in the context relevant to the question at hand, just say \\\"Hmm, I'm not sure\\\" and stop after that. Refuse to answer any question not about the info. Never break character.\\n------------\\n{context}\\n------------\\nREMEMBER: If there is no relevant information within the context, just say \\\"Hmm, I'm not sure\\\". Don't try to make up an answer. Never break character.\",\n            \"id\": \"conversationalRetrievalQAChain_0-input-responsePrompt-string\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Chat Model\",\n            \"name\": \"model\",\n            \"type\": \"BaseChatModel\",\n            \"id\": \"conversationalRetrievalQAChain_0-input-model-BaseChatModel\"\n          },\n          {\n            \"label\": \"Vector Store Retriever\",\n            \"name\": \"vectorStoreRetriever\",\n            \"type\": \"BaseRetriever\",\n            \"id\": \"conversationalRetrievalQAChain_0-input-vectorStoreRetriever-BaseRetriever\"\n          },\n          {\n            \"label\": \"Memory\",\n            \"name\": \"memory\",\n            \"type\": \"BaseMemory\",\n            \"optional\": true,\n            \"description\": \"If left empty, a default BufferMemory will be used\",\n            \"id\": \"conversationalRetrievalQAChain_0-input-memory-BaseMemory\"\n          },\n          {\n            \"label\": \"Input Moderation\",\n            \"description\": \"Detect text that could generate harmful output and prevent it from being sent to the language model\",\n            \"name\": \"inputModeration\",\n            \"type\": \"Moderation\",\n            \"optional\": true,\n            \"list\": true,\n            \"id\": \"conversationalRetrievalQAChain_0-input-inputModeration-Moderation\"\n          }\n        ],\n        \"inputs\": {\n          \"model\": \"{{chatCohere_0.data.instance}}\",\n          \"vectorStoreRetriever\": \"{{pinecone_0.data.instance}}\",\n          \"memory\": \"{{bufferMemory_0.data.instance}}\",\n          \"returnSourceDocuments\": true,\n          \"rephrasePrompt\": \"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone Question:\",\n          \"responsePrompt\": \"I want you to act as a document that I am having a conversation with. Your name is \\\"AI Assistant\\\". Using the provided context, answer the user's question to the best of your ability using the resources provided.\\nIf there is nothing in the context relevant to the question at hand, just say \\\"Hmm, I'm not sure\\\" and stop after that. Refuse to answer any question not about the info. Never break character.\\n------------\\n{context}\\n------------\\nREMEMBER: If there is no relevant information within the context, just say \\\"Hmm, I'm not sure\\\". Don't try to make up an answer. Never break character.\",\n          \"inputModeration\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"conversationalRetrievalQAChain_0-output-conversationalRetrievalQAChain-ConversationalRetrievalQAChain|BaseChain|Runnable\",\n            \"name\": \"conversationalRetrievalQAChain\",\n            \"label\": \"ConversationalRetrievalQAChain\",\n            \"description\": \"Document QA - built on RetrievalQAChain to provide a chat history component\",\n            \"type\": \"ConversationalRetrievalQAChain | BaseChain | Runnable\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 531,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 1572.6783013037793,\n        \"y\": 159.46579950343676\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"docxFile_0\",\n      \"position\": {\n        \"x\": 326.18103017722296,\n        \"y\": -460.8040267341804\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"docxFile_0\",\n        \"label\": \"Docx File\",\n        \"version\": 2,\n        \"name\": \"docxFile\",\n        \"type\": \"Document\",\n        \"baseClasses\": [\n          \"Document\"\n        ],\n        \"category\": \"Document Loaders\",\n        \"description\": \"Load data from DOCX files\",\n        \"inputParams\": [\n          {\n            \"label\": \"Docx File\",\n            \"name\": \"docxFile\",\n            \"type\": \"file\",\n            \"fileType\": \".docx\",\n            \"id\": \"docxFile_0-input-docxFile-file\"\n          },\n          {\n            \"label\": \"Additional Metadata\",\n            \"name\": \"metadata\",\n            \"type\": \"json\",\n            \"description\": \"Additional metadata to be added to the extracted documents\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"docxFile_0-input-metadata-json\"\n          },\n          {\n            \"label\": \"Omit Metadata Keys\",\n            \"name\": \"omitMetadataKeys\",\n            \"type\": \"string\",\n            \"rows\": 4,\n            \"description\": \"Each document loader comes with a default set of metadata keys that are extracted from the document. You can use this field to omit some of the default metadata keys. The value should be a list of keys, seperated by comma. Use * to omit all metadata keys execept the ones you specify in the Additional Metadata field\",\n            \"placeholder\": \"key1, key2, key3.nestedKey1\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"docxFile_0-input-omitMetadataKeys-string\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Text Splitter\",\n            \"name\": \"textSplitter\",\n            \"type\": \"TextSplitter\",\n            \"optional\": true,\n            \"id\": \"docxFile_0-input-textSplitter-TextSplitter\"\n          }\n        ],\n        \"inputs\": {\n          \"textSplitter\": \"{{recursiveCharacterTextSplitter_1.data.instance}}\",\n          \"metadata\": \"\",\n          \"omitMetadataKeys\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"name\": \"output\",\n            \"label\": \"Output\",\n            \"type\": \"options\",\n            \"description\": \"Array of document objects containing metadata and pageContent\",\n            \"options\": [\n              {\n                \"id\": \"docxFile_0-output-document-Document|json\",\n                \"name\": \"document\",\n                \"label\": \"Document\",\n                \"description\": \"Array of document objects containing metadata and pageContent\",\n                \"type\": \"Document | json\"\n              },\n              {\n                \"id\": \"docxFile_0-output-text-string|json\",\n                \"name\": \"text\",\n                \"label\": \"Text\",\n                \"description\": \"Concatenated string from pageContent of documents\",\n                \"type\": \"string | json\"\n              }\n            ],\n            \"default\": \"document\"\n          }\n        ],\n        \"outputs\": {\n          \"output\": \"document\"\n        },\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 438,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 326.18103017722296,\n        \"y\": -460.8040267341804\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"pinecone_0\",\n      \"position\": {\n        \"x\": 870.8904261696961,\n        \"y\": 183.24303042421127\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"pinecone_0\",\n        \"label\": \"Pinecone\",\n        \"version\": 5,\n        \"name\": \"pinecone\",\n        \"type\": \"Pinecone\",\n        \"baseClasses\": [\n          \"Pinecone\",\n          \"VectorStoreRetriever\",\n          \"BaseRetriever\"\n        ],\n        \"category\": \"Vector Stores\",\n        \"description\": \"Upsert embedded data and perform similarity or mmr search using Pinecone, a leading fully managed hosted vector database\",\n        \"inputParams\": [\n          {\n            \"label\": \"Connect Credential\",\n            \"name\": \"credential\",\n            \"type\": \"credential\",\n            \"credentialNames\": [\n              \"pineconeApi\"\n            ],\n            \"id\": \"pinecone_0-input-credential-credential\"\n          },\n          {\n            \"label\": \"Pinecone Index\",\n            \"name\": \"pineconeIndex\",\n            \"type\": \"string\",\n            \"id\": \"pinecone_0-input-pineconeIndex-string\"\n          },\n          {\n            \"label\": \"Pinecone Namespace\",\n            \"name\": \"pineconeNamespace\",\n            \"type\": \"string\",\n            \"placeholder\": \"my-first-namespace\",\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"id\": \"pinecone_0-input-pineconeNamespace-string\"\n          },\n          {\n            \"label\": \"File Upload\",\n            \"name\": \"fileUpload\",\n            \"description\": \"Allow file upload on the chat\",\n            \"hint\": {\n              \"label\": \"How to use\",\n              \"value\": \"\\n**File Upload**\\n\\nThis allows file upload on the chat. Uploaded files will be upserted on the fly to the vector store.\\n\\n**Note:**\\n- You can only turn on file upload for one vector store at a time.\\n- At least one Document Loader node should be connected to the document input.\\n- Document Loader should be file types like PDF, DOCX, TXT, etc.\\n\\n**How it works**\\n- Uploaded files will have the metadata updated with the chatId.\\n- This will allow the file to be associated with the chatId.\\n- When querying, metadata will be filtered by chatId to retrieve files associated with the chatId.\\n\"\n            },\n            \"type\": \"boolean\",\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"id\": \"pinecone_0-input-fileUpload-boolean\"\n          },\n          {\n            \"label\": \"Pinecone Text Key\",\n            \"name\": \"pineconeTextKey\",\n            \"description\": \"The key in the metadata for storing text. Default to `text`\",\n            \"type\": \"string\",\n            \"placeholder\": \"text\",\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"id\": \"pinecone_0-input-pineconeTextKey-string\"\n          },\n          {\n            \"label\": \"Pinecone Metadata Filter\",\n            \"name\": \"pineconeMetadataFilter\",\n            \"type\": \"json\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"pinecone_0-input-pineconeMetadataFilter-json\"\n          },\n          {\n            \"label\": \"Top K\",\n            \"name\": \"topK\",\n            \"description\": \"Number of top results to fetch. Default to 4\",\n            \"placeholder\": \"4\",\n            \"type\": \"number\",\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"id\": \"pinecone_0-input-topK-number\"\n          },\n          {\n            \"label\": \"Search Type\",\n            \"name\": \"searchType\",\n            \"type\": \"options\",\n            \"default\": \"similarity\",\n            \"options\": [\n              {\n                \"label\": \"Similarity\",\n                \"name\": \"similarity\"\n              },\n              {\n                \"label\": \"Max Marginal Relevance\",\n                \"name\": \"mmr\"\n              }\n            ],\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"id\": \"pinecone_0-input-searchType-options\"\n          },\n          {\n            \"label\": \"Fetch K (for MMR Search)\",\n            \"name\": \"fetchK\",\n            \"description\": \"Number of initial documents to fetch for MMR reranking. Default to 20. Used only when the search type is MMR\",\n            \"placeholder\": \"20\",\n            \"type\": \"number\",\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"id\": \"pinecone_0-input-fetchK-number\"\n          },\n          {\n            \"label\": \"Lambda (for MMR Search)\",\n            \"name\": \"lambda\",\n            \"description\": \"Number between 0 and 1 that determines the degree of diversity among the results, where 0 corresponds to maximum diversity and 1 to minimum diversity. Used only when the search type is MMR\",\n            \"placeholder\": \"0.5\",\n            \"type\": \"number\",\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"id\": \"pinecone_0-input-lambda-number\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Document\",\n            \"name\": \"document\",\n            \"type\": \"Document\",\n            \"list\": true,\n            \"optional\": true,\n            \"id\": \"pinecone_0-input-document-Document\"\n          },\n          {\n            \"label\": \"Embeddings\",\n            \"name\": \"embeddings\",\n            \"type\": \"Embeddings\",\n            \"id\": \"pinecone_0-input-embeddings-Embeddings\"\n          },\n          {\n            \"label\": \"Record Manager\",\n            \"name\": \"recordManager\",\n            \"type\": \"RecordManager\",\n            \"description\": \"Keep track of the record to prevent duplication\",\n            \"optional\": true,\n            \"id\": \"pinecone_0-input-recordManager-RecordManager\"\n          }\n        ],\n        \"inputs\": {\n          \"document\": [\n            \"{{docxFile_0.data.instance}}\"\n          ],\n          \"embeddings\": \"{{cohereEmbeddings_0.data.instance}}\",\n          \"recordManager\": \"\",\n          \"pineconeIndex\": \"portfolio\",\n          \"pineconeNamespace\": \"\",\n          \"fileUpload\": \"\",\n          \"pineconeTextKey\": \"\",\n          \"pineconeMetadataFilter\": \"\",\n          \"topK\": \"\",\n          \"searchType\": \"similarity\",\n          \"fetchK\": \"\",\n          \"lambda\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"name\": \"output\",\n            \"label\": \"Output\",\n            \"type\": \"options\",\n            \"description\": \"\",\n            \"options\": [\n              {\n                \"id\": \"pinecone_0-output-retriever-Pinecone|VectorStoreRetriever|BaseRetriever\",\n                \"name\": \"retriever\",\n                \"label\": \"Pinecone Retriever\",\n                \"description\": \"\",\n                \"type\": \"Pinecone | VectorStoreRetriever | BaseRetriever\"\n              },\n              {\n                \"id\": \"pinecone_0-output-vectorStore-Pinecone|VectorStore\",\n                \"name\": \"vectorStore\",\n                \"label\": \"Pinecone Vector Store\",\n                \"description\": \"\",\n                \"type\": \"Pinecone | VectorStore\"\n              }\n            ],\n            \"default\": \"retriever\"\n          }\n        ],\n        \"outputs\": {\n          \"output\": \"retriever\"\n        },\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 605,\n      \"selected\": false,\n      \"dragging\": false,\n      \"positionAbsolute\": {\n        \"x\": 870.8904261696961,\n        \"y\": 183.24303042421127\n      }\n    },\n    {\n      \"id\": \"recursiveCharacterTextSplitter_1\",\n      \"position\": {\n        \"x\": -224.57801171570682,\n        \"y\": -421.19315140536423\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"recursiveCharacterTextSplitter_1\",\n        \"label\": \"Recursive Character Text Splitter\",\n        \"version\": 2,\n        \"name\": \"recursiveCharacterTextSplitter\",\n        \"type\": \"RecursiveCharacterTextSplitter\",\n        \"baseClasses\": [\n          \"RecursiveCharacterTextSplitter\",\n          \"TextSplitter\",\n          \"BaseDocumentTransformer\",\n          \"Runnable\"\n        ],\n        \"category\": \"Text Splitters\",\n        \"description\": \"Split documents recursively by different characters - starting with \\\"\\\\n\\\\n\\\", then \\\"\\\\n\\\", then \\\" \\\"\",\n        \"inputParams\": [\n          {\n            \"label\": \"Chunk Size\",\n            \"name\": \"chunkSize\",\n            \"type\": \"number\",\n            \"description\": \"Number of characters in each chunk. Default is 1000.\",\n            \"default\": 1000,\n            \"optional\": true,\n            \"id\": \"recursiveCharacterTextSplitter_1-input-chunkSize-number\"\n          },\n          {\n            \"label\": \"Chunk Overlap\",\n            \"name\": \"chunkOverlap\",\n            \"type\": \"number\",\n            \"description\": \"Number of characters to overlap between chunks. Default is 200.\",\n            \"default\": 200,\n            \"optional\": true,\n            \"id\": \"recursiveCharacterTextSplitter_1-input-chunkOverlap-number\"\n          },\n          {\n            \"label\": \"Custom Separators\",\n            \"name\": \"separators\",\n            \"type\": \"string\",\n            \"rows\": 4,\n            \"description\": \"Array of custom separators to determine when to split the text, will override the default separators\",\n            \"placeholder\": \"[\\\"|\\\", \\\"##\\\", \\\">\\\", \\\"-\\\"]\",\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"id\": \"recursiveCharacterTextSplitter_1-input-separators-string\"\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"chunkSize\": 1000,\n          \"chunkOverlap\": 200,\n          \"separators\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"recursiveCharacterTextSplitter_1-output-recursiveCharacterTextSplitter-RecursiveCharacterTextSplitter|TextSplitter|BaseDocumentTransformer|Runnable\",\n            \"name\": \"recursiveCharacterTextSplitter\",\n            \"label\": \"RecursiveCharacterTextSplitter\",\n            \"description\": \"Split documents recursively by different characters - starting with \\\"\\\\n\\\\n\\\", then \\\"\\\\n\\\", then \\\" \\\"\",\n            \"type\": \"RecursiveCharacterTextSplitter | TextSplitter | BaseDocumentTransformer | Runnable\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 429,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": -224.57801171570682,\n        \"y\": -421.19315140536423\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"chatCohere_0\",\n      \"position\": {\n        \"x\": 819.7579709297781,\n        \"y\": -605.9743119260077\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"chatCohere_0\",\n        \"label\": \"ChatCohere\",\n        \"version\": 2,\n        \"name\": \"chatCohere\",\n        \"type\": \"ChatCohere\",\n        \"baseClasses\": [\n          \"ChatCohere\",\n          \"BaseChatModel\",\n          \"BaseLanguageModel\",\n          \"Runnable\"\n        ],\n        \"category\": \"Chat Models\",\n        \"description\": \"Wrapper around Cohere Chat Endpoints\",\n        \"inputParams\": [\n          {\n            \"label\": \"Connect Credential\",\n            \"name\": \"credential\",\n            \"type\": \"credential\",\n            \"credentialNames\": [\n              \"cohereApi\"\n            ],\n            \"id\": \"chatCohere_0-input-credential-credential\"\n          },\n          {\n            \"label\": \"Model Name\",\n            \"name\": \"modelName\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"default\": \"command-r\",\n            \"id\": \"chatCohere_0-input-modelName-asyncOptions\"\n          },\n          {\n            \"label\": \"Temperature\",\n            \"name\": \"temperature\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"default\": 0.7,\n            \"optional\": true,\n            \"id\": \"chatCohere_0-input-temperature-number\"\n          },\n          {\n            \"label\": \"Streaming\",\n            \"name\": \"streaming\",\n            \"type\": \"boolean\",\n            \"default\": true,\n            \"optional\": true,\n            \"id\": \"chatCohere_0-input-streaming-boolean\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Cache\",\n            \"name\": \"cache\",\n            \"type\": \"BaseCache\",\n            \"optional\": true,\n            \"id\": \"chatCohere_0-input-cache-BaseCache\"\n          }\n        ],\n        \"inputs\": {\n          \"cache\": \"\",\n          \"modelName\": \"command-r\",\n          \"temperature\": 0.7,\n          \"streaming\": true\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"chatCohere_0-output-chatCohere-ChatCohere|BaseChatModel|BaseLanguageModel|Runnable\",\n            \"name\": \"chatCohere\",\n            \"label\": \"ChatCohere\",\n            \"description\": \"Wrapper around Cohere Chat Endpoints\",\n            \"type\": \"ChatCohere | BaseChatModel | BaseLanguageModel | Runnable\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 617,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 819.7579709297781,\n        \"y\": -605.9743119260077\n      },\n      \"dragging\": false\n    }\n  ],\n  \"edges\": [\n    {\n      \"source\": \"bufferMemory_0\",\n      \"sourceHandle\": \"bufferMemory_0-output-bufferMemory-BufferMemory|BaseChatMemory|BaseMemory\",\n      \"target\": \"conversationalRetrievalQAChain_0\",\n      \"targetHandle\": \"conversationalRetrievalQAChain_0-input-memory-BaseMemory\",\n      \"type\": \"buttonedge\",\n      \"id\": \"bufferMemory_0-bufferMemory_0-output-bufferMemory-BufferMemory|BaseChatMemory|BaseMemory-conversationalRetrievalQAChain_0-conversationalRetrievalQAChain_0-input-memory-BaseMemory\"\n    },\n    {\n      \"source\": \"docxFile_0\",\n      \"sourceHandle\": \"docxFile_0-output-document-Document|json\",\n      \"target\": \"pinecone_0\",\n      \"targetHandle\": \"pinecone_0-input-document-Document\",\n      \"type\": \"buttonedge\",\n      \"id\": \"docxFile_0-docxFile_0-output-document-Document|json-pinecone_0-pinecone_0-input-document-Document\"\n    },\n    {\n      \"source\": \"pinecone_0\",\n      \"sourceHandle\": \"pinecone_0-output-retriever-Pinecone|VectorStoreRetriever|BaseRetriever\",\n      \"target\": \"conversationalRetrievalQAChain_0\",\n      \"targetHandle\": \"conversationalRetrievalQAChain_0-input-vectorStoreRetriever-BaseRetriever\",\n      \"type\": \"buttonedge\",\n      \"id\": \"pinecone_0-pinecone_0-output-retriever-Pinecone|VectorStoreRetriever|BaseRetriever-conversationalRetrievalQAChain_0-conversationalRetrievalQAChain_0-input-vectorStoreRetriever-BaseRetriever\"\n    },\n    {\n      \"source\": \"cohereEmbeddings_0\",\n      \"sourceHandle\": \"cohereEmbeddings_0-output-cohereEmbeddings-CohereEmbeddings|Embeddings\",\n      \"target\": \"pinecone_0\",\n      \"targetHandle\": \"pinecone_0-input-embeddings-Embeddings\",\n      \"type\": \"buttonedge\",\n      \"id\": \"cohereEmbeddings_0-cohereEmbeddings_0-output-cohereEmbeddings-CohereEmbeddings|Embeddings-pinecone_0-pinecone_0-input-embeddings-Embeddings\"\n    },\n    {\n      \"source\": \"recursiveCharacterTextSplitter_1\",\n      \"sourceHandle\": \"recursiveCharacterTextSplitter_1-output-recursiveCharacterTextSplitter-RecursiveCharacterTextSplitter|TextSplitter|BaseDocumentTransformer|Runnable\",\n      \"target\": \"docxFile_0\",\n      \"targetHandle\": \"docxFile_0-input-textSplitter-TextSplitter\",\n      \"type\": \"buttonedge\",\n      \"id\": \"recursiveCharacterTextSplitter_1-recursiveCharacterTextSplitter_1-output-recursiveCharacterTextSplitter-RecursiveCharacterTextSplitter|TextSplitter|BaseDocumentTransformer|Runnable-docxFile_0-docxFile_0-input-textSplitter-TextSplitter\"\n    },\n    {\n      \"source\": \"chatCohere_0\",\n      \"sourceHandle\": \"chatCohere_0-output-chatCohere-ChatCohere|BaseChatModel|BaseLanguageModel|Runnable\",\n      \"target\": \"conversationalRetrievalQAChain_0\",\n      \"targetHandle\": \"conversationalRetrievalQAChain_0-input-model-BaseChatModel\",\n      \"type\": \"buttonedge\",\n      \"id\": \"chatCohere_0-chatCohere_0-output-chatCohere-ChatCohere|BaseChatModel|BaseLanguageModel|Runnable-conversationalRetrievalQAChain_0-conversationalRetrievalQAChain_0-input-model-BaseChatModel\"\n    }\n  ]\n}",
      "type": "CHATFLOW"
    },
    {
      "id": "6fb05ef9-8364-41bc-adf1-e373e0d54869",
      "name": "grok+llama (1)",
      "flowData": "{\n  \"nodes\": [\n    {\n      \"id\": \"groqChat_0\",\n      \"position\": {\n        \"x\": 537.9587263279853,\n        \"y\": -135.886011176785\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"groqChat_0\",\n        \"label\": \"GroqChat\",\n        \"version\": 3,\n        \"name\": \"groqChat\",\n        \"type\": \"GroqChat\",\n        \"baseClasses\": [\n          \"GroqChat\",\n          \"BaseChatModel\",\n          \"BaseLanguageModel\",\n          \"Runnable\"\n        ],\n        \"category\": \"Chat Models\",\n        \"description\": \"Wrapper around Groq API with LPU Inference Engine\",\n        \"inputParams\": [\n          {\n            \"label\": \"Connect Credential\",\n            \"name\": \"credential\",\n            \"type\": \"credential\",\n            \"credentialNames\": [\n              \"groqApi\"\n            ],\n            \"optional\": true,\n            \"id\": \"groqChat_0-input-credential-credential\"\n          },\n          {\n            \"label\": \"Model Name\",\n            \"name\": \"modelName\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"placeholder\": \"llama3-70b-8192\",\n            \"id\": \"groqChat_0-input-modelName-asyncOptions\"\n          },\n          {\n            \"label\": \"Temperature\",\n            \"name\": \"temperature\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"default\": 0.9,\n            \"optional\": true,\n            \"id\": \"groqChat_0-input-temperature-number\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Cache\",\n            \"name\": \"cache\",\n            \"type\": \"BaseCache\",\n            \"optional\": true,\n            \"id\": \"groqChat_0-input-cache-BaseCache\"\n          }\n        ],\n        \"inputs\": {\n          \"cache\": \"\",\n          \"modelName\": \"llama-3.2-3b-preview\",\n          \"temperature\": \"0.2\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"groqChat_0-output-groqChat-GroqChat|BaseChatModel|BaseLanguageModel|Runnable\",\n            \"name\": \"groqChat\",\n            \"label\": \"GroqChat\",\n            \"description\": \"Wrapper around Groq API with LPU Inference Engine\",\n            \"type\": \"GroqChat | BaseChatModel | BaseLanguageModel | Runnable\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 519,\n      \"selected\": false,\n      \"dragging\": false,\n      \"positionAbsolute\": {\n        \"x\": 537.9587263279853,\n        \"y\": -135.886011176785\n      }\n    },\n    {\n      \"id\": \"ollama_0\",\n      \"position\": {\n        \"x\": 1368.8171609309877,\n        \"y\": 34.01899639575146\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"ollama_0\",\n        \"label\": \"Ollama\",\n        \"version\": 2,\n        \"name\": \"ollama\",\n        \"type\": \"Ollama\",\n        \"baseClasses\": [\n          \"Ollama\",\n          \"LLM\",\n          \"BaseLLM\",\n          \"BaseLanguageModel\",\n          \"Runnable\"\n        ],\n        \"category\": \"LLMs\",\n        \"description\": \"Wrapper around open source large language models on Ollama\",\n        \"inputParams\": [\n          {\n            \"label\": \"Base URL\",\n            \"name\": \"baseUrl\",\n            \"type\": \"string\",\n            \"default\": \"http://localhost:11434\",\n            \"id\": \"ollama_0-input-baseUrl-string\"\n          },\n          {\n            \"label\": \"Model Name\",\n            \"name\": \"modelName\",\n            \"type\": \"string\",\n            \"placeholder\": \"llama2\",\n            \"id\": \"ollama_0-input-modelName-string\"\n          },\n          {\n            \"label\": \"Temperature\",\n            \"name\": \"temperature\",\n            \"type\": \"number\",\n            \"description\": \"The temperature of the model. Increasing the temperature will make the model answer more creatively. (Default: 0.8). Refer to <a target=\\\"_blank\\\" href=\\\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\\\">docs</a> for more details\",\n            \"step\": 0.1,\n            \"default\": 0.9,\n            \"optional\": true,\n            \"id\": \"ollama_0-input-temperature-number\"\n          },\n          {\n            \"label\": \"Top P\",\n            \"name\": \"topP\",\n            \"type\": \"number\",\n            \"description\": \"Works together with top-k. A higher value (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. (Default: 0.9). Refer to <a target=\\\"_blank\\\" href=\\\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\\\">docs</a> for more details\",\n            \"step\": 0.1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"ollama_0-input-topP-number\"\n          },\n          {\n            \"label\": \"Top K\",\n            \"name\": \"topK\",\n            \"type\": \"number\",\n            \"description\": \"Reduces the probability of generating nonsense. A higher value (e.g. 100) will give more diverse answers, while a lower value (e.g. 10) will be more conservative. (Default: 40). Refer to <a target=\\\"_blank\\\" href=\\\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\\\">docs</a> for more details\",\n            \"step\": 1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"ollama_0-input-topK-number\"\n          },\n          {\n            \"label\": \"Mirostat\",\n            \"name\": \"mirostat\",\n            \"type\": \"number\",\n            \"description\": \"Enable Mirostat sampling for controlling perplexity. (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0). Refer to <a target=\\\"_blank\\\" href=\\\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\\\">docs</a> for more details\",\n            \"step\": 1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"ollama_0-input-mirostat-number\"\n          },\n          {\n            \"label\": \"Mirostat ETA\",\n            \"name\": \"mirostatEta\",\n            \"type\": \"number\",\n            \"description\": \"Influences how quickly the algorithm responds to feedback from the generated text. A lower learning rate will result in slower adjustments, while a higher learning rate will make the algorithm more responsive. (Default: 0.1) Refer to <a target=\\\"_blank\\\" href=\\\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\\\">docs</a> for more details\",\n            \"step\": 0.1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"ollama_0-input-mirostatEta-number\"\n          },\n          {\n            \"label\": \"Mirostat TAU\",\n            \"name\": \"mirostatTau\",\n            \"type\": \"number\",\n            \"description\": \"Controls the balance between coherence and diversity of the output. A lower value will result in more focused and coherent text. (Default: 5.0) Refer to <a target=\\\"_blank\\\" href=\\\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\\\">docs</a> for more details\",\n            \"step\": 0.1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"ollama_0-input-mirostatTau-number\"\n          },\n          {\n            \"label\": \"Context Window Size\",\n            \"name\": \"numCtx\",\n            \"type\": \"number\",\n            \"description\": \"Sets the size of the context window used to generate the next token. (Default: 2048) Refer to <a target=\\\"_blank\\\" href=\\\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\\\">docs</a> for more details\",\n            \"step\": 1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"ollama_0-input-numCtx-number\"\n          },\n          {\n            \"label\": \"Number of GQA groups\",\n            \"name\": \"numGqa\",\n            \"type\": \"number\",\n            \"description\": \"The number of GQA groups in the transformer layer. Required for some models, for example it is 8 for llama2:70b. Refer to <a target=\\\"_blank\\\" href=\\\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\\\">docs</a> for more details\",\n            \"step\": 1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"ollama_0-input-numGqa-number\"\n          },\n          {\n            \"label\": \"Number of GPU\",\n            \"name\": \"numGpu\",\n            \"type\": \"number\",\n            \"description\": \"The number of layers to send to the GPU(s). On macOS it defaults to 1 to enable metal support, 0 to disable. Refer to <a target=\\\"_blank\\\" href=\\\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\\\">docs</a> for more details\",\n            \"step\": 1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"ollama_0-input-numGpu-number\"\n          },\n          {\n            \"label\": \"Number of Thread\",\n            \"name\": \"numThread\",\n            \"type\": \"number\",\n            \"description\": \"Sets the number of threads to use during computation. By default, Ollama will detect this for optimal performance. It is recommended to set this value to the number of physical CPU cores your system has (as opposed to the logical number of cores). Refer to <a target=\\\"_blank\\\" href=\\\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\\\">docs</a> for more details\",\n            \"step\": 1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"ollama_0-input-numThread-number\"\n          },\n          {\n            \"label\": \"Repeat Last N\",\n            \"name\": \"repeatLastN\",\n            \"type\": \"number\",\n            \"description\": \"Sets how far back for the model to look back to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx). Refer to <a target=\\\"_blank\\\" href=\\\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\\\">docs</a> for more details\",\n            \"step\": 1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"ollama_0-input-repeatLastN-number\"\n          },\n          {\n            \"label\": \"Repeat Penalty\",\n            \"name\": \"repeatPenalty\",\n            \"type\": \"number\",\n            \"description\": \"Sets how strongly to penalize repetitions. A higher value (e.g., 1.5) will penalize repetitions more strongly, while a lower value (e.g., 0.9) will be more lenient. (Default: 1.1). Refer to <a target=\\\"_blank\\\" href=\\\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\\\">docs</a> for more details\",\n            \"step\": 0.1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"ollama_0-input-repeatPenalty-number\"\n          },\n          {\n            \"label\": \"Stop Sequence\",\n            \"name\": \"stop\",\n            \"type\": \"string\",\n            \"rows\": 4,\n            \"placeholder\": \"AI assistant:\",\n            \"description\": \"Sets the stop sequences to use. Use comma to seperate different sequences. Refer to <a target=\\\"_blank\\\" href=\\\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\\\">docs</a> for more details\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"ollama_0-input-stop-string\"\n          },\n          {\n            \"label\": \"Tail Free Sampling\",\n            \"name\": \"tfsZ\",\n            \"type\": \"number\",\n            \"description\": \"Tail free sampling is used to reduce the impact of less probable tokens from the output. A higher value (e.g., 2.0) will reduce the impact more, while a value of 1.0 disables this setting. (Default: 1). Refer to <a target=\\\"_blank\\\" href=\\\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\\\">docs</a> for more details\",\n            \"step\": 0.1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"ollama_0-input-tfsZ-number\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Cache\",\n            \"name\": \"cache\",\n            \"type\": \"BaseCache\",\n            \"optional\": true,\n            \"id\": \"ollama_0-input-cache-BaseCache\"\n          }\n        ],\n        \"inputs\": {\n          \"cache\": \"\",\n          \"baseUrl\": \"http://localhost:11434\",\n          \"modelName\": \"\",\n          \"temperature\": 0.9,\n          \"topP\": \"\",\n          \"topK\": \"\",\n          \"mirostat\": \"\",\n          \"mirostatEta\": \"\",\n          \"mirostatTau\": \"\",\n          \"numCtx\": \"\",\n          \"numGqa\": \"\",\n          \"numGpu\": \"\",\n          \"numThread\": \"\",\n          \"repeatLastN\": \"\",\n          \"repeatPenalty\": \"\",\n          \"stop\": \"\",\n          \"tfsZ\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"ollama_0-output-ollama-Ollama|LLM|BaseLLM|BaseLanguageModel|Runnable\",\n            \"name\": \"ollama\",\n            \"label\": \"Ollama\",\n            \"description\": \"Wrapper around open source large language models on Ollama\",\n            \"type\": \"Ollama | LLM | BaseLLM | BaseLanguageModel | Runnable\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 577,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 1368.8171609309877,\n        \"y\": 34.01899639575146\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"voyageAIEmbeddings_0\",\n      \"position\": {\n        \"x\": -474.03672636892577,\n        \"y\": -8.234365703505162\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"voyageAIEmbeddings_0\",\n        \"label\": \"VoyageAI Embeddings\",\n        \"version\": 2,\n        \"name\": \"voyageAIEmbeddings\",\n        \"type\": \"VoyageAIEmbeddings\",\n        \"baseClasses\": [\n          \"VoyageAIEmbeddings\",\n          \"Embeddings\"\n        ],\n        \"category\": \"Embeddings\",\n        \"description\": \"Voyage AI API to generate embeddings for a given text\",\n        \"inputParams\": [\n          {\n            \"label\": \"Connect Credential\",\n            \"name\": \"credential\",\n            \"type\": \"credential\",\n            \"credentialNames\": [\n              \"voyageAIApi\"\n            ],\n            \"id\": \"voyageAIEmbeddings_0-input-credential-credential\"\n          },\n          {\n            \"label\": \"Model Name\",\n            \"name\": \"modelName\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"default\": \"voyage-2\",\n            \"id\": \"voyageAIEmbeddings_0-input-modelName-asyncOptions\"\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"modelName\": \"voyage-2\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"voyageAIEmbeddings_0-output-voyageAIEmbeddings-VoyageAIEmbeddings|Embeddings\",\n            \"name\": \"voyageAIEmbeddings\",\n            \"label\": \"VoyageAIEmbeddings\",\n            \"description\": \"Voyage AI API to generate embeddings for a given text\",\n            \"type\": \"VoyageAIEmbeddings | Embeddings\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 370,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": -474.03672636892577,\n        \"y\": -8.234365703505162\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"recursiveCharacterTextSplitter_0\",\n      \"position\": {\n        \"x\": -852.7594120161409,\n        \"y\": -296.42262793276734\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"recursiveCharacterTextSplitter_0\",\n        \"label\": \"Recursive Character Text Splitter\",\n        \"version\": 2,\n        \"name\": \"recursiveCharacterTextSplitter\",\n        \"type\": \"RecursiveCharacterTextSplitter\",\n        \"baseClasses\": [\n          \"RecursiveCharacterTextSplitter\",\n          \"TextSplitter\",\n          \"BaseDocumentTransformer\",\n          \"Runnable\"\n        ],\n        \"category\": \"Text Splitters\",\n        \"description\": \"Split documents recursively by different characters - starting with \\\"\\\\n\\\\n\\\", then \\\"\\\\n\\\", then \\\" \\\"\",\n        \"inputParams\": [\n          {\n            \"label\": \"Chunk Size\",\n            \"name\": \"chunkSize\",\n            \"type\": \"number\",\n            \"description\": \"Number of characters in each chunk. Default is 1000.\",\n            \"default\": 1000,\n            \"optional\": true,\n            \"id\": \"recursiveCharacterTextSplitter_0-input-chunkSize-number\"\n          },\n          {\n            \"label\": \"Chunk Overlap\",\n            \"name\": \"chunkOverlap\",\n            \"type\": \"number\",\n            \"description\": \"Number of characters to overlap between chunks. Default is 200.\",\n            \"default\": 200,\n            \"optional\": true,\n            \"id\": \"recursiveCharacterTextSplitter_0-input-chunkOverlap-number\"\n          },\n          {\n            \"label\": \"Custom Separators\",\n            \"name\": \"separators\",\n            \"type\": \"string\",\n            \"rows\": 4,\n            \"description\": \"Array of custom separators to determine when to split the text, will override the default separators\",\n            \"placeholder\": \"[\\\"|\\\", \\\"##\\\", \\\">\\\", \\\"-\\\"]\",\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"id\": \"recursiveCharacterTextSplitter_0-input-separators-string\"\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"chunkSize\": \"200\",\n          \"chunkOverlap\": \"50\",\n          \"separators\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"recursiveCharacterTextSplitter_0-output-recursiveCharacterTextSplitter-RecursiveCharacterTextSplitter|TextSplitter|BaseDocumentTransformer|Runnable\",\n            \"name\": \"recursiveCharacterTextSplitter\",\n            \"label\": \"RecursiveCharacterTextSplitter\",\n            \"description\": \"Split documents recursively by different characters - starting with \\\"\\\\n\\\\n\\\", then \\\"\\\\n\\\", then \\\" \\\"\",\n            \"type\": \"RecursiveCharacterTextSplitter | TextSplitter | BaseDocumentTransformer | Runnable\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 427,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": -852.7594120161409,\n        \"y\": -296.42262793276734\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"toolAgent_0\",\n      \"position\": {\n        \"x\": 908.8994838628555,\n        \"y\": -345.4249792208967\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"toolAgent_0\",\n        \"label\": \"Tool Agent\",\n        \"version\": 2,\n        \"name\": \"toolAgent\",\n        \"type\": \"AgentExecutor\",\n        \"baseClasses\": [\n          \"AgentExecutor\",\n          \"BaseChain\",\n          \"Runnable\"\n        ],\n        \"category\": \"Agents\",\n        \"description\": \"Agent that uses Function Calling to pick the tools and args to call\",\n        \"inputParams\": [\n          {\n            \"label\": \"System Message\",\n            \"name\": \"systemMessage\",\n            \"type\": \"string\",\n            \"default\": \"You are a helpful AI assistant.\",\n            \"description\": \"If Chat Prompt Template is provided, this will be ignored\",\n            \"rows\": 4,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"toolAgent_0-input-systemMessage-string\"\n          },\n          {\n            \"label\": \"Max Iterations\",\n            \"name\": \"maxIterations\",\n            \"type\": \"number\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"toolAgent_0-input-maxIterations-number\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Tools\",\n            \"name\": \"tools\",\n            \"type\": \"Tool\",\n            \"list\": true,\n            \"id\": \"toolAgent_0-input-tools-Tool\"\n          },\n          {\n            \"label\": \"Memory\",\n            \"name\": \"memory\",\n            \"type\": \"BaseChatMemory\",\n            \"id\": \"toolAgent_0-input-memory-BaseChatMemory\"\n          },\n          {\n            \"label\": \"Tool Calling Chat Model\",\n            \"name\": \"model\",\n            \"type\": \"BaseChatModel\",\n            \"description\": \"Only compatible with models that are capable of function calling: ChatOpenAI, ChatMistral, ChatAnthropic, ChatGoogleGenerativeAI, ChatVertexAI, GroqChat\",\n            \"id\": \"toolAgent_0-input-model-BaseChatModel\"\n          },\n          {\n            \"label\": \"Chat Prompt Template\",\n            \"name\": \"chatPromptTemplate\",\n            \"type\": \"ChatPromptTemplate\",\n            \"description\": \"Override existing prompt with Chat Prompt Template. Human Message must includes {input} variable\",\n            \"optional\": true,\n            \"id\": \"toolAgent_0-input-chatPromptTemplate-ChatPromptTemplate\"\n          },\n          {\n            \"label\": \"Input Moderation\",\n            \"description\": \"Detect text that could generate harmful output and prevent it from being sent to the language model\",\n            \"name\": \"inputModeration\",\n            \"type\": \"Moderation\",\n            \"optional\": true,\n            \"list\": true,\n            \"id\": \"toolAgent_0-input-inputModeration-Moderation\"\n          }\n        ],\n        \"inputs\": {\n          \"tools\": [\n            \"{{retrieverTool_0.data.instance}}\"\n          ],\n          \"memory\": \"{{bufferMemory_0.data.instance}}\",\n          \"model\": \"{{groqChat_0.data.instance}}\",\n          \"chatPromptTemplate\": \"\",\n          \"systemMessage\": \"You are a helpful AI assistant.\",\n          \"inputModeration\": \"\",\n          \"maxIterations\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"toolAgent_0-output-toolAgent-AgentExecutor|BaseChain|Runnable\",\n            \"name\": \"toolAgent\",\n            \"label\": \"AgentExecutor\",\n            \"description\": \"Agent that uses Function Calling to pick the tools and args to call\",\n            \"type\": \"AgentExecutor | BaseChain | Runnable\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 483,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 908.8994838628555,\n        \"y\": -345.4249792208967\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"bufferMemory_0\",\n      \"position\": {\n        \"x\": 528.8654996263605,\n        \"y\": -424.39532863216294\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"bufferMemory_0\",\n        \"label\": \"Buffer Memory\",\n        \"version\": 2,\n        \"name\": \"bufferMemory\",\n        \"type\": \"BufferMemory\",\n        \"baseClasses\": [\n          \"BufferMemory\",\n          \"BaseChatMemory\",\n          \"BaseMemory\"\n        ],\n        \"category\": \"Memory\",\n        \"description\": \"Retrieve chat messages stored in database\",\n        \"inputParams\": [\n          {\n            \"label\": \"Session Id\",\n            \"name\": \"sessionId\",\n            \"type\": \"string\",\n            \"description\": \"If not specified, a random id will be used. Learn <a target=\\\"_blank\\\" href=\\\"https://docs.flowiseai.com/memory#ui-and-embedded-chat\\\">more</a>\",\n            \"default\": \"\",\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"id\": \"bufferMemory_0-input-sessionId-string\"\n          },\n          {\n            \"label\": \"Memory Key\",\n            \"name\": \"memoryKey\",\n            \"type\": \"string\",\n            \"default\": \"chat_history\",\n            \"additionalParams\": true,\n            \"id\": \"bufferMemory_0-input-memoryKey-string\"\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"sessionId\": \"\",\n          \"memoryKey\": \"chat_history\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"bufferMemory_0-output-bufferMemory-BufferMemory|BaseChatMemory|BaseMemory\",\n            \"name\": \"bufferMemory\",\n            \"label\": \"BufferMemory\",\n            \"description\": \"Retrieve chat messages stored in database\",\n            \"type\": \"BufferMemory | BaseChatMemory | BaseMemory\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 250,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 528.8654996263605,\n        \"y\": -424.39532863216294\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"docxFile_0\",\n      \"position\": {\n        \"x\": -498.16084802538484,\n        \"y\": -465.4129839843411\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"docxFile_0\",\n        \"label\": \"Docx File\",\n        \"version\": 1,\n        \"name\": \"docxFile\",\n        \"type\": \"Document\",\n        \"baseClasses\": [\n          \"Document\"\n        ],\n        \"category\": \"Document Loaders\",\n        \"description\": \"Load data from DOCX files\",\n        \"inputParams\": [\n          {\n            \"label\": \"Docx File\",\n            \"name\": \"docxFile\",\n            \"type\": \"file\",\n            \"fileType\": \".docx\",\n            \"id\": \"docxFile_0-input-docxFile-file\"\n          },\n          {\n            \"label\": \"Additional Metadata\",\n            \"name\": \"metadata\",\n            \"type\": \"json\",\n            \"description\": \"Additional metadata to be added to the extracted documents\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"docxFile_0-input-metadata-json\"\n          },\n          {\n            \"label\": \"Omit Metadata Keys\",\n            \"name\": \"omitMetadataKeys\",\n            \"type\": \"string\",\n            \"rows\": 4,\n            \"description\": \"Each document loader comes with a default set of metadata keys that are extracted from the document. You can use this field to omit some of the default metadata keys. The value should be a list of keys, seperated by comma. Use * to omit all metadata keys execept the ones you specify in the Additional Metadata field\",\n            \"placeholder\": \"key1, key2, key3.nestedKey1\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"docxFile_0-input-omitMetadataKeys-string\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Text Splitter\",\n            \"name\": \"textSplitter\",\n            \"type\": \"TextSplitter\",\n            \"optional\": true,\n            \"id\": \"docxFile_0-input-textSplitter-TextSplitter\"\n          }\n        ],\n        \"inputs\": {\n          \"textSplitter\": \"{{recursiveCharacterTextSplitter_0.data.instance}}\",\n          \"metadata\": \"\",\n          \"omitMetadataKeys\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"docxFile_0-output-docxFile-Document\",\n            \"name\": \"docxFile\",\n            \"label\": \"Document\",\n            \"description\": \"Load data from DOCX files\",\n            \"type\": \"Document\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 410,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": -498.16084802538484,\n        \"y\": -465.4129839843411\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"faiss_0\",\n      \"position\": {\n        \"x\": -143.4779216934812,\n        \"y\": -416.56918453185494\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"faiss_0\",\n        \"label\": \"Faiss\",\n        \"version\": 1,\n        \"name\": \"faiss\",\n        \"type\": \"Faiss\",\n        \"baseClasses\": [\n          \"Faiss\",\n          \"VectorStoreRetriever\",\n          \"BaseRetriever\"\n        ],\n        \"category\": \"Vector Stores\",\n        \"description\": \"Upsert embedded data and perform similarity search upon query using Faiss library from Meta\",\n        \"inputParams\": [\n          {\n            \"label\": \"Base Path to load\",\n            \"name\": \"basePath\",\n            \"description\": \"Path to load faiss.index file\",\n            \"placeholder\": \"C:\\\\Users\\\\User\\\\Desktop\",\n            \"type\": \"string\",\n            \"id\": \"faiss_0-input-basePath-string\"\n          },\n          {\n            \"label\": \"Top K\",\n            \"name\": \"topK\",\n            \"description\": \"Number of top results to fetch. Default to 4\",\n            \"placeholder\": \"4\",\n            \"type\": \"number\",\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"id\": \"faiss_0-input-topK-number\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Document\",\n            \"name\": \"document\",\n            \"type\": \"Document\",\n            \"list\": true,\n            \"optional\": true,\n            \"id\": \"faiss_0-input-document-Document\"\n          },\n          {\n            \"label\": \"Embeddings\",\n            \"name\": \"embeddings\",\n            \"type\": \"Embeddings\",\n            \"id\": \"faiss_0-input-embeddings-Embeddings\"\n          }\n        ],\n        \"inputs\": {\n          \"document\": [\n            \"{{docxFile_0.data.instance}}\"\n          ],\n          \"embeddings\": \"{{voyageAIEmbeddings_0.data.instance}}\",\n          \"basePath\": \"/opt/render/.flowise/1177\",\n          \"topK\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"name\": \"output\",\n            \"label\": \"Output\",\n            \"type\": \"options\",\n            \"description\": \"\",\n            \"options\": [\n              {\n                \"id\": \"faiss_0-output-retriever-Faiss|VectorStoreRetriever|BaseRetriever\",\n                \"name\": \"retriever\",\n                \"label\": \"Faiss Retriever\",\n                \"description\": \"\",\n                \"type\": \"Faiss | VectorStoreRetriever | BaseRetriever\"\n              },\n              {\n                \"id\": \"faiss_0-output-vectorStore-Faiss|SaveableVectorStore|VectorStore\",\n                \"name\": \"vectorStore\",\n                \"label\": \"Faiss Vector Store\",\n                \"description\": \"\",\n                \"type\": \"Faiss | SaveableVectorStore | VectorStore\"\n              }\n            ],\n            \"default\": \"retriever\"\n          }\n        ],\n        \"outputs\": {\n          \"output\": \"retriever\"\n        },\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 456,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": -143.4779216934812,\n        \"y\": -416.56918453185494\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"retrieverTool_0\",\n      \"position\": {\n        \"x\": 204.54833182164043,\n        \"y\": -399.9178877515032\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"retrieverTool_0\",\n        \"label\": \"Retriever Tool\",\n        \"version\": 2,\n        \"name\": \"retrieverTool\",\n        \"type\": \"RetrieverTool\",\n        \"baseClasses\": [\n          \"RetrieverTool\",\n          \"DynamicTool\",\n          \"Tool\",\n          \"StructuredTool\",\n          \"Runnable\"\n        ],\n        \"category\": \"Tools\",\n        \"description\": \"Use a retriever as allowed tool for agent\",\n        \"inputParams\": [\n          {\n            \"label\": \"Retriever Name\",\n            \"name\": \"name\",\n            \"type\": \"string\",\n            \"placeholder\": \"search_state_of_union\",\n            \"id\": \"retrieverTool_0-input-name-string\"\n          },\n          {\n            \"label\": \"Retriever Description\",\n            \"name\": \"description\",\n            \"type\": \"string\",\n            \"description\": \"When should agent uses to retrieve documents\",\n            \"rows\": 3,\n            \"placeholder\": \"Searches and returns documents regarding the state-of-the-union.\",\n            \"id\": \"retrieverTool_0-input-description-string\"\n          },\n          {\n            \"label\": \"Return Source Documents\",\n            \"name\": \"returnSourceDocuments\",\n            \"type\": \"boolean\",\n            \"optional\": true,\n            \"id\": \"retrieverTool_0-input-returnSourceDocuments-boolean\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Retriever\",\n            \"name\": \"retriever\",\n            \"type\": \"BaseRetriever\",\n            \"id\": \"retrieverTool_0-input-retriever-BaseRetriever\"\n          }\n        ],\n        \"inputs\": {\n          \"name\": \"\",\n          \"description\": \"\",\n          \"retriever\": \"{{faiss_0.data.instance}}\",\n          \"returnSourceDocuments\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"retrieverTool_0-output-retrieverTool-RetrieverTool|DynamicTool|Tool|StructuredTool|Runnable\",\n            \"name\": \"retrieverTool\",\n            \"label\": \"RetrieverTool\",\n            \"description\": \"Use a retriever as allowed tool for agent\",\n            \"type\": \"RetrieverTool | DynamicTool | Tool | StructuredTool | Runnable\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 601,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 204.54833182164043,\n        \"y\": -399.9178877515032\n      },\n      \"dragging\": false\n    }\n  ],\n  \"edges\": [\n    {\n      \"source\": \"groqChat_0\",\n      \"sourceHandle\": \"groqChat_0-output-groqChat-GroqChat|BaseChatModel|BaseLanguageModel|Runnable\",\n      \"target\": \"toolAgent_0\",\n      \"targetHandle\": \"toolAgent_0-input-model-BaseChatModel\",\n      \"type\": \"buttonedge\",\n      \"id\": \"groqChat_0-groqChat_0-output-groqChat-GroqChat|BaseChatModel|BaseLanguageModel|Runnable-toolAgent_0-toolAgent_0-input-model-BaseChatModel\"\n    },\n    {\n      \"source\": \"bufferMemory_0\",\n      \"sourceHandle\": \"bufferMemory_0-output-bufferMemory-BufferMemory|BaseChatMemory|BaseMemory\",\n      \"target\": \"toolAgent_0\",\n      \"targetHandle\": \"toolAgent_0-input-memory-BaseChatMemory\",\n      \"type\": \"buttonedge\",\n      \"id\": \"bufferMemory_0-bufferMemory_0-output-bufferMemory-BufferMemory|BaseChatMemory|BaseMemory-toolAgent_0-toolAgent_0-input-memory-BaseChatMemory\"\n    },\n    {\n      \"source\": \"recursiveCharacterTextSplitter_0\",\n      \"sourceHandle\": \"recursiveCharacterTextSplitter_0-output-recursiveCharacterTextSplitter-RecursiveCharacterTextSplitter|TextSplitter|BaseDocumentTransformer|Runnable\",\n      \"target\": \"docxFile_0\",\n      \"targetHandle\": \"docxFile_0-input-textSplitter-TextSplitter\",\n      \"type\": \"buttonedge\",\n      \"id\": \"recursiveCharacterTextSplitter_0-recursiveCharacterTextSplitter_0-output-recursiveCharacterTextSplitter-RecursiveCharacterTextSplitter|TextSplitter|BaseDocumentTransformer|Runnable-docxFile_0-docxFile_0-input-textSplitter-TextSplitter\"\n    },\n    {\n      \"source\": \"docxFile_0\",\n      \"sourceHandle\": \"docxFile_0-output-docxFile-Document\",\n      \"target\": \"faiss_0\",\n      \"targetHandle\": \"faiss_0-input-document-Document\",\n      \"type\": \"buttonedge\",\n      \"id\": \"docxFile_0-docxFile_0-output-docxFile-Document-faiss_0-faiss_0-input-document-Document\"\n    },\n    {\n      \"source\": \"voyageAIEmbeddings_0\",\n      \"sourceHandle\": \"voyageAIEmbeddings_0-output-voyageAIEmbeddings-VoyageAIEmbeddings|Embeddings\",\n      \"target\": \"faiss_0\",\n      \"targetHandle\": \"faiss_0-input-embeddings-Embeddings\",\n      \"type\": \"buttonedge\",\n      \"id\": \"voyageAIEmbeddings_0-voyageAIEmbeddings_0-output-voyageAIEmbeddings-VoyageAIEmbeddings|Embeddings-faiss_0-faiss_0-input-embeddings-Embeddings\"\n    },\n    {\n      \"source\": \"faiss_0\",\n      \"sourceHandle\": \"faiss_0-output-retriever-Faiss|VectorStoreRetriever|BaseRetriever\",\n      \"target\": \"retrieverTool_0\",\n      \"targetHandle\": \"retrieverTool_0-input-retriever-BaseRetriever\",\n      \"type\": \"buttonedge\",\n      \"id\": \"faiss_0-faiss_0-output-retriever-Faiss|VectorStoreRetriever|BaseRetriever-retrieverTool_0-retrieverTool_0-input-retriever-BaseRetriever\"\n    },\n    {\n      \"source\": \"retrieverTool_0\",\n      \"sourceHandle\": \"retrieverTool_0-output-retrieverTool-RetrieverTool|DynamicTool|Tool|StructuredTool|Runnable\",\n      \"target\": \"toolAgent_0\",\n      \"targetHandle\": \"toolAgent_0-input-tools-Tool\",\n      \"type\": \"buttonedge\",\n      \"id\": \"retrieverTool_0-retrieverTool_0-output-retrieverTool-RetrieverTool|DynamicTool|Tool|StructuredTool|Runnable-toolAgent_0-toolAgent_0-input-tools-Tool\"\n    }\n  ]\n}",
      "type": "CHATFLOW"
    },
    {
      "id": "6a31b9c4-f52f-4163-a2d2-f5143c46a6bf",
      "name": "chatbotapp (1)",
      "flowData": "{\n  \"nodes\": [\n    {\n      \"id\": \"bufferMemory_0\",\n      \"position\": {\n        \"x\": -6.314443413509821,\n        \"y\": 95.32422326416776\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"bufferMemory_0\",\n        \"label\": \"Buffer Memory\",\n        \"version\": 2,\n        \"name\": \"bufferMemory\",\n        \"type\": \"BufferMemory\",\n        \"baseClasses\": [\n          \"BufferMemory\",\n          \"BaseChatMemory\",\n          \"BaseMemory\"\n        ],\n        \"category\": \"Memory\",\n        \"description\": \"Retrieve chat messages stored in database\",\n        \"inputParams\": [\n          {\n            \"label\": \"Session Id\",\n            \"name\": \"sessionId\",\n            \"type\": \"string\",\n            \"description\": \"If not specified, a random id will be used. Learn <a target=\\\"_blank\\\" href=\\\"https://docs.flowiseai.com/memory#ui-and-embedded-chat\\\">more</a>\",\n            \"default\": \"\",\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"id\": \"bufferMemory_0-input-sessionId-string\"\n          },\n          {\n            \"label\": \"Memory Key\",\n            \"name\": \"memoryKey\",\n            \"type\": \"string\",\n            \"default\": \"chat_history\",\n            \"additionalParams\": true,\n            \"id\": \"bufferMemory_0-input-memoryKey-string\"\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"sessionId\": \"\",\n          \"memoryKey\": \"chat_history\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"bufferMemory_0-output-bufferMemory-BufferMemory|BaseChatMemory|BaseMemory\",\n            \"name\": \"bufferMemory\",\n            \"label\": \"BufferMemory\",\n            \"description\": \"Retrieve chat messages stored in database\",\n            \"type\": \"BufferMemory | BaseChatMemory | BaseMemory\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 252,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": -6.314443413509821,\n        \"y\": 95.32422326416776\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"conversationChain_0\",\n      \"position\": {\n        \"x\": 383.4651119523668,\n        \"y\": -43.653607960785735\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"conversationChain_0\",\n        \"label\": \"Conversation Chain\",\n        \"version\": 3,\n        \"name\": \"conversationChain\",\n        \"type\": \"ConversationChain\",\n        \"baseClasses\": [\n          \"ConversationChain\",\n          \"LLMChain\",\n          \"BaseChain\",\n          \"Runnable\"\n        ],\n        \"category\": \"Chains\",\n        \"description\": \"Chat models specific conversational chain with memory\",\n        \"inputParams\": [\n          {\n            \"label\": \"System Message\",\n            \"name\": \"systemMessagePrompt\",\n            \"type\": \"string\",\n            \"rows\": 4,\n            \"description\": \"If Chat Prompt Template is provided, this will be ignored\",\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"default\": \"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\",\n            \"placeholder\": \"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\",\n            \"id\": \"conversationChain_0-input-systemMessagePrompt-string\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Chat Model\",\n            \"name\": \"model\",\n            \"type\": \"BaseChatModel\",\n            \"id\": \"conversationChain_0-input-model-BaseChatModel\"\n          },\n          {\n            \"label\": \"Memory\",\n            \"name\": \"memory\",\n            \"type\": \"BaseMemory\",\n            \"id\": \"conversationChain_0-input-memory-BaseMemory\"\n          },\n          {\n            \"label\": \"Chat Prompt Template\",\n            \"name\": \"chatPromptTemplate\",\n            \"type\": \"ChatPromptTemplate\",\n            \"description\": \"Override existing prompt with Chat Prompt Template. Human Message must includes {input} variable\",\n            \"optional\": true,\n            \"id\": \"conversationChain_0-input-chatPromptTemplate-ChatPromptTemplate\"\n          },\n          {\n            \"label\": \"Input Moderation\",\n            \"description\": \"Detect text that could generate harmful output and prevent it from being sent to the language model\",\n            \"name\": \"inputModeration\",\n            \"type\": \"Moderation\",\n            \"optional\": true,\n            \"list\": true,\n            \"id\": \"conversationChain_0-input-inputModeration-Moderation\"\n          }\n        ],\n        \"inputs\": {\n          \"model\": \"\",\n          \"memory\": \"{{bufferMemory_0.data.instance}}\",\n          \"chatPromptTemplate\": \"\",\n          \"inputModeration\": \"\",\n          \"systemMessagePrompt\": \"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"conversationChain_0-output-conversationChain-ConversationChain|LLMChain|BaseChain|Runnable\",\n            \"name\": \"conversationChain\",\n            \"label\": \"ConversationChain\",\n            \"description\": \"Chat models specific conversational chain with memory\",\n            \"type\": \"ConversationChain | LLMChain | BaseChain | Runnable\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 434,\n      \"positionAbsolute\": {\n        \"x\": 383.4651119523668,\n        \"y\": -43.653607960785735\n      },\n      \"selected\": false,\n      \"dragging\": false\n    },\n    {\n      \"id\": \"chatOpenAI_0\",\n      \"position\": {\n        \"x\": 1343.994611621198,\n        \"y\": 335.312418054151\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"chatOpenAI_0\",\n        \"label\": \"ChatOpenAI\",\n        \"version\": 7,\n        \"name\": \"chatOpenAI\",\n        \"type\": \"ChatOpenAI\",\n        \"baseClasses\": [\n          \"ChatOpenAI\",\n          \"BaseChatModel\",\n          \"BaseLanguageModel\",\n          \"Runnable\"\n        ],\n        \"category\": \"Chat Models\",\n        \"description\": \"Wrapper around OpenAI large language models that use the Chat endpoint\",\n        \"inputParams\": [\n          {\n            \"label\": \"Connect Credential\",\n            \"name\": \"credential\",\n            \"type\": \"credential\",\n            \"credentialNames\": [\n              \"openAIApi\"\n            ],\n            \"id\": \"chatOpenAI_0-input-credential-credential\"\n          },\n          {\n            \"label\": \"Model Name\",\n            \"name\": \"modelName\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"default\": \"gpt-3.5-turbo\",\n            \"id\": \"chatOpenAI_0-input-modelName-asyncOptions\"\n          },\n          {\n            \"label\": \"Temperature\",\n            \"name\": \"temperature\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"default\": 0.9,\n            \"optional\": true,\n            \"id\": \"chatOpenAI_0-input-temperature-number\"\n          },\n          {\n            \"label\": \"Max Tokens\",\n            \"name\": \"maxTokens\",\n            \"type\": \"number\",\n            \"step\": 1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_0-input-maxTokens-number\"\n          },\n          {\n            \"label\": \"Top Probability\",\n            \"name\": \"topP\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_0-input-topP-number\"\n          },\n          {\n            \"label\": \"Frequency Penalty\",\n            \"name\": \"frequencyPenalty\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_0-input-frequencyPenalty-number\"\n          },\n          {\n            \"label\": \"Presence Penalty\",\n            \"name\": \"presencePenalty\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_0-input-presencePenalty-number\"\n          },\n          {\n            \"label\": \"Timeout\",\n            \"name\": \"timeout\",\n            \"type\": \"number\",\n            \"step\": 1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_0-input-timeout-number\"\n          },\n          {\n            \"label\": \"BasePath\",\n            \"name\": \"basepath\",\n            \"type\": \"string\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_0-input-basepath-string\"\n          },\n          {\n            \"label\": \"Proxy Url\",\n            \"name\": \"proxyUrl\",\n            \"type\": \"string\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_0-input-proxyUrl-string\"\n          },\n          {\n            \"label\": \"Stop Sequence\",\n            \"name\": \"stopSequence\",\n            \"type\": \"string\",\n            \"rows\": 4,\n            \"optional\": true,\n            \"description\": \"List of stop words to use when generating. Use comma to separate multiple stop words.\",\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_0-input-stopSequence-string\"\n          },\n          {\n            \"label\": \"BaseOptions\",\n            \"name\": \"baseOptions\",\n            \"type\": \"json\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_0-input-baseOptions-json\"\n          },\n          {\n            \"label\": \"Allow Image Uploads\",\n            \"name\": \"allowImageUploads\",\n            \"type\": \"boolean\",\n            \"description\": \"Automatically uses gpt-4-vision-preview when image is being uploaded from chat. Only works with LLMChain, Conversation Chain, ReAct Agent, Conversational Agent, Tool Agent\",\n            \"default\": false,\n            \"optional\": true,\n            \"id\": \"chatOpenAI_0-input-allowImageUploads-boolean\"\n          },\n          {\n            \"label\": \"Image Resolution\",\n            \"description\": \"This parameter controls the resolution in which the model views the image.\",\n            \"name\": \"imageResolution\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"Low\",\n                \"name\": \"low\"\n              },\n              {\n                \"label\": \"High\",\n                \"name\": \"high\"\n              },\n              {\n                \"label\": \"Auto\",\n                \"name\": \"auto\"\n              }\n            ],\n            \"default\": \"low\",\n            \"optional\": false,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_0-input-imageResolution-options\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Cache\",\n            \"name\": \"cache\",\n            \"type\": \"BaseCache\",\n            \"optional\": true,\n            \"id\": \"chatOpenAI_0-input-cache-BaseCache\"\n          }\n        ],\n        \"inputs\": {\n          \"cache\": \"\",\n          \"modelName\": \"gpt-3.5-turbo\",\n          \"temperature\": \"0.7\",\n          \"maxTokens\": \"\",\n          \"topP\": \"\",\n          \"frequencyPenalty\": \"\",\n          \"presencePenalty\": \"\",\n          \"timeout\": \"\",\n          \"basepath\": \"\",\n          \"proxyUrl\": \"\",\n          \"stopSequence\": \"\",\n          \"baseOptions\": \"\",\n          \"allowImageUploads\": \"\",\n          \"imageResolution\": \"low\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"chatOpenAI_0-output-chatOpenAI-ChatOpenAI|BaseChatModel|BaseLanguageModel|Runnable\",\n            \"name\": \"chatOpenAI\",\n            \"label\": \"ChatOpenAI\",\n            \"description\": \"Wrapper around OpenAI large language models that use the Chat endpoint\",\n            \"type\": \"ChatOpenAI | BaseChatModel | BaseLanguageModel | Runnable\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 669,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 1343.994611621198,\n        \"y\": 335.312418054151\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"chatGoogleGenerativeAI_0\",\n      \"position\": {\n        \"x\": 826.5171836812219,\n        \"y\": -583.0533636469298\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"chatGoogleGenerativeAI_0\",\n        \"label\": \"ChatGoogleGenerativeAI\",\n        \"version\": 3,\n        \"name\": \"chatGoogleGenerativeAI\",\n        \"type\": \"ChatGoogleGenerativeAI\",\n        \"baseClasses\": [\n          \"ChatGoogleGenerativeAI\",\n          \"LangchainChatGoogleGenerativeAI\",\n          \"BaseChatModel\",\n          \"BaseLanguageModel\",\n          \"Runnable\"\n        ],\n        \"category\": \"Chat Models\",\n        \"description\": \"Wrapper around Google Gemini large language models that use the Chat endpoint\",\n        \"inputParams\": [\n          {\n            \"label\": \"Connect Credential\",\n            \"name\": \"credential\",\n            \"type\": \"credential\",\n            \"credentialNames\": [\n              \"googleGenerativeAI\"\n            ],\n            \"optional\": false,\n            \"description\": \"Google Generative AI credential.\",\n            \"id\": \"chatGoogleGenerativeAI_0-input-credential-credential\"\n          },\n          {\n            \"label\": \"Model Name\",\n            \"name\": \"modelName\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"default\": \"gemini-1.5-flash-latest\",\n            \"id\": \"chatGoogleGenerativeAI_0-input-modelName-asyncOptions\"\n          },\n          {\n            \"label\": \"Custom Model Name\",\n            \"name\": \"customModelName\",\n            \"type\": \"string\",\n            \"placeholder\": \"gemini-1.5-pro-exp-0801\",\n            \"description\": \"Custom model name to use. If provided, it will override the model selected\",\n            \"additionalParams\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-customModelName-string\"\n          },\n          {\n            \"label\": \"Temperature\",\n            \"name\": \"temperature\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"default\": 0.9,\n            \"optional\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-temperature-number\"\n          },\n          {\n            \"label\": \"Streaming\",\n            \"name\": \"streaming\",\n            \"type\": \"boolean\",\n            \"default\": true,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-streaming-boolean\"\n          },\n          {\n            \"label\": \"Max Output Tokens\",\n            \"name\": \"maxOutputTokens\",\n            \"type\": \"number\",\n            \"step\": 1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-maxOutputTokens-number\"\n          },\n          {\n            \"label\": \"Top Probability\",\n            \"name\": \"topP\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-topP-number\"\n          },\n          {\n            \"label\": \"Top Next Highest Probability Tokens\",\n            \"name\": \"topK\",\n            \"type\": \"number\",\n            \"description\": \"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive\",\n            \"step\": 1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-topK-number\"\n          },\n          {\n            \"label\": \"Harm Category\",\n            \"name\": \"harmCategory\",\n            \"type\": \"multiOptions\",\n            \"description\": \"Refer to <a target=\\\"_blank\\\" href=\\\"https://cloud.google.com/vertex-ai/docs/generative-ai/multimodal/configure-safety-attributes#safety_attribute_definitions\\\">official guide</a> on how to use Harm Category\",\n            \"options\": [\n              {\n                \"label\": \"Dangerous\",\n                \"name\": \"HARM_CATEGORY_DANGEROUS_CONTENT\"\n              },\n              {\n                \"label\": \"Harassment\",\n                \"name\": \"HARM_CATEGORY_HARASSMENT\"\n              },\n              {\n                \"label\": \"Hate Speech\",\n                \"name\": \"HARM_CATEGORY_HATE_SPEECH\"\n              },\n              {\n                \"label\": \"Sexually Explicit\",\n                \"name\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\"\n              }\n            ],\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-harmCategory-multiOptions\"\n          },\n          {\n            \"label\": \"Harm Block Threshold\",\n            \"name\": \"harmBlockThreshold\",\n            \"type\": \"multiOptions\",\n            \"description\": \"Refer to <a target=\\\"_blank\\\" href=\\\"https://cloud.google.com/vertex-ai/docs/generative-ai/multimodal/configure-safety-attributes#safety_setting_thresholds\\\">official guide</a> on how to use Harm Block Threshold\",\n            \"options\": [\n              {\n                \"label\": \"Low and Above\",\n                \"name\": \"BLOCK_LOW_AND_ABOVE\"\n              },\n              {\n                \"label\": \"Medium and Above\",\n                \"name\": \"BLOCK_MEDIUM_AND_ABOVE\"\n              },\n              {\n                \"label\": \"None\",\n                \"name\": \"BLOCK_NONE\"\n              },\n              {\n                \"label\": \"Only High\",\n                \"name\": \"BLOCK_ONLY_HIGH\"\n              },\n              {\n                \"label\": \"Threshold Unspecified\",\n                \"name\": \"HARM_BLOCK_THRESHOLD_UNSPECIFIED\"\n              }\n            ],\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-harmBlockThreshold-multiOptions\"\n          },\n          {\n            \"label\": \"Allow Image Uploads\",\n            \"name\": \"allowImageUploads\",\n            \"type\": \"boolean\",\n            \"description\": \"Allow image input. Refer to the <a href=\\\"https://docs.flowiseai.com/using-flowise/uploads#image\\\" target=\\\"_blank\\\">docs</a> for more details.\",\n            \"default\": false,\n            \"optional\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-allowImageUploads-boolean\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Cache\",\n            \"name\": \"cache\",\n            \"type\": \"BaseCache\",\n            \"optional\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-cache-BaseCache\"\n          }\n        ],\n        \"inputs\": {\n          \"cache\": \"\",\n          \"modelName\": \"gemini-2.0-flash-001\",\n          \"customModelName\": \"\",\n          \"temperature\": 0.9,\n          \"streaming\": true,\n          \"maxOutputTokens\": \"\",\n          \"topP\": \"\",\n          \"topK\": \"\",\n          \"harmCategory\": \"\",\n          \"harmBlockThreshold\": \"\",\n          \"allowImageUploads\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"chatGoogleGenerativeAI_0-output-chatGoogleGenerativeAI-ChatGoogleGenerativeAI|LangchainChatGoogleGenerativeAI|BaseChatModel|BaseLanguageModel|Runnable\",\n            \"name\": \"chatGoogleGenerativeAI\",\n            \"label\": \"ChatGoogleGenerativeAI\",\n            \"description\": \"Wrapper around Google Gemini large language models that use the Chat endpoint\",\n            \"type\": \"ChatGoogleGenerativeAI | LangchainChatGoogleGenerativeAI | BaseChatModel | BaseLanguageModel | Runnable\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 669,\n      \"positionAbsolute\": {\n        \"x\": 826.5171836812219,\n        \"y\": -583.0533636469298\n      },\n      \"selected\": false,\n      \"dragging\": false\n    },\n    {\n      \"id\": \"chatDeepseek_0\",\n      \"position\": {\n        \"x\": 957.0098017055498,\n        \"y\": 302.2464071508231\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"chatDeepseek_0\",\n        \"label\": \"ChatDeepseek\",\n        \"version\": 1,\n        \"name\": \"chatDeepseek\",\n        \"type\": \"chatDeepseek\",\n        \"baseClasses\": [\n          \"chatDeepseek\",\n          \"BaseChatModel\",\n          \"BaseLanguageModel\",\n          \"Runnable\"\n        ],\n        \"category\": \"Chat Models\",\n        \"description\": \"Wrapper around Deepseek large language models that use the Chat endpoint\",\n        \"inputParams\": [\n          {\n            \"label\": \"Connect Credential\",\n            \"name\": \"credential\",\n            \"type\": \"credential\",\n            \"credentialNames\": [\n              \"deepseekApi\"\n            ],\n            \"id\": \"chatDeepseek_0-input-credential-credential\"\n          },\n          {\n            \"label\": \"Model Name\",\n            \"name\": \"modelName\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"default\": \"deepseek-chat\",\n            \"id\": \"chatDeepseek_0-input-modelName-asyncOptions\"\n          },\n          {\n            \"label\": \"Temperature\",\n            \"name\": \"temperature\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"default\": 0.7,\n            \"optional\": true,\n            \"id\": \"chatDeepseek_0-input-temperature-number\"\n          },\n          {\n            \"label\": \"Streaming\",\n            \"name\": \"streaming\",\n            \"type\": \"boolean\",\n            \"default\": true,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatDeepseek_0-input-streaming-boolean\"\n          },\n          {\n            \"label\": \"Max Tokens\",\n            \"name\": \"maxTokens\",\n            \"type\": \"number\",\n            \"step\": 1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatDeepseek_0-input-maxTokens-number\"\n          },\n          {\n            \"label\": \"Top Probability\",\n            \"name\": \"topP\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatDeepseek_0-input-topP-number\"\n          },\n          {\n            \"label\": \"Frequency Penalty\",\n            \"name\": \"frequencyPenalty\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatDeepseek_0-input-frequencyPenalty-number\"\n          },\n          {\n            \"label\": \"Presence Penalty\",\n            \"name\": \"presencePenalty\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatDeepseek_0-input-presencePenalty-number\"\n          },\n          {\n            \"label\": \"Timeout\",\n            \"name\": \"timeout\",\n            \"type\": \"number\",\n            \"step\": 1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatDeepseek_0-input-timeout-number\"\n          },\n          {\n            \"label\": \"Stop Sequence\",\n            \"name\": \"stopSequence\",\n            \"type\": \"string\",\n            \"rows\": 4,\n            \"optional\": true,\n            \"description\": \"List of stop words to use when generating. Use comma to separate multiple stop words.\",\n            \"additionalParams\": true,\n            \"id\": \"chatDeepseek_0-input-stopSequence-string\"\n          },\n          {\n            \"label\": \"Base Options\",\n            \"name\": \"baseOptions\",\n            \"type\": \"json\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"description\": \"Additional options to pass to the Deepseek client. This should be a JSON object.\",\n            \"id\": \"chatDeepseek_0-input-baseOptions-json\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Cache\",\n            \"name\": \"cache\",\n            \"type\": \"BaseCache\",\n            \"optional\": true,\n            \"id\": \"chatDeepseek_0-input-cache-BaseCache\"\n          }\n        ],\n        \"inputs\": {\n          \"cache\": \"\",\n          \"modelName\": \"deepseek-chat\",\n          \"temperature\": 0.7,\n          \"streaming\": true,\n          \"maxTokens\": \"\",\n          \"topP\": \"\",\n          \"frequencyPenalty\": \"\",\n          \"presencePenalty\": \"\",\n          \"timeout\": \"\",\n          \"stopSequence\": \"\",\n          \"baseOptions\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"chatDeepseek_0-output-chatDeepseek-chatDeepseek|BaseChatModel|BaseLanguageModel|Runnable\",\n            \"name\": \"chatDeepseek\",\n            \"label\": \"chatDeepseek\",\n            \"description\": \"Wrapper around Deepseek large language models that use the Chat endpoint\",\n            \"type\": \"chatDeepseek | BaseChatModel | BaseLanguageModel | Runnable\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 573,\n      \"selected\": false,\n      \"dragging\": false,\n      \"positionAbsolute\": {\n        \"x\": 957.0098017055498,\n        \"y\": 302.2464071508231\n      }\n    }\n  ],\n  \"edges\": [\n    {\n      \"source\": \"bufferMemory_0\",\n      \"sourceHandle\": \"bufferMemory_0-output-bufferMemory-BufferMemory|BaseChatMemory|BaseMemory\",\n      \"target\": \"conversationChain_0\",\n      \"targetHandle\": \"conversationChain_0-input-memory-BaseMemory\",\n      \"type\": \"buttonedge\",\n      \"id\": \"bufferMemory_0-bufferMemory_0-output-bufferMemory-BufferMemory|BaseChatMemory|BaseMemory-conversationChain_0-conversationChain_0-input-memory-BaseMemory\"\n    }\n  ]\n}",
      "type": "CHATFLOW"
    },
    {
      "id": "ea0366ea-7576-407c-b6f4-fc4b2cd9810e",
      "name": "pdfretriverusingmistralai (1)",
      "flowData": "{\n  \"nodes\": [\n    {\n      \"width\": 300,\n      \"height\": 421,\n      \"id\": \"openAIEmbeddings_0\",\n      \"position\": {\n        \"x\": 598.4691381182121,\n        \"y\": 614.7222554103136\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"openAIEmbeddings_0\",\n        \"label\": \"OpenAI Embeddings\",\n        \"version\": 4,\n        \"name\": \"openAIEmbeddings\",\n        \"type\": \"OpenAIEmbeddings\",\n        \"baseClasses\": [\n          \"OpenAIEmbeddings\",\n          \"Embeddings\"\n        ],\n        \"category\": \"Embeddings\",\n        \"description\": \"OpenAI API to generate embeddings for a given text\",\n        \"inputParams\": [\n          {\n            \"label\": \"Connect Credential\",\n            \"name\": \"credential\",\n            \"type\": \"credential\",\n            \"credentialNames\": [\n              \"openAIApi\"\n            ],\n            \"id\": \"openAIEmbeddings_0-input-credential-credential\"\n          },\n          {\n            \"label\": \"Model Name\",\n            \"name\": \"modelName\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"default\": \"text-embedding-ada-002\",\n            \"id\": \"openAIEmbeddings_0-input-modelName-asyncOptions\"\n          },\n          {\n            \"label\": \"Strip New Lines\",\n            \"name\": \"stripNewLines\",\n            \"type\": \"boolean\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"openAIEmbeddings_0-input-stripNewLines-boolean\"\n          },\n          {\n            \"label\": \"Batch Size\",\n            \"name\": \"batchSize\",\n            \"type\": \"number\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"openAIEmbeddings_0-input-batchSize-number\"\n          },\n          {\n            \"label\": \"Timeout\",\n            \"name\": \"timeout\",\n            \"type\": \"number\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"openAIEmbeddings_0-input-timeout-number\"\n          },\n          {\n            \"label\": \"BasePath\",\n            \"name\": \"basepath\",\n            \"type\": \"string\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"openAIEmbeddings_0-input-basepath-string\"\n          },\n          {\n            \"label\": \"Dimensions\",\n            \"name\": \"dimensions\",\n            \"type\": \"number\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"openAIEmbeddings_0-input-dimensions-number\"\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"modelName\": \"text-embedding-ada-002\",\n          \"stripNewLines\": \"\",\n          \"batchSize\": \"\",\n          \"timeout\": \"\",\n          \"basepath\": \"\",\n          \"dimensions\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"openAIEmbeddings_0-output-openAIEmbeddings-OpenAIEmbeddings|Embeddings\",\n            \"name\": \"openAIEmbeddings\",\n            \"label\": \"OpenAIEmbeddings\",\n            \"description\": \"OpenAI API to generate embeddings for a given text\",\n            \"type\": \"OpenAIEmbeddings | Embeddings\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 598.4691381182121,\n        \"y\": 614.7222554103136\n      },\n      \"dragging\": false\n    },\n    {\n      \"width\": 300,\n      \"height\": 427,\n      \"id\": \"recursiveCharacterTextSplitter_0\",\n      \"position\": {\n        \"x\": 124.11835209314779,\n        \"y\": 151.81643824918933\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"recursiveCharacterTextSplitter_0\",\n        \"label\": \"Recursive Character Text Splitter\",\n        \"version\": 2,\n        \"name\": \"recursiveCharacterTextSplitter\",\n        \"type\": \"RecursiveCharacterTextSplitter\",\n        \"baseClasses\": [\n          \"RecursiveCharacterTextSplitter\",\n          \"TextSplitter\"\n        ],\n        \"category\": \"Text Splitters\",\n        \"description\": \"Split documents recursively by different characters - starting with \\\"\\\\n\\\\n\\\", then \\\"\\\\n\\\", then \\\" \\\"\",\n        \"inputParams\": [\n          {\n            \"label\": \"Chunk Size\",\n            \"name\": \"chunkSize\",\n            \"type\": \"number\",\n            \"default\": 1000,\n            \"optional\": true,\n            \"id\": \"recursiveCharacterTextSplitter_0-input-chunkSize-number\"\n          },\n          {\n            \"label\": \"Chunk Overlap\",\n            \"name\": \"chunkOverlap\",\n            \"type\": \"number\",\n            \"optional\": true,\n            \"id\": \"recursiveCharacterTextSplitter_0-input-chunkOverlap-number\"\n          },\n          {\n            \"label\": \"Custom Separators\",\n            \"name\": \"separators\",\n            \"type\": \"string\",\n            \"rows\": 4,\n            \"description\": \"Array of custom separators to determine when to split the text, will override the default separators\",\n            \"placeholder\": \"[\\\"|\\\", \\\"##\\\", \\\">\\\", \\\"-\\\"]\",\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"id\": \"recursiveCharacterTextSplitter_0-input-separators-string\"\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"chunkSize\": \"500\",\n          \"chunkOverlap\": \"150\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"recursiveCharacterTextSplitter_0-output-recursiveCharacterTextSplitter-RecursiveCharacterTextSplitter|TextSplitter\",\n            \"name\": \"recursiveCharacterTextSplitter\",\n            \"label\": \"RecursiveCharacterTextSplitter\",\n            \"type\": \"RecursiveCharacterTextSplitter | TextSplitter\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 124.11835209314779,\n        \"y\": 151.81643824918933\n      },\n      \"dragging\": false\n    },\n    {\n      \"width\": 300,\n      \"height\": 529,\n      \"id\": \"conversationalRetrievalQAChain_0\",\n      \"position\": {\n        \"x\": 1558.6564094656787,\n        \"y\": 386.60217819991124\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"conversationalRetrievalQAChain_0\",\n        \"label\": \"Conversational Retrieval QA Chain\",\n        \"version\": 3,\n        \"name\": \"conversationalRetrievalQAChain\",\n        \"type\": \"ConversationalRetrievalQAChain\",\n        \"baseClasses\": [\n          \"ConversationalRetrievalQAChain\",\n          \"BaseChain\",\n          \"Runnable\"\n        ],\n        \"category\": \"Chains\",\n        \"description\": \"Document QA - built on RetrievalQAChain to provide a chat history component\",\n        \"inputParams\": [\n          {\n            \"label\": \"Return Source Documents\",\n            \"name\": \"returnSourceDocuments\",\n            \"type\": \"boolean\",\n            \"optional\": true,\n            \"id\": \"conversationalRetrievalQAChain_0-input-returnSourceDocuments-boolean\"\n          },\n          {\n            \"label\": \"Rephrase Prompt\",\n            \"name\": \"rephrasePrompt\",\n            \"type\": \"string\",\n            \"description\": \"Using previous chat history, rephrase question into a standalone question\",\n            \"warning\": \"Prompt must include input variables: {chat_history} and {question}\",\n            \"rows\": 4,\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"default\": \"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone Question:\",\n            \"id\": \"conversationalRetrievalQAChain_0-input-rephrasePrompt-string\"\n          },\n          {\n            \"label\": \"Response Prompt\",\n            \"name\": \"responsePrompt\",\n            \"type\": \"string\",\n            \"description\": \"Taking the rephrased question, search for answer from the provided context\",\n            \"warning\": \"Prompt must include input variable: {context}\",\n            \"rows\": 4,\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"default\": \"You are a helpful assistant. Using the provided context, answer the user's question to the best of your ability using the resources provided.\\nIf there is nothing in the context relevant to the question at hand, just say \\\"Hmm, I'm not sure.\\\" Don't try to make up an answer.\\n------------\\n{context}\\n------------\\nREMEMBER: If there is no relevant information within the context, just say \\\"Hmm, I'm not sure.\\\" Don't try to make up an answer.\",\n            \"id\": \"conversationalRetrievalQAChain_0-input-responsePrompt-string\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Chat Model\",\n            \"name\": \"model\",\n            \"type\": \"BaseChatModel\",\n            \"id\": \"conversationalRetrievalQAChain_0-input-model-BaseChatModel\"\n          },\n          {\n            \"label\": \"Vector Store Retriever\",\n            \"name\": \"vectorStoreRetriever\",\n            \"type\": \"BaseRetriever\",\n            \"id\": \"conversationalRetrievalQAChain_0-input-vectorStoreRetriever-BaseRetriever\"\n          },\n          {\n            \"label\": \"Memory\",\n            \"name\": \"memory\",\n            \"type\": \"BaseMemory\",\n            \"optional\": true,\n            \"description\": \"If left empty, a default BufferMemory will be used\",\n            \"id\": \"conversationalRetrievalQAChain_0-input-memory-BaseMemory\"\n          },\n          {\n            \"label\": \"Input Moderation\",\n            \"description\": \"Detect text that could generate harmful output and prevent it from being sent to the language model\",\n            \"name\": \"inputModeration\",\n            \"type\": \"Moderation\",\n            \"optional\": true,\n            \"list\": true,\n            \"id\": \"conversationalRetrievalQAChain_0-input-inputModeration-Moderation\"\n          }\n        ],\n        \"inputs\": {\n          \"inputModeration\": \"\",\n          \"model\": \"{{chatMistralAI_0.data.instance}}\",\n          \"vectorStoreRetriever\": \"{{faiss_0.data.instance}}\",\n          \"memory\": \"{{bufferMemory_0.data.instance}}\",\n          \"rephrasePrompt\": \"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone Question:\",\n          \"responsePrompt\": \"You are a helpful assistant. Using the provided context, answer the user's question to the best of your ability using the resources provided.\\nIf there is nothing in the context relevant to the question at hand, just say \\\"Hmm, I'm not sure.\\\" Don't try to make up an answer.\\n------------\\n{context}\\n------------\\nREMEMBER: If there is no relevant information within the context, just say \\\"Hmm, I'm not sure.\\\" Don't try to make up an answer.\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"conversationalRetrievalQAChain_0-output-conversationalRetrievalQAChain-ConversationalRetrievalQAChain|BaseChain|Runnable\",\n            \"name\": \"conversationalRetrievalQAChain\",\n            \"label\": \"ConversationalRetrievalQAChain\",\n            \"type\": \"ConversationalRetrievalQAChain | BaseChain | Runnable\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"positionAbsolute\": {\n        \"x\": 1558.6564094656787,\n        \"y\": 386.60217819991124\n      },\n      \"selected\": false,\n      \"dragging\": false\n    },\n    {\n      \"id\": \"faiss_0\",\n      \"position\": {\n        \"x\": 1026.272061488701,\n        \"y\": 540.7157860306309\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"faiss_0\",\n        \"label\": \"Faiss\",\n        \"version\": 1,\n        \"name\": \"faiss\",\n        \"type\": \"Faiss\",\n        \"baseClasses\": [\n          \"Faiss\",\n          \"VectorStoreRetriever\",\n          \"BaseRetriever\"\n        ],\n        \"category\": \"Vector Stores\",\n        \"description\": \"Upsert embedded data and perform similarity search upon query using Faiss library from Meta\",\n        \"inputParams\": [\n          {\n            \"label\": \"Base Path to load\",\n            \"name\": \"basePath\",\n            \"description\": \"Path to load faiss.index file\",\n            \"placeholder\": \"C:\\\\Users\\\\User\\\\Desktop\",\n            \"type\": \"string\",\n            \"id\": \"faiss_0-input-basePath-string\"\n          },\n          {\n            \"label\": \"Top K\",\n            \"name\": \"topK\",\n            \"description\": \"Number of top results to fetch. Default to 4\",\n            \"placeholder\": \"4\",\n            \"type\": \"number\",\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"id\": \"faiss_0-input-topK-number\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Document\",\n            \"name\": \"document\",\n            \"type\": \"Document\",\n            \"list\": true,\n            \"optional\": true,\n            \"id\": \"faiss_0-input-document-Document\"\n          },\n          {\n            \"label\": \"Embeddings\",\n            \"name\": \"embeddings\",\n            \"type\": \"Embeddings\",\n            \"id\": \"faiss_0-input-embeddings-Embeddings\"\n          }\n        ],\n        \"inputs\": {\n          \"document\": [\n            \"{{documentStore_0.data.instance}}\",\n            \"{{pdfFile_0.data.instance}}\"\n          ],\n          \"embeddings\": \"{{openAIEmbeddings_0.data.instance}}\",\n          \"basePath\": \"/opt/render/.flowise/1177\",\n          \"topK\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"name\": \"output\",\n            \"label\": \"Output\",\n            \"type\": \"options\",\n            \"description\": \"\",\n            \"options\": [\n              {\n                \"id\": \"faiss_0-output-retriever-Faiss|VectorStoreRetriever|BaseRetriever\",\n                \"name\": \"retriever\",\n                \"label\": \"Faiss Retriever\",\n                \"description\": \"\",\n                \"type\": \"Faiss | VectorStoreRetriever | BaseRetriever\"\n              },\n              {\n                \"id\": \"faiss_0-output-vectorStore-Faiss|SaveableVectorStore|VectorStore\",\n                \"name\": \"vectorStore\",\n                \"label\": \"Faiss Vector Store\",\n                \"description\": \"\",\n                \"type\": \"Faiss | SaveableVectorStore | VectorStore\"\n              }\n            ],\n            \"default\": \"retriever\"\n          }\n        ],\n        \"outputs\": {\n          \"output\": \"retriever\"\n        },\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 456,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 1026.272061488701,\n        \"y\": 540.7157860306309\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"documentStore_0\",\n      \"position\": {\n        \"x\": 826.5653750371686,\n        \"y\": -229.4786988814253\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"documentStore_0\",\n        \"label\": \"Document Store\",\n        \"version\": 1,\n        \"name\": \"documentStore\",\n        \"type\": \"Document\",\n        \"baseClasses\": [\n          \"Document\"\n        ],\n        \"category\": \"Document Loaders\",\n        \"description\": \"Load data from pre-configured document stores\",\n        \"inputParams\": [\n          {\n            \"label\": \"Select Store\",\n            \"name\": \"selectedStore\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listStores\",\n            \"id\": \"documentStore_0-input-selectedStore-asyncOptions\"\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"selectedStore\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"name\": \"output\",\n            \"label\": \"Output\",\n            \"type\": \"options\",\n            \"description\": \"Array of document objects containing metadata and pageContent\",\n            \"options\": [\n              {\n                \"id\": \"documentStore_0-output-document-Document|json\",\n                \"name\": \"document\",\n                \"label\": \"Document\",\n                \"description\": \"Array of document objects containing metadata and pageContent\",\n                \"type\": \"Document | json\"\n              },\n              {\n                \"id\": \"documentStore_0-output-text-string|json\",\n                \"name\": \"text\",\n                \"label\": \"Text\",\n                \"description\": \"Concatenated string from pageContent of documents\",\n                \"type\": \"string | json\"\n              }\n            ],\n            \"default\": \"document\"\n          }\n        ],\n        \"outputs\": {\n          \"output\": \"document\"\n        },\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 310,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 826.5653750371686,\n        \"y\": -229.4786988814253\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"chatMistralAI_0\",\n      \"position\": {\n        \"x\": 1185.9624817228073,\n        \"y\": -60.75719138037451\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"chatMistralAI_0\",\n        \"label\": \"ChatMistralAI\",\n        \"version\": 3,\n        \"name\": \"chatMistralAI\",\n        \"type\": \"ChatMistralAI\",\n        \"baseClasses\": [\n          \"ChatMistralAI\",\n          \"BaseChatModel\",\n          \"BaseLanguageModel\",\n          \"Runnable\"\n        ],\n        \"category\": \"Chat Models\",\n        \"description\": \"Wrapper around Mistral large language models that use the Chat endpoint\",\n        \"inputParams\": [\n          {\n            \"label\": \"Connect Credential\",\n            \"name\": \"credential\",\n            \"type\": \"credential\",\n            \"credentialNames\": [\n              \"mistralAIApi\"\n            ],\n            \"id\": \"chatMistralAI_0-input-credential-credential\"\n          },\n          {\n            \"label\": \"Model Name\",\n            \"name\": \"modelName\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"default\": \"mistral-tiny\",\n            \"id\": \"chatMistralAI_0-input-modelName-asyncOptions\"\n          },\n          {\n            \"label\": \"Temperature\",\n            \"name\": \"temperature\",\n            \"type\": \"number\",\n            \"description\": \"What sampling temperature to use, between 0.0 and 1.0. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\",\n            \"step\": 0.1,\n            \"default\": 0.9,\n            \"optional\": true,\n            \"id\": \"chatMistralAI_0-input-temperature-number\"\n          },\n          {\n            \"label\": \"Max Output Tokens\",\n            \"name\": \"maxOutputTokens\",\n            \"type\": \"number\",\n            \"description\": \"The maximum number of tokens to generate in the completion.\",\n            \"step\": 1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatMistralAI_0-input-maxOutputTokens-number\"\n          },\n          {\n            \"label\": \"Top Probability\",\n            \"name\": \"topP\",\n            \"type\": \"number\",\n            \"description\": \"Nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\",\n            \"step\": 0.1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatMistralAI_0-input-topP-number\"\n          },\n          {\n            \"label\": \"Random Seed\",\n            \"name\": \"randomSeed\",\n            \"type\": \"number\",\n            \"description\": \"The seed to use for random sampling. If set, different calls will generate deterministic results.\",\n            \"step\": 1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatMistralAI_0-input-randomSeed-number\"\n          },\n          {\n            \"label\": \"Safe Mode\",\n            \"name\": \"safeMode\",\n            \"type\": \"boolean\",\n            \"description\": \"Whether to inject a safety prompt before all conversations.\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatMistralAI_0-input-safeMode-boolean\"\n          },\n          {\n            \"label\": \"Override Endpoint\",\n            \"name\": \"overrideEndpoint\",\n            \"type\": \"string\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatMistralAI_0-input-overrideEndpoint-string\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Cache\",\n            \"name\": \"cache\",\n            \"type\": \"BaseCache\",\n            \"optional\": true,\n            \"id\": \"chatMistralAI_0-input-cache-BaseCache\"\n          }\n        ],\n        \"inputs\": {\n          \"cache\": \"\",\n          \"modelName\": \"open-mistral-7b\",\n          \"temperature\": \"0.7\",\n          \"maxOutputTokens\": \"\",\n          \"topP\": \"\",\n          \"randomSeed\": \"\",\n          \"safeMode\": \"\",\n          \"overrideEndpoint\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"chatMistralAI_0-output-chatMistralAI-ChatMistralAI|BaseChatModel|BaseLanguageModel|Runnable\",\n            \"name\": \"chatMistralAI\",\n            \"label\": \"ChatMistralAI\",\n            \"description\": \"Wrapper around Mistral large language models that use the Chat endpoint\",\n            \"type\": \"ChatMistralAI | BaseChatModel | BaseLanguageModel | Runnable\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 571,\n      \"positionAbsolute\": {\n        \"x\": 1185.9624817228073,\n        \"y\": -60.75719138037451\n      },\n      \"selected\": false,\n      \"dragging\": false\n    },\n    {\n      \"id\": \"bufferMemory_0\",\n      \"position\": {\n        \"x\": 1511.4347003706323,\n        \"y\": 45.4357477285582\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"bufferMemory_0\",\n        \"label\": \"Buffer Memory\",\n        \"version\": 2,\n        \"name\": \"bufferMemory\",\n        \"type\": \"BufferMemory\",\n        \"baseClasses\": [\n          \"BufferMemory\",\n          \"BaseChatMemory\",\n          \"BaseMemory\"\n        ],\n        \"category\": \"Memory\",\n        \"description\": \"Retrieve chat messages stored in database\",\n        \"inputParams\": [\n          {\n            \"label\": \"Session Id\",\n            \"name\": \"sessionId\",\n            \"type\": \"string\",\n            \"description\": \"If not specified, a random id will be used. Learn <a target=\\\"_blank\\\" href=\\\"https://docs.flowiseai.com/memory#ui-and-embedded-chat\\\">more</a>\",\n            \"default\": \"\",\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"id\": \"bufferMemory_0-input-sessionId-string\"\n          },\n          {\n            \"label\": \"Memory Key\",\n            \"name\": \"memoryKey\",\n            \"type\": \"string\",\n            \"default\": \"chat_history\",\n            \"additionalParams\": true,\n            \"id\": \"bufferMemory_0-input-memoryKey-string\"\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"sessionId\": \"\",\n          \"memoryKey\": \"chat_history\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"bufferMemory_0-output-bufferMemory-BufferMemory|BaseChatMemory|BaseMemory\",\n            \"name\": \"bufferMemory\",\n            \"label\": \"BufferMemory\",\n            \"description\": \"Retrieve chat messages stored in database\",\n            \"type\": \"BufferMemory | BaseChatMemory | BaseMemory\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 250,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 1511.4347003706323,\n        \"y\": 45.4357477285582\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"pdfFile_0\",\n      \"position\": {\n        \"x\": 489.91103540268375,\n        \"y\": 14.39872090193586\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"pdfFile_0\",\n        \"label\": \"Pdf File\",\n        \"version\": 1,\n        \"name\": \"pdfFile\",\n        \"type\": \"Document\",\n        \"baseClasses\": [\n          \"Document\"\n        ],\n        \"category\": \"Document Loaders\",\n        \"description\": \"Load data from PDF files\",\n        \"inputParams\": [\n          {\n            \"label\": \"Pdf File\",\n            \"name\": \"pdfFile\",\n            \"type\": \"file\",\n            \"fileType\": \".pdf\",\n            \"id\": \"pdfFile_0-input-pdfFile-file\"\n          },\n          {\n            \"label\": \"Usage\",\n            \"name\": \"usage\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"One document per page\",\n                \"name\": \"perPage\"\n              },\n              {\n                \"label\": \"One document per file\",\n                \"name\": \"perFile\"\n              }\n            ],\n            \"default\": \"perPage\",\n            \"id\": \"pdfFile_0-input-usage-options\"\n          },\n          {\n            \"label\": \"Use Legacy Build\",\n            \"name\": \"legacyBuild\",\n            \"type\": \"boolean\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"pdfFile_0-input-legacyBuild-boolean\"\n          },\n          {\n            \"label\": \"Additional Metadata\",\n            \"name\": \"metadata\",\n            \"type\": \"json\",\n            \"description\": \"Additional metadata to be added to the extracted documents\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"pdfFile_0-input-metadata-json\"\n          },\n          {\n            \"label\": \"Omit Metadata Keys\",\n            \"name\": \"omitMetadataKeys\",\n            \"type\": \"string\",\n            \"rows\": 4,\n            \"description\": \"Each document loader comes with a default set of metadata keys that are extracted from the document. You can use this field to omit some of the default metadata keys. The value should be a list of keys, seperated by comma. Use * to omit all metadata keys execept the ones you specify in the Additional Metadata field\",\n            \"placeholder\": \"key1, key2, key3.nestedKey1\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"pdfFile_0-input-omitMetadataKeys-string\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Text Splitter\",\n            \"name\": \"textSplitter\",\n            \"type\": \"TextSplitter\",\n            \"optional\": true,\n            \"id\": \"pdfFile_0-input-textSplitter-TextSplitter\"\n          }\n        ],\n        \"inputs\": {\n          \"textSplitter\": \"{{recursiveCharacterTextSplitter_0.data.instance}}\",\n          \"usage\": \"perPage\",\n          \"legacyBuild\": \"\",\n          \"metadata\": \"\",\n          \"omitMetadataKeys\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"pdfFile_0-output-pdfFile-Document\",\n            \"name\": \"pdfFile\",\n            \"label\": \"Document\",\n            \"description\": \"Load data from PDF files\",\n            \"type\": \"Document\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 524,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 489.91103540268375,\n        \"y\": 14.39872090193586\n      },\n      \"dragging\": false\n    }\n  ],\n  \"edges\": [\n    {\n      \"source\": \"openAIEmbeddings_0\",\n      \"sourceHandle\": \"openAIEmbeddings_0-output-openAIEmbeddings-OpenAIEmbeddings|Embeddings\",\n      \"target\": \"faiss_0\",\n      \"targetHandle\": \"faiss_0-input-embeddings-Embeddings\",\n      \"type\": \"buttonedge\",\n      \"id\": \"openAIEmbeddings_0-openAIEmbeddings_0-output-openAIEmbeddings-OpenAIEmbeddings|Embeddings-faiss_0-faiss_0-input-embeddings-Embeddings\"\n    },\n    {\n      \"source\": \"documentStore_0\",\n      \"sourceHandle\": \"documentStore_0-output-document-Document|json\",\n      \"target\": \"faiss_0\",\n      \"targetHandle\": \"faiss_0-input-document-Document\",\n      \"type\": \"buttonedge\",\n      \"id\": \"documentStore_0-documentStore_0-output-document-Document|json-faiss_0-faiss_0-input-document-Document\"\n    },\n    {\n      \"source\": \"faiss_0\",\n      \"sourceHandle\": \"faiss_0-output-retriever-Faiss|VectorStoreRetriever|BaseRetriever\",\n      \"target\": \"conversationalRetrievalQAChain_0\",\n      \"targetHandle\": \"conversationalRetrievalQAChain_0-input-vectorStoreRetriever-BaseRetriever\",\n      \"type\": \"buttonedge\",\n      \"id\": \"faiss_0-faiss_0-output-retriever-Faiss|VectorStoreRetriever|BaseRetriever-conversationalRetrievalQAChain_0-conversationalRetrievalQAChain_0-input-vectorStoreRetriever-BaseRetriever\"\n    },\n    {\n      \"source\": \"chatMistralAI_0\",\n      \"sourceHandle\": \"chatMistralAI_0-output-chatMistralAI-ChatMistralAI|BaseChatModel|BaseLanguageModel|Runnable\",\n      \"target\": \"conversationalRetrievalQAChain_0\",\n      \"targetHandle\": \"conversationalRetrievalQAChain_0-input-model-BaseChatModel\",\n      \"type\": \"buttonedge\",\n      \"id\": \"chatMistralAI_0-chatMistralAI_0-output-chatMistralAI-ChatMistralAI|BaseChatModel|BaseLanguageModel|Runnable-conversationalRetrievalQAChain_0-conversationalRetrievalQAChain_0-input-model-BaseChatModel\"\n    },\n    {\n      \"source\": \"bufferMemory_0\",\n      \"sourceHandle\": \"bufferMemory_0-output-bufferMemory-BufferMemory|BaseChatMemory|BaseMemory\",\n      \"target\": \"conversationalRetrievalQAChain_0\",\n      \"targetHandle\": \"conversationalRetrievalQAChain_0-input-memory-BaseMemory\",\n      \"type\": \"buttonedge\",\n      \"id\": \"bufferMemory_0-bufferMemory_0-output-bufferMemory-BufferMemory|BaseChatMemory|BaseMemory-conversationalRetrievalQAChain_0-conversationalRetrievalQAChain_0-input-memory-BaseMemory\"\n    },\n    {\n      \"source\": \"recursiveCharacterTextSplitter_0\",\n      \"sourceHandle\": \"recursiveCharacterTextSplitter_0-output-recursiveCharacterTextSplitter-RecursiveCharacterTextSplitter|TextSplitter\",\n      \"target\": \"pdfFile_0\",\n      \"targetHandle\": \"pdfFile_0-input-textSplitter-TextSplitter\",\n      \"type\": \"buttonedge\",\n      \"id\": \"recursiveCharacterTextSplitter_0-recursiveCharacterTextSplitter_0-output-recursiveCharacterTextSplitter-RecursiveCharacterTextSplitter|TextSplitter-pdfFile_0-pdfFile_0-input-textSplitter-TextSplitter\"\n    },\n    {\n      \"source\": \"pdfFile_0\",\n      \"sourceHandle\": \"pdfFile_0-output-pdfFile-Document\",\n      \"target\": \"faiss_0\",\n      \"targetHandle\": \"faiss_0-input-document-Document\",\n      \"type\": \"buttonedge\",\n      \"id\": \"pdfFile_0-pdfFile_0-output-pdfFile-Document-faiss_0-faiss_0-input-document-Document\"\n    }\n  ]\n}",
      "type": "CHATFLOW"
    },
    {
      "id": "866674be-aac7-4de2-8d3f-52c298b38649",
      "name": "databaseretriver (1)",
      "flowData": "{\n  \"nodes\": [\n    {\n      \"id\": \"customFunction_0\",\n      \"position\": {\n        \"x\": 815.8106338990393,\n        \"y\": -1586.9699386384095\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"customFunction_0\",\n        \"label\": \"Custom JS Function\",\n        \"version\": 3,\n        \"name\": \"customFunction\",\n        \"type\": \"CustomFunction\",\n        \"baseClasses\": [\n          \"CustomFunction\",\n          \"Utilities\"\n        ],\n        \"tags\": [\n          \"Utilities\"\n        ],\n        \"category\": \"Utilities\",\n        \"description\": \"Execute custom javascript function\",\n        \"inputParams\": [\n          {\n            \"label\": \"Input Variables\",\n            \"name\": \"functionInputVariables\",\n            \"description\": \"Input variables can be used in the function with prefix $. For example: $var\",\n            \"type\": \"json\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"list\": true,\n            \"id\": \"customFunction_0-input-functionInputVariables-json\"\n          },\n          {\n            \"label\": \"Function Name\",\n            \"name\": \"functionName\",\n            \"type\": \"string\",\n            \"optional\": true,\n            \"placeholder\": \"My Function\",\n            \"id\": \"customFunction_0-input-functionName-string\"\n          },\n          {\n            \"label\": \"Javascript Function\",\n            \"name\": \"javascriptFunction\",\n            \"type\": \"code\",\n            \"id\": \"customFunction_0-input-javascriptFunction-code\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Additional Tools\",\n            \"description\": \"Tools can be used in the function with $tools.{tool_name}.invoke(args)\",\n            \"name\": \"tools\",\n            \"type\": \"Tool\",\n            \"list\": true,\n            \"optional\": true,\n            \"id\": \"customFunction_0-input-tools-Tool\"\n          }\n        ],\n        \"inputs\": {\n          \"functionInputVariables\": \"\",\n          \"functionName\": \"\",\n          \"tools\": \"\",\n          \"javascriptFunction\": \"const HOST = 'localhost';\\nconst USER = 'postgres';\\nconst PASSWORD = '0912';\\nconst DATABASE = 'newdata';\\nconst TABLE = 'newdata';\\nconst { Pool } = require('pg'); // Import the pg module for PostgreSQL\\n\\nlet sqlSchemaPrompt;\\n\\nasync function getSQLPrompt() {\\n  try {\\n    const pool = new Pool({\\n      host: HOST,\\n      user: USER,\\n      password: PASSWORD,\\n      database: DATABASE,\\n    });\\n\\n    // Get schema info\\n    const schemaQuery = `SELECT column_name, data_type, is_nullable \\n                         FROM information_schema.columns \\n                         WHERE table_name = $1`;\\n\\n    const schemaResult = await pool.query(schemaQuery, [TABLE]);\\n    const createColumns = [];\\n    const columnNames = [];\\n\\n    for (const schemaData of schemaResult.rows) {\\n      columnNames.push(schemaData.column_name);\\n      createColumns.push(`${schemaData.column_name} ${schemaData.data_type} ${schemaData.is_nullable === 'NO' ? 'NOT NULL' : ''}`);\\n    }\\n\\n    const sqlCreateTableQuery = `CREATE TABLE ${TABLE} (${createColumns.join(', ')})`;\\n    const sqlSelectTableQuery = `SELECT * FROM ${TABLE} LIMIT 3`;\\n\\n    // Get first 3 rows\\n    const rowsResult = await pool.query(sqlSelectTableQuery);\\n    const allValues = rowsResult.rows.map(row => Object.values(row).join(' '));\\n\\n    sqlSchemaPrompt = sqlCreateTableQuery + '\\\\n' + sqlSelectTableQuery + '\\\\n' + columnNames.join(' ') + '\\\\n' + allValues.join('\\\\n');\\n\\n    await pool.end(); // Close the connection pool\\n  } catch (e) {\\n    console.error(e);\\n    throw e;\\n  }\\n}\\n\\nasync function main() {\\n  await getSQLPrompt();\\n}\\n\\nmain().then(() => console.log(sqlSchemaPrompt)).catch(console.error);\\n\"\n        },\n        \"outputAnchors\": [\n          {\n            \"name\": \"output\",\n            \"label\": \"Output\",\n            \"type\": \"options\",\n            \"description\": \"\",\n            \"options\": [\n              {\n                \"id\": \"customFunction_0-output-output-string|number|boolean|json|array\",\n                \"name\": \"output\",\n                \"label\": \"Output\",\n                \"description\": \"\",\n                \"type\": \"string | number | boolean | json | array\"\n              },\n              {\n                \"id\": \"customFunction_0-output-EndingNode-CustomFunction\",\n                \"name\": \"EndingNode\",\n                \"label\": \"Ending Node\",\n                \"description\": \"\",\n                \"type\": \"CustomFunction\"\n              }\n            ],\n            \"default\": \"output\"\n          }\n        ],\n        \"outputs\": {\n          \"output\": \"output\"\n        },\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 723,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 815.8106338990393,\n        \"y\": -1586.9699386384095\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"chatOpenAI_0\",\n      \"position\": {\n        \"x\": 1139.6518116443945,\n        \"y\": -1935.2947005945705\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"chatOpenAI_0\",\n        \"label\": \"ChatOpenAI\",\n        \"version\": 7,\n        \"name\": \"chatOpenAI\",\n        \"type\": \"ChatOpenAI\",\n        \"baseClasses\": [\n          \"ChatOpenAI\",\n          \"BaseChatModel\",\n          \"BaseLanguageModel\",\n          \"Runnable\"\n        ],\n        \"category\": \"Chat Models\",\n        \"description\": \"Wrapper around OpenAI large language models that use the Chat endpoint\",\n        \"inputParams\": [\n          {\n            \"label\": \"Connect Credential\",\n            \"name\": \"credential\",\n            \"type\": \"credential\",\n            \"credentialNames\": [\n              \"openAIApi\"\n            ],\n            \"id\": \"chatOpenAI_0-input-credential-credential\"\n          },\n          {\n            \"label\": \"Model Name\",\n            \"name\": \"modelName\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"default\": \"gpt-3.5-turbo\",\n            \"id\": \"chatOpenAI_0-input-modelName-asyncOptions\"\n          },\n          {\n            \"label\": \"Temperature\",\n            \"name\": \"temperature\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"default\": 0.9,\n            \"optional\": true,\n            \"id\": \"chatOpenAI_0-input-temperature-number\"\n          },\n          {\n            \"label\": \"Max Tokens\",\n            \"name\": \"maxTokens\",\n            \"type\": \"number\",\n            \"step\": 1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_0-input-maxTokens-number\"\n          },\n          {\n            \"label\": \"Top Probability\",\n            \"name\": \"topP\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_0-input-topP-number\"\n          },\n          {\n            \"label\": \"Frequency Penalty\",\n            \"name\": \"frequencyPenalty\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_0-input-frequencyPenalty-number\"\n          },\n          {\n            \"label\": \"Presence Penalty\",\n            \"name\": \"presencePenalty\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_0-input-presencePenalty-number\"\n          },\n          {\n            \"label\": \"Timeout\",\n            \"name\": \"timeout\",\n            \"type\": \"number\",\n            \"step\": 1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_0-input-timeout-number\"\n          },\n          {\n            \"label\": \"BasePath\",\n            \"name\": \"basepath\",\n            \"type\": \"string\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_0-input-basepath-string\"\n          },\n          {\n            \"label\": \"Proxy Url\",\n            \"name\": \"proxyUrl\",\n            \"type\": \"string\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_0-input-proxyUrl-string\"\n          },\n          {\n            \"label\": \"Stop Sequence\",\n            \"name\": \"stopSequence\",\n            \"type\": \"string\",\n            \"rows\": 4,\n            \"optional\": true,\n            \"description\": \"List of stop words to use when generating. Use comma to separate multiple stop words.\",\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_0-input-stopSequence-string\"\n          },\n          {\n            \"label\": \"BaseOptions\",\n            \"name\": \"baseOptions\",\n            \"type\": \"json\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_0-input-baseOptions-json\"\n          },\n          {\n            \"label\": \"Allow Image Uploads\",\n            \"name\": \"allowImageUploads\",\n            \"type\": \"boolean\",\n            \"description\": \"Automatically uses gpt-4-vision-preview when image is being uploaded from chat. Only works with LLMChain, Conversation Chain, ReAct Agent, Conversational Agent, Tool Agent\",\n            \"default\": false,\n            \"optional\": true,\n            \"id\": \"chatOpenAI_0-input-allowImageUploads-boolean\"\n          },\n          {\n            \"label\": \"Image Resolution\",\n            \"description\": \"This parameter controls the resolution in which the model views the image.\",\n            \"name\": \"imageResolution\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"Low\",\n                \"name\": \"low\"\n              },\n              {\n                \"label\": \"High\",\n                \"name\": \"high\"\n              },\n              {\n                \"label\": \"Auto\",\n                \"name\": \"auto\"\n              }\n            ],\n            \"default\": \"low\",\n            \"optional\": false,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_0-input-imageResolution-options\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Cache\",\n            \"name\": \"cache\",\n            \"type\": \"BaseCache\",\n            \"optional\": true,\n            \"id\": \"chatOpenAI_0-input-cache-BaseCache\"\n          }\n        ],\n        \"inputs\": {\n          \"cache\": \"\",\n          \"modelName\": \"gpt-3.5-turbo\",\n          \"temperature\": \"0.7\",\n          \"maxTokens\": \"\",\n          \"topP\": \"\",\n          \"frequencyPenalty\": \"\",\n          \"presencePenalty\": \"\",\n          \"timeout\": \"\",\n          \"basepath\": \"\",\n          \"proxyUrl\": \"\",\n          \"stopSequence\": \"\",\n          \"baseOptions\": \"\",\n          \"allowImageUploads\": \"\",\n          \"imageResolution\": \"low\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"chatOpenAI_0-output-chatOpenAI-ChatOpenAI|BaseChatModel|BaseLanguageModel|Runnable\",\n            \"name\": \"chatOpenAI\",\n            \"label\": \"ChatOpenAI\",\n            \"description\": \"Wrapper around OpenAI large language models that use the Chat endpoint\",\n            \"type\": \"ChatOpenAI | BaseChatModel | BaseLanguageModel | Runnable\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 668,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 1139.6518116443945,\n        \"y\": -1935.2947005945705\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"llmChain_0\",\n      \"position\": {\n        \"x\": 1661.2911278200088,\n        \"y\": -1461.1712392874351\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"llmChain_0\",\n        \"label\": \"LLM Chain\",\n        \"version\": 3,\n        \"name\": \"llmChain\",\n        \"type\": \"LLMChain\",\n        \"baseClasses\": [\n          \"LLMChain\",\n          \"BaseChain\",\n          \"Runnable\"\n        ],\n        \"category\": \"Chains\",\n        \"description\": \"Chain to run queries against LLMs\",\n        \"inputParams\": [\n          {\n            \"label\": \"Chain Name\",\n            \"name\": \"chainName\",\n            \"type\": \"string\",\n            \"placeholder\": \"Name Your Chain\",\n            \"optional\": true,\n            \"id\": \"llmChain_0-input-chainName-string\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Language Model\",\n            \"name\": \"model\",\n            \"type\": \"BaseLanguageModel\",\n            \"id\": \"llmChain_0-input-model-BaseLanguageModel\"\n          },\n          {\n            \"label\": \"Prompt\",\n            \"name\": \"prompt\",\n            \"type\": \"BasePromptTemplate\",\n            \"id\": \"llmChain_0-input-prompt-BasePromptTemplate\"\n          },\n          {\n            \"label\": \"Output Parser\",\n            \"name\": \"outputParser\",\n            \"type\": \"BaseLLMOutputParser\",\n            \"optional\": true,\n            \"id\": \"llmChain_0-input-outputParser-BaseLLMOutputParser\"\n          },\n          {\n            \"label\": \"Input Moderation\",\n            \"description\": \"Detect text that could generate harmful output and prevent it from being sent to the language model\",\n            \"name\": \"inputModeration\",\n            \"type\": \"Moderation\",\n            \"optional\": true,\n            \"list\": true,\n            \"id\": \"llmChain_0-input-inputModeration-Moderation\"\n          }\n        ],\n        \"inputs\": {\n          \"model\": \"{{chatOpenAI_0.data.instance}}\",\n          \"prompt\": \"{{promptTemplate_4.data.instance}}\",\n          \"outputParser\": \"\",\n          \"inputModeration\": \"\",\n          \"chainName\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"name\": \"output\",\n            \"label\": \"Output\",\n            \"type\": \"options\",\n            \"description\": \"\",\n            \"options\": [\n              {\n                \"id\": \"llmChain_0-output-llmChain-LLMChain|BaseChain|Runnable\",\n                \"name\": \"llmChain\",\n                \"label\": \"LLM Chain\",\n                \"description\": \"\",\n                \"type\": \"LLMChain | BaseChain | Runnable\"\n              },\n              {\n                \"id\": \"llmChain_0-output-outputPrediction-string|json\",\n                \"name\": \"outputPrediction\",\n                \"label\": \"Output Prediction\",\n                \"description\": \"\",\n                \"type\": \"string | json\"\n              }\n            ],\n            \"default\": \"llmChain\"\n          }\n        ],\n        \"outputs\": {\n          \"output\": \"llmChain\"\n        },\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 506,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 1661.2911278200088,\n        \"y\": -1461.1712392874351\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"ifElseFunction_0\",\n      \"position\": {\n        \"x\": 367.89910000015686,\n        \"y\": -1558.5029771198094\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"ifElseFunction_0\",\n        \"label\": \"IfElse Function\",\n        \"version\": 2,\n        \"name\": \"ifElseFunction\",\n        \"type\": \"IfElseFunction\",\n        \"baseClasses\": [\n          \"IfElseFunction\",\n          \"Utilities\"\n        ],\n        \"tags\": [\n          \"Utilities\"\n        ],\n        \"category\": \"Utilities\",\n        \"description\": \"Split flows based on If Else javascript functions\",\n        \"inputParams\": [\n          {\n            \"label\": \"Input Variables\",\n            \"name\": \"functionInputVariables\",\n            \"description\": \"Input variables can be used in the function with prefix $. For example: $var\",\n            \"type\": \"json\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"list\": true,\n            \"id\": \"ifElseFunction_0-input-functionInputVariables-json\"\n          },\n          {\n            \"label\": \"IfElse Name\",\n            \"name\": \"functionName\",\n            \"type\": \"string\",\n            \"optional\": true,\n            \"placeholder\": \"If Condition Match\",\n            \"id\": \"ifElseFunction_0-input-functionName-string\"\n          },\n          {\n            \"label\": \"If Function\",\n            \"name\": \"ifFunction\",\n            \"description\": \"Function must return a value\",\n            \"type\": \"code\",\n            \"rows\": 2,\n            \"default\": \"if (\\\"hello\\\" == \\\"hello\\\") {\\n    return true;\\n}\",\n            \"id\": \"ifElseFunction_0-input-ifFunction-code\"\n          },\n          {\n            \"label\": \"Else Function\",\n            \"name\": \"elseFunction\",\n            \"description\": \"Function must return a value\",\n            \"type\": \"code\",\n            \"rows\": 2,\n            \"default\": \"return false;\",\n            \"id\": \"ifElseFunction_0-input-elseFunction-code\"\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"functionInputVariables\": \"\",\n          \"functionName\": \"\",\n          \"ifFunction\": \"// Ensure $sqlQuery is defined and not null\\nconst sqlQuery = $sqlQuery && $sqlQuery.trim ? $sqlQuery.trim() : '';\\n\\nconst regex = /SELECT\\\\s.*?(?:\\\\n|$)/gi;\\n\\n// Extracting the SQL part\\nconst matches = sqlQuery.match(regex);\\nconst cleanSql = matches ? matches[0].trim() : \\\"\\\";\\n\\n// Check if the query includes SELECT and WHERE for PostgreSQL\\nif (cleanSql.toUpperCase().includes(\\\"SELECT\\\") && cleanSql.toUpperCase().includes(\\\"WHERE\\\")) {\\n    return cleanSql;\\n}\\n\",\n          \"elseFunction\": \"return $sqlQuery;\"\n        },\n        \"outputAnchors\": [\n          {\n            \"name\": \"output\",\n            \"label\": \"Output\",\n            \"type\": \"options\",\n            \"description\": \"\",\n            \"options\": [\n              {\n                \"id\": \"ifElseFunction_0-output-returnTrue-string|number|boolean|json|array\",\n                \"name\": \"returnTrue\",\n                \"label\": \"True\",\n                \"description\": \"\",\n                \"type\": \"string | number | boolean | json | array\",\n                \"isAnchor\": true\n              },\n              {\n                \"id\": \"ifElseFunction_0-output-returnFalse-string|number|boolean|json|array\",\n                \"name\": \"returnFalse\",\n                \"label\": \"False\",\n                \"description\": \"\",\n                \"type\": \"string | number | boolean | json | array\",\n                \"isAnchor\": true\n              }\n            ],\n            \"default\": \"returnTrue\"\n          }\n        ],\n        \"outputs\": {\n          \"output\": \"returnTrue\"\n        },\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 764,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 367.89910000015686,\n        \"y\": -1558.5029771198094\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"customFunction_1\",\n      \"position\": {\n        \"x\": -855.0387778308864,\n        \"y\": -1503.6724441072079\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"customFunction_1\",\n        \"label\": \"Custom JS Function\",\n        \"version\": 3,\n        \"name\": \"customFunction\",\n        \"type\": \"CustomFunction\",\n        \"baseClasses\": [\n          \"CustomFunction\",\n          \"Utilities\"\n        ],\n        \"tags\": [\n          \"Utilities\"\n        ],\n        \"category\": \"Utilities\",\n        \"description\": \"Execute custom javascript function\",\n        \"inputParams\": [\n          {\n            \"label\": \"Input Variables\",\n            \"name\": \"functionInputVariables\",\n            \"description\": \"Input variables can be used in the function with prefix $. For example: $var\",\n            \"type\": \"json\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"list\": true,\n            \"id\": \"customFunction_1-input-functionInputVariables-json\"\n          },\n          {\n            \"label\": \"Function Name\",\n            \"name\": \"functionName\",\n            \"type\": \"string\",\n            \"optional\": true,\n            \"placeholder\": \"My Function\",\n            \"id\": \"customFunction_1-input-functionName-string\"\n          },\n          {\n            \"label\": \"Javascript Function\",\n            \"name\": \"javascriptFunction\",\n            \"type\": \"code\",\n            \"id\": \"customFunction_1-input-javascriptFunction-code\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Additional Tools\",\n            \"description\": \"Tools can be used in the function with $tools.{tool_name}.invoke(args)\",\n            \"name\": \"tools\",\n            \"type\": \"Tool\",\n            \"list\": true,\n            \"optional\": true,\n            \"id\": \"customFunction_1-input-tools-Tool\"\n          }\n        ],\n        \"inputs\": {\n          \"functionInputVariables\": \"\",\n          \"functionName\": \"\",\n          \"tools\": \"\",\n          \"javascriptFunction\": \"const HOST = 'localhost';\\nconst USER = 'postgres';\\nconst PASSWORD = '0912';\\nconst DATABASE = 'newdata';\\nconst TABLE = 'newdata';\\nconst { Pool } = require('pg'); // Import the pg module for PostgreSQL\\n\\nlet sqlSchemaPrompt;\\n\\nasync function getSQLPrompt() {\\n  try {\\n    const pool = new Pool({\\n      host: HOST,\\n      user: USER,\\n      password: PASSWORD,\\n      database: DATABASE,\\n    });\\n\\n    // Get schema info\\n    const schemaQuery = `SELECT column_name, data_type, is_nullable \\n                         FROM information_schema.columns \\n                         WHERE table_name = $1`;\\n\\n    const schemaResult = await pool.query(schemaQuery, [TABLE]);\\n    const createColumns = [];\\n    const columnNames = [];\\n\\n    // Loop through schema result and build the CREATE TABLE statement\\n    for (const schemaData of schemaResult.rows) {\\n      columnNames.push(schemaData.column_name);\\n      createColumns.push(`${schemaData.column_name} ${schemaData.data_type} ${schemaData.is_nullable === 'NO' ? 'NOT NULL' : ''}`);\\n    }\\n\\n    // Build the CREATE TABLE SQL query\\n    const sqlCreateTableQuery = `CREATE TABLE ${TABLE} (${createColumns.join(', ')})`;\\n\\n    // Select first 3 rows from the table\\n    const sqlSelectTableQuery = `SELECT * FROM ${TABLE} `;\\n\\n    // Get first 3 rows from the table\\n    const rowsResult = await pool.query(sqlSelectTableQuery);\\n    const allValues = rowsResult.rows.map(row => Object.values(row).join(' '));\\n\\n    // Combine schema info, queries, and values\\n    sqlSchemaPrompt = sqlCreateTableQuery + '\\\\n' + sqlSelectTableQuery + '\\\\n' + columnNames.join(' ') + '\\\\n' + allValues.join('\\\\n');\\n\\n    await pool.end(); // Close the connection pool\\n  } catch (e) {\\n    console.error(e);\\n    throw e;\\n  }\\n}\\n\\nasync function main() {\\n  await getSQLPrompt();  // Fetch schema and data\\n}\\n\\nmain().then(() => console.log(sqlSchemaPrompt)).catch(console.error);\\n\"\n        },\n        \"outputAnchors\": [\n          {\n            \"name\": \"output\",\n            \"label\": \"Output\",\n            \"type\": \"options\",\n            \"description\": \"\",\n            \"options\": [\n              {\n                \"id\": \"customFunction_1-output-output-string|number|boolean|json|array\",\n                \"name\": \"output\",\n                \"label\": \"Output\",\n                \"description\": \"\",\n                \"type\": \"string | number | boolean | json | array\"\n              },\n              {\n                \"id\": \"customFunction_1-output-EndingNode-CustomFunction\",\n                \"name\": \"EndingNode\",\n                \"label\": \"Ending Node\",\n                \"description\": \"\",\n                \"type\": \"CustomFunction\"\n              }\n            ],\n            \"default\": \"output\"\n          }\n        ],\n        \"outputs\": {\n          \"output\": \"output\"\n        },\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 723,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": -855.0387778308864,\n        \"y\": -1503.6724441072079\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"chatOpenAI_1\",\n      \"position\": {\n        \"x\": -418.2408770778424,\n        \"y\": -1555.1083268726193\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"chatOpenAI_1\",\n        \"label\": \"ChatOpenAI\",\n        \"version\": 7,\n        \"name\": \"chatOpenAI\",\n        \"type\": \"ChatOpenAI\",\n        \"baseClasses\": [\n          \"ChatOpenAI\",\n          \"BaseChatModel\",\n          \"BaseLanguageModel\",\n          \"Runnable\"\n        ],\n        \"category\": \"Chat Models\",\n        \"description\": \"Wrapper around OpenAI large language models that use the Chat endpoint\",\n        \"inputParams\": [\n          {\n            \"label\": \"Connect Credential\",\n            \"name\": \"credential\",\n            \"type\": \"credential\",\n            \"credentialNames\": [\n              \"openAIApi\"\n            ],\n            \"id\": \"chatOpenAI_1-input-credential-credential\"\n          },\n          {\n            \"label\": \"Model Name\",\n            \"name\": \"modelName\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"default\": \"gpt-3.5-turbo\",\n            \"id\": \"chatOpenAI_1-input-modelName-asyncOptions\"\n          },\n          {\n            \"label\": \"Temperature\",\n            \"name\": \"temperature\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"default\": 0.9,\n            \"optional\": true,\n            \"id\": \"chatOpenAI_1-input-temperature-number\"\n          },\n          {\n            \"label\": \"Max Tokens\",\n            \"name\": \"maxTokens\",\n            \"type\": \"number\",\n            \"step\": 1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_1-input-maxTokens-number\"\n          },\n          {\n            \"label\": \"Top Probability\",\n            \"name\": \"topP\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_1-input-topP-number\"\n          },\n          {\n            \"label\": \"Frequency Penalty\",\n            \"name\": \"frequencyPenalty\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_1-input-frequencyPenalty-number\"\n          },\n          {\n            \"label\": \"Presence Penalty\",\n            \"name\": \"presencePenalty\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_1-input-presencePenalty-number\"\n          },\n          {\n            \"label\": \"Timeout\",\n            \"name\": \"timeout\",\n            \"type\": \"number\",\n            \"step\": 1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_1-input-timeout-number\"\n          },\n          {\n            \"label\": \"BasePath\",\n            \"name\": \"basepath\",\n            \"type\": \"string\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_1-input-basepath-string\"\n          },\n          {\n            \"label\": \"Proxy Url\",\n            \"name\": \"proxyUrl\",\n            \"type\": \"string\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_1-input-proxyUrl-string\"\n          },\n          {\n            \"label\": \"Stop Sequence\",\n            \"name\": \"stopSequence\",\n            \"type\": \"string\",\n            \"rows\": 4,\n            \"optional\": true,\n            \"description\": \"List of stop words to use when generating. Use comma to separate multiple stop words.\",\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_1-input-stopSequence-string\"\n          },\n          {\n            \"label\": \"BaseOptions\",\n            \"name\": \"baseOptions\",\n            \"type\": \"json\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_1-input-baseOptions-json\"\n          },\n          {\n            \"label\": \"Allow Image Uploads\",\n            \"name\": \"allowImageUploads\",\n            \"type\": \"boolean\",\n            \"description\": \"Automatically uses gpt-4-vision-preview when image is being uploaded from chat. Only works with LLMChain, Conversation Chain, ReAct Agent, Conversational Agent, Tool Agent\",\n            \"default\": false,\n            \"optional\": true,\n            \"id\": \"chatOpenAI_1-input-allowImageUploads-boolean\"\n          },\n          {\n            \"label\": \"Image Resolution\",\n            \"description\": \"This parameter controls the resolution in which the model views the image.\",\n            \"name\": \"imageResolution\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"Low\",\n                \"name\": \"low\"\n              },\n              {\n                \"label\": \"High\",\n                \"name\": \"high\"\n              },\n              {\n                \"label\": \"Auto\",\n                \"name\": \"auto\"\n              }\n            ],\n            \"default\": \"low\",\n            \"optional\": false,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_1-input-imageResolution-options\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Cache\",\n            \"name\": \"cache\",\n            \"type\": \"BaseCache\",\n            \"optional\": true,\n            \"id\": \"chatOpenAI_1-input-cache-BaseCache\"\n          }\n        ],\n        \"inputs\": {\n          \"cache\": \"\",\n          \"modelName\": \"gpt-3.5-turbo\",\n          \"temperature\": \"0.7\",\n          \"maxTokens\": \"\",\n          \"topP\": \"\",\n          \"frequencyPenalty\": \"\",\n          \"presencePenalty\": \"\",\n          \"timeout\": \"\",\n          \"basepath\": \"\",\n          \"proxyUrl\": \"\",\n          \"stopSequence\": \"\",\n          \"baseOptions\": \"\",\n          \"allowImageUploads\": \"\",\n          \"imageResolution\": \"low\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"chatOpenAI_1-output-chatOpenAI-ChatOpenAI|BaseChatModel|BaseLanguageModel|Runnable\",\n            \"name\": \"chatOpenAI\",\n            \"label\": \"ChatOpenAI\",\n            \"description\": \"Wrapper around OpenAI large language models that use the Chat endpoint\",\n            \"type\": \"ChatOpenAI | BaseChatModel | BaseLanguageModel | Runnable\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 668,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": -418.2408770778424,\n        \"y\": -1555.1083268726193\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"llmChain_1\",\n      \"position\": {\n        \"x\": -20.791200171381718,\n        \"y\": -1585.292554108034\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"llmChain_1\",\n        \"label\": \"LLM Chain\",\n        \"version\": 3,\n        \"name\": \"llmChain\",\n        \"type\": \"LLMChain\",\n        \"baseClasses\": [\n          \"LLMChain\",\n          \"BaseChain\",\n          \"Runnable\"\n        ],\n        \"category\": \"Chains\",\n        \"description\": \"Chain to run queries against LLMs\",\n        \"inputParams\": [\n          {\n            \"label\": \"Chain Name\",\n            \"name\": \"chainName\",\n            \"type\": \"string\",\n            \"placeholder\": \"Name Your Chain\",\n            \"optional\": true,\n            \"id\": \"llmChain_1-input-chainName-string\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Language Model\",\n            \"name\": \"model\",\n            \"type\": \"BaseLanguageModel\",\n            \"id\": \"llmChain_1-input-model-BaseLanguageModel\"\n          },\n          {\n            \"label\": \"Prompt\",\n            \"name\": \"prompt\",\n            \"type\": \"BasePromptTemplate\",\n            \"id\": \"llmChain_1-input-prompt-BasePromptTemplate\"\n          },\n          {\n            \"label\": \"Output Parser\",\n            \"name\": \"outputParser\",\n            \"type\": \"BaseLLMOutputParser\",\n            \"optional\": true,\n            \"id\": \"llmChain_1-input-outputParser-BaseLLMOutputParser\"\n          },\n          {\n            \"label\": \"Input Moderation\",\n            \"description\": \"Detect text that could generate harmful output and prevent it from being sent to the language model\",\n            \"name\": \"inputModeration\",\n            \"type\": \"Moderation\",\n            \"optional\": true,\n            \"list\": true,\n            \"id\": \"llmChain_1-input-inputModeration-Moderation\"\n          }\n        ],\n        \"inputs\": {\n          \"model\": \"{{chatOpenAI_1.data.instance}}\",\n          \"prompt\": \"{{promptTemplate_1.data.instance}}\",\n          \"outputParser\": \"\",\n          \"inputModeration\": \"\",\n          \"chainName\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"name\": \"output\",\n            \"label\": \"Output\",\n            \"type\": \"options\",\n            \"description\": \"\",\n            \"options\": [\n              {\n                \"id\": \"llmChain_1-output-llmChain-LLMChain|BaseChain|Runnable\",\n                \"name\": \"llmChain\",\n                \"label\": \"LLM Chain\",\n                \"description\": \"\",\n                \"type\": \"LLMChain | BaseChain | Runnable\"\n              },\n              {\n                \"id\": \"llmChain_1-output-outputPrediction-string|json\",\n                \"name\": \"outputPrediction\",\n                \"label\": \"Output Prediction\",\n                \"description\": \"\",\n                \"type\": \"string | json\"\n              }\n            ],\n            \"default\": \"llmChain\"\n          }\n        ],\n        \"outputs\": {\n          \"output\": \"outputPrediction\"\n        },\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 506,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": -20.791200171381718,\n        \"y\": -1585.292554108034\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"promptTemplate_1\",\n      \"position\": {\n        \"x\": -395.72333221813,\n        \"y\": -822.9496528372629\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"promptTemplate_1\",\n        \"label\": \"Prompt Template\",\n        \"version\": 1,\n        \"name\": \"promptTemplate\",\n        \"type\": \"PromptTemplate\",\n        \"baseClasses\": [\n          \"PromptTemplate\",\n          \"BaseStringPromptTemplate\",\n          \"BasePromptTemplate\",\n          \"Runnable\"\n        ],\n        \"category\": \"Prompts\",\n        \"description\": \"Schema to represent a basic prompt for an LLM\",\n        \"inputParams\": [\n          {\n            \"label\": \"Template\",\n            \"name\": \"template\",\n            \"type\": \"string\",\n            \"rows\": 4,\n            \"placeholder\": \"What is a good name for a company that makes {product}?\",\n            \"id\": \"promptTemplate_1-input-template-string\"\n          },\n          {\n            \"label\": \"Format Prompt Values\",\n            \"name\": \"promptValues\",\n            \"type\": \"json\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"list\": true,\n            \"id\": \"promptTemplate_1-input-promptValues-json\"\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"template\": \"Based on the provided postgresql  table schema and question below, return a SQL SELECT ALL query that would answer the user's question. For example: SELECT * FROM table WHERE id = '1'.\\n------------\\nSCHEMA: {schema}\\n------------\\nQUESTION: {question}\\n------------\\nSQL QUERY:\",\n          \"promptValues\": \"{\\\"schema\\\":\\\"{{customFunction_0.data.instance}}\\\",\\\"question\\\":\\\"{{question}}\\\"}\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"promptTemplate_1-output-promptTemplate-PromptTemplate|BaseStringPromptTemplate|BasePromptTemplate|Runnable\",\n            \"name\": \"promptTemplate\",\n            \"label\": \"PromptTemplate\",\n            \"description\": \"Schema to represent a basic prompt for an LLM\",\n            \"type\": \"PromptTemplate | BaseStringPromptTemplate | BasePromptTemplate | Runnable\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 509,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": -395.72333221813,\n        \"y\": -822.9496528372629\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"llmChain_2\",\n      \"position\": {\n        \"x\": 1303.092374471539,\n        \"y\": -642.3580063367729\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"llmChain_2\",\n        \"label\": \"LLM Chain\",\n        \"version\": 3,\n        \"name\": \"llmChain\",\n        \"type\": \"LLMChain\",\n        \"baseClasses\": [\n          \"LLMChain\",\n          \"BaseChain\",\n          \"Runnable\"\n        ],\n        \"category\": \"Chains\",\n        \"description\": \"Chain to run queries against LLMs\",\n        \"inputParams\": [\n          {\n            \"label\": \"Chain Name\",\n            \"name\": \"chainName\",\n            \"type\": \"string\",\n            \"placeholder\": \"Name Your Chain\",\n            \"optional\": true,\n            \"id\": \"llmChain_2-input-chainName-string\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Language Model\",\n            \"name\": \"model\",\n            \"type\": \"BaseLanguageModel\",\n            \"id\": \"llmChain_2-input-model-BaseLanguageModel\"\n          },\n          {\n            \"label\": \"Prompt\",\n            \"name\": \"prompt\",\n            \"type\": \"BasePromptTemplate\",\n            \"id\": \"llmChain_2-input-prompt-BasePromptTemplate\"\n          },\n          {\n            \"label\": \"Output Parser\",\n            \"name\": \"outputParser\",\n            \"type\": \"BaseLLMOutputParser\",\n            \"optional\": true,\n            \"id\": \"llmChain_2-input-outputParser-BaseLLMOutputParser\"\n          },\n          {\n            \"label\": \"Input Moderation\",\n            \"description\": \"Detect text that could generate harmful output and prevent it from being sent to the language model\",\n            \"name\": \"inputModeration\",\n            \"type\": \"Moderation\",\n            \"optional\": true,\n            \"list\": true,\n            \"id\": \"llmChain_2-input-inputModeration-Moderation\"\n          }\n        ],\n        \"inputs\": {\n          \"model\": \"\",\n          \"prompt\": \"{{promptTemplate_2.data.instance}}\",\n          \"outputParser\": \"\",\n          \"inputModeration\": \"\",\n          \"chainName\": \"Fallback chain\"\n        },\n        \"outputAnchors\": [\n          {\n            \"name\": \"output\",\n            \"label\": \"Output\",\n            \"type\": \"options\",\n            \"description\": \"\",\n            \"options\": [\n              {\n                \"id\": \"llmChain_2-output-llmChain-LLMChain|BaseChain|Runnable\",\n                \"name\": \"llmChain\",\n                \"label\": \"LLM Chain\",\n                \"description\": \"\",\n                \"type\": \"LLMChain | BaseChain | Runnable\"\n              },\n              {\n                \"id\": \"llmChain_2-output-outputPrediction-string|json\",\n                \"name\": \"outputPrediction\",\n                \"label\": \"Output Prediction\",\n                \"description\": \"\",\n                \"type\": \"string | json\"\n              }\n            ],\n            \"default\": \"llmChain\"\n          }\n        ],\n        \"outputs\": {\n          \"output\": \"llmChain\"\n        },\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 506,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 1303.092374471539,\n        \"y\": -642.3580063367729\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"promptTemplate_2\",\n      \"position\": {\n        \"x\": 869.7952222419366,\n        \"y\": -756.4302333273861\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"promptTemplate_2\",\n        \"label\": \"Prompt Template\",\n        \"version\": 1,\n        \"name\": \"promptTemplate\",\n        \"type\": \"PromptTemplate\",\n        \"baseClasses\": [\n          \"PromptTemplate\",\n          \"BaseStringPromptTemplate\",\n          \"BasePromptTemplate\",\n          \"Runnable\"\n        ],\n        \"category\": \"Prompts\",\n        \"description\": \"Schema to represent a basic prompt for an LLM\",\n        \"inputParams\": [\n          {\n            \"label\": \"Template\",\n            \"name\": \"template\",\n            \"type\": \"string\",\n            \"rows\": 4,\n            \"placeholder\": \"What is a good name for a company that makes {product}?\",\n            \"id\": \"promptTemplate_2-input-template-string\"\n          },\n          {\n            \"label\": \"Format Prompt Values\",\n            \"name\": \"promptValues\",\n            \"type\": \"json\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"list\": true,\n            \"id\": \"promptTemplate_2-input-promptValues-json\"\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"template\": \"politely say i am not able to answer the query\",\n          \"promptValues\": \"{\\\"schema\\\":\\\"{{customFunction_0.data.instance}}\\\",\\\"question\\\":\\\"{{question}}\\\"}\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"promptTemplate_2-output-promptTemplate-PromptTemplate|BaseStringPromptTemplate|BasePromptTemplate|Runnable\",\n            \"name\": \"promptTemplate\",\n            \"label\": \"PromptTemplate\",\n            \"description\": \"Schema to represent a basic prompt for an LLM\",\n            \"type\": \"PromptTemplate | BaseStringPromptTemplate | BasePromptTemplate | Runnable\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 509,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 869.7952222419366,\n        \"y\": -756.4302333273861\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"promptTemplate_4\",\n      \"position\": {\n        \"x\": 1201.5587822935686,\n        \"y\": -1227.1641815986081\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"promptTemplate_4\",\n        \"label\": \"Prompt Template\",\n        \"version\": 1,\n        \"name\": \"promptTemplate\",\n        \"type\": \"PromptTemplate\",\n        \"baseClasses\": [\n          \"PromptTemplate\",\n          \"BaseStringPromptTemplate\",\n          \"BasePromptTemplate\",\n          \"Runnable\"\n        ],\n        \"category\": \"Prompts\",\n        \"description\": \"Schema to represent a basic prompt for an LLM\",\n        \"inputParams\": [\n          {\n            \"label\": \"Template\",\n            \"name\": \"template\",\n            \"type\": \"string\",\n            \"rows\": 4,\n            \"placeholder\": \"What is a good name for a company that makes {product}?\",\n            \"id\": \"promptTemplate_4-input-template-string\"\n          },\n          {\n            \"label\": \"Format Prompt Values\",\n            \"name\": \"promptValues\",\n            \"type\": \"json\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"list\": true,\n            \"id\": \"promptTemplate_4-input-promptValues-json\"\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"template\": \"Based on the question, and SQL response, write a natural language response, be details as possible:\\n------------\\nQUESTION: {question}\\n------------\\nSQL RESPONSE: {sqlResponse}\\n------------\\nNATURAL LANGUAGE RESPONSE:\",\n          \"promptValues\": \"{\\\"schema\\\":\\\"{{customFunction_0.data.instance}}\\\",\\\"question\\\":\\\"{{question}}\\\"}\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"promptTemplate_4-output-promptTemplate-PromptTemplate|BaseStringPromptTemplate|BasePromptTemplate|Runnable\",\n            \"name\": \"promptTemplate\",\n            \"label\": \"PromptTemplate\",\n            \"description\": \"Schema to represent a basic prompt for an LLM\",\n            \"type\": \"PromptTemplate | BaseStringPromptTemplate | BasePromptTemplate | Runnable\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 509,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 1201.5587822935686,\n        \"y\": -1227.1641815986081\n      },\n      \"dragging\": false\n    }\n  ],\n  \"edges\": [\n    {\n      \"source\": \"chatOpenAI_0\",\n      \"sourceHandle\": \"chatOpenAI_0-output-chatOpenAI-ChatOpenAI|BaseChatModel|BaseLanguageModel|Runnable\",\n      \"target\": \"llmChain_0\",\n      \"targetHandle\": \"llmChain_0-input-model-BaseLanguageModel\",\n      \"type\": \"buttonedge\",\n      \"id\": \"chatOpenAI_0-chatOpenAI_0-output-chatOpenAI-ChatOpenAI|BaseChatModel|BaseLanguageModel|Runnable-llmChain_0-llmChain_0-input-model-BaseLanguageModel\"\n    },\n    {\n      \"source\": \"chatOpenAI_1\",\n      \"sourceHandle\": \"chatOpenAI_1-output-chatOpenAI-ChatOpenAI|BaseChatModel|BaseLanguageModel|Runnable\",\n      \"target\": \"llmChain_1\",\n      \"targetHandle\": \"llmChain_1-input-model-BaseLanguageModel\",\n      \"type\": \"buttonedge\",\n      \"id\": \"chatOpenAI_1-chatOpenAI_1-output-chatOpenAI-ChatOpenAI|BaseChatModel|BaseLanguageModel|Runnable-llmChain_1-llmChain_1-input-model-BaseLanguageModel\"\n    },\n    {\n      \"source\": \"promptTemplate_1\",\n      \"sourceHandle\": \"promptTemplate_1-output-promptTemplate-PromptTemplate|BaseStringPromptTemplate|BasePromptTemplate|Runnable\",\n      \"target\": \"llmChain_1\",\n      \"targetHandle\": \"llmChain_1-input-prompt-BasePromptTemplate\",\n      \"type\": \"buttonedge\",\n      \"id\": \"promptTemplate_1-promptTemplate_1-output-promptTemplate-PromptTemplate|BaseStringPromptTemplate|BasePromptTemplate|Runnable-llmChain_1-llmChain_1-input-prompt-BasePromptTemplate\"\n    },\n    {\n      \"source\": \"llmChain_1\",\n      \"sourceHandle\": \"llmChain_1-output-outputPrediction-string|json\",\n      \"target\": \"ifElseFunction_0\",\n      \"targetHandle\": \"ifElseFunction_0-input-functionInputVariables-json\",\n      \"type\": \"buttonedge\",\n      \"id\": \"llmChain_1-llmChain_1-output-outputPrediction-string|json-ifElseFunction_0-ifElseFunction_0-input-functionInputVariables-json\"\n    },\n    {\n      \"source\": \"customFunction_1\",\n      \"sourceHandle\": \"customFunction_1-output-output-string|number|boolean|json|array\",\n      \"target\": \"promptTemplate_1\",\n      \"targetHandle\": \"promptTemplate_1-input-promptValues-json\",\n      \"type\": \"buttonedge\",\n      \"id\": \"customFunction_1-customFunction_1-output-output-string|number|boolean|json|array-promptTemplate_1-promptTemplate_1-input-promptValues-json\"\n    },\n    {\n      \"source\": \"ifElseFunction_0\",\n      \"sourceHandle\": \"ifElseFunction_0-output-returnTrue-string|number|boolean|json|array\",\n      \"target\": \"customFunction_0\",\n      \"targetHandle\": \"customFunction_0-input-functionInputVariables-json\",\n      \"type\": \"buttonedge\",\n      \"id\": \"ifElseFunction_0-ifElseFunction_0-output-returnTrue-string|number|boolean|json|array-customFunction_0-customFunction_0-input-functionInputVariables-json\"\n    },\n    {\n      \"source\": \"ifElseFunction_0\",\n      \"sourceHandle\": \"ifElseFunction_0-output-returnFalse-string|number|boolean|json|array\",\n      \"target\": \"promptTemplate_2\",\n      \"targetHandle\": \"promptTemplate_2-input-promptValues-json\",\n      \"type\": \"buttonedge\",\n      \"id\": \"ifElseFunction_0-ifElseFunction_0-output-returnFalse-string|number|boolean|json|array-promptTemplate_2-promptTemplate_2-input-promptValues-json\"\n    },\n    {\n      \"source\": \"promptTemplate_2\",\n      \"sourceHandle\": \"promptTemplate_2-output-promptTemplate-PromptTemplate|BaseStringPromptTemplate|BasePromptTemplate|Runnable\",\n      \"target\": \"llmChain_2\",\n      \"targetHandle\": \"llmChain_2-input-prompt-BasePromptTemplate\",\n      \"type\": \"buttonedge\",\n      \"id\": \"promptTemplate_2-promptTemplate_2-output-promptTemplate-PromptTemplate|BaseStringPromptTemplate|BasePromptTemplate|Runnable-llmChain_2-llmChain_2-input-prompt-BasePromptTemplate\"\n    },\n    {\n      \"source\": \"promptTemplate_4\",\n      \"sourceHandle\": \"promptTemplate_4-output-promptTemplate-PromptTemplate|BaseStringPromptTemplate|BasePromptTemplate|Runnable\",\n      \"target\": \"llmChain_0\",\n      \"targetHandle\": \"llmChain_0-input-prompt-BasePromptTemplate\",\n      \"type\": \"buttonedge\",\n      \"id\": \"promptTemplate_4-promptTemplate_4-output-promptTemplate-PromptTemplate|BaseStringPromptTemplate|BasePromptTemplate|Runnable-llmChain_0-llmChain_0-input-prompt-BasePromptTemplate\"\n    },\n    {\n      \"source\": \"customFunction_0\",\n      \"sourceHandle\": \"customFunction_0-output-output-string|number|boolean|json|array\",\n      \"target\": \"promptTemplate_4\",\n      \"targetHandle\": \"promptTemplate_4-input-promptValues-json\",\n      \"type\": \"buttonedge\",\n      \"id\": \"customFunction_0-customFunction_0-output-output-string|number|boolean|json|array-promptTemplate_4-promptTemplate_4-input-promptValues-json\"\n    }\n  ]\n}",
      "type": "CHATFLOW"
    },
    {
      "id": "09170cee-8bfe-4cd9-80bc-4dc72d42ad9e",
      "name": "webpage q&a",
      "flowData": "{\n  \"nodes\": [\n    {\n      \"width\": 300,\n      \"height\": 423,\n      \"id\": \"openAIEmbeddings_0\",\n      \"position\": {\n        \"x\": 805.4033852865127,\n        \"y\": 289.17383087232275\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"openAIEmbeddings_0\",\n        \"label\": \"OpenAI Embeddings\",\n        \"version\": 4,\n        \"name\": \"openAIEmbeddings\",\n        \"type\": \"OpenAIEmbeddings\",\n        \"baseClasses\": [\n          \"OpenAIEmbeddings\",\n          \"Embeddings\"\n        ],\n        \"category\": \"Embeddings\",\n        \"description\": \"OpenAI API to generate embeddings for a given text\",\n        \"inputParams\": [\n          {\n            \"label\": \"Connect Credential\",\n            \"name\": \"credential\",\n            \"type\": \"credential\",\n            \"credentialNames\": [\n              \"openAIApi\"\n            ],\n            \"id\": \"openAIEmbeddings_0-input-credential-credential\"\n          },\n          {\n            \"label\": \"Model Name\",\n            \"name\": \"modelName\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"default\": \"text-embedding-ada-002\",\n            \"id\": \"openAIEmbeddings_0-input-modelName-asyncOptions\"\n          },\n          {\n            \"label\": \"Strip New Lines\",\n            \"name\": \"stripNewLines\",\n            \"type\": \"boolean\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"openAIEmbeddings_0-input-stripNewLines-boolean\"\n          },\n          {\n            \"label\": \"Batch Size\",\n            \"name\": \"batchSize\",\n            \"type\": \"number\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"openAIEmbeddings_0-input-batchSize-number\"\n          },\n          {\n            \"label\": \"Timeout\",\n            \"name\": \"timeout\",\n            \"type\": \"number\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"openAIEmbeddings_0-input-timeout-number\"\n          },\n          {\n            \"label\": \"BasePath\",\n            \"name\": \"basepath\",\n            \"type\": \"string\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"openAIEmbeddings_0-input-basepath-string\"\n          },\n          {\n            \"label\": \"Dimensions\",\n            \"name\": \"dimensions\",\n            \"type\": \"number\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"openAIEmbeddings_0-input-dimensions-number\"\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"modelName\": \"text-embedding-3-large\",\n          \"stripNewLines\": \"\",\n          \"batchSize\": \"\",\n          \"timeout\": \"\",\n          \"basepath\": \"\",\n          \"dimensions\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"openAIEmbeddings_0-output-openAIEmbeddings-OpenAIEmbeddings|Embeddings\",\n            \"name\": \"openAIEmbeddings\",\n            \"label\": \"OpenAIEmbeddings\",\n            \"description\": \"OpenAI API to generate embeddings for a given text\",\n            \"type\": \"OpenAIEmbeddings | Embeddings\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 805.4033852865127,\n        \"y\": 289.17383087232275\n      },\n      \"dragging\": false\n    },\n    {\n      \"width\": 300,\n      \"height\": 376,\n      \"id\": \"htmlToMarkdownTextSplitter_0\",\n      \"position\": {\n        \"x\": 313.9274321425419,\n        \"y\": -13.58414896069695\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"htmlToMarkdownTextSplitter_0\",\n        \"label\": \"HtmlToMarkdown Text Splitter\",\n        \"version\": 1,\n        \"name\": \"htmlToMarkdownTextSplitter\",\n        \"type\": \"HtmlToMarkdownTextSplitter\",\n        \"baseClasses\": [\n          \"HtmlToMarkdownTextSplitter\",\n          \"MarkdownTextSplitter\",\n          \"RecursiveCharacterTextSplitter\",\n          \"TextSplitter\",\n          \"BaseDocumentTransformer\"\n        ],\n        \"category\": \"Text Splitters\",\n        \"description\": \"Converts Html to Markdown and then split your content into documents based on the Markdown headers\",\n        \"inputParams\": [\n          {\n            \"label\": \"Chunk Size\",\n            \"name\": \"chunkSize\",\n            \"type\": \"number\",\n            \"default\": 1000,\n            \"optional\": true,\n            \"id\": \"htmlToMarkdownTextSplitter_0-input-chunkSize-number\"\n          },\n          {\n            \"label\": \"Chunk Overlap\",\n            \"name\": \"chunkOverlap\",\n            \"type\": \"number\",\n            \"optional\": true,\n            \"id\": \"htmlToMarkdownTextSplitter_0-input-chunkOverlap-number\"\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"chunkSize\": \"1000\",\n          \"chunkOverlap\": \"200\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"htmlToMarkdownTextSplitter_0-output-htmlToMarkdownTextSplitter-HtmlToMarkdownTextSplitter|MarkdownTextSplitter|RecursiveCharacterTextSplitter|TextSplitter|BaseDocumentTransformer\",\n            \"name\": \"htmlToMarkdownTextSplitter\",\n            \"label\": \"HtmlToMarkdownTextSplitter\",\n            \"type\": \"HtmlToMarkdownTextSplitter | MarkdownTextSplitter | RecursiveCharacterTextSplitter | TextSplitter | BaseDocumentTransformer\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 313.9274321425419,\n        \"y\": -13.58414896069695\n      },\n      \"dragging\": false\n    },\n    {\n      \"width\": 300,\n      \"height\": 530,\n      \"id\": \"conversationalRetrievalQAChain_0\",\n      \"position\": {\n        \"x\": 1892.82894546983,\n        \"y\": 282.2572649522094\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"conversationalRetrievalQAChain_0\",\n        \"label\": \"Conversational Retrieval QA Chain\",\n        \"version\": 3,\n        \"name\": \"conversationalRetrievalQAChain\",\n        \"type\": \"ConversationalRetrievalQAChain\",\n        \"baseClasses\": [\n          \"ConversationalRetrievalQAChain\",\n          \"BaseChain\",\n          \"Runnable\"\n        ],\n        \"category\": \"Chains\",\n        \"description\": \"Document QA - built on RetrievalQAChain to provide a chat history component\",\n        \"inputParams\": [\n          {\n            \"label\": \"Return Source Documents\",\n            \"name\": \"returnSourceDocuments\",\n            \"type\": \"boolean\",\n            \"optional\": true,\n            \"id\": \"conversationalRetrievalQAChain_0-input-returnSourceDocuments-boolean\"\n          },\n          {\n            \"label\": \"Rephrase Prompt\",\n            \"name\": \"rephrasePrompt\",\n            \"type\": \"string\",\n            \"description\": \"Using previous chat history, rephrase question into a standalone question\",\n            \"warning\": \"Prompt must include input variables: {chat_history} and {question}\",\n            \"rows\": 4,\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"default\": \"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone Question:\",\n            \"id\": \"conversationalRetrievalQAChain_0-input-rephrasePrompt-string\"\n          },\n          {\n            \"label\": \"Response Prompt\",\n            \"name\": \"responsePrompt\",\n            \"type\": \"string\",\n            \"description\": \"Taking the rephrased question, search for answer from the provided context\",\n            \"warning\": \"Prompt must include input variable: {context}\",\n            \"rows\": 4,\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"default\": \"You are a helpful assistant. Using the provided context, answer the user's question to the best of your ability using the resources provided.\\nIf there is nothing in the context relevant to the question at hand, just say \\\"Hmm, I'm not sure.\\\" Don't try to make up an answer.\\n------------\\n{context}\\n------------\\nREMEMBER: If there is no relevant information within the context, just say \\\"Hmm, I'm not sure.\\\" Don't try to make up an answer.\",\n            \"id\": \"conversationalRetrievalQAChain_0-input-responsePrompt-string\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Chat Model\",\n            \"name\": \"model\",\n            \"type\": \"BaseChatModel\",\n            \"id\": \"conversationalRetrievalQAChain_0-input-model-BaseChatModel\"\n          },\n          {\n            \"label\": \"Vector Store Retriever\",\n            \"name\": \"vectorStoreRetriever\",\n            \"type\": \"BaseRetriever\",\n            \"id\": \"conversationalRetrievalQAChain_0-input-vectorStoreRetriever-BaseRetriever\"\n          },\n          {\n            \"label\": \"Memory\",\n            \"name\": \"memory\",\n            \"type\": \"BaseMemory\",\n            \"optional\": true,\n            \"description\": \"If left empty, a default BufferMemory will be used\",\n            \"id\": \"conversationalRetrievalQAChain_0-input-memory-BaseMemory\"\n          },\n          {\n            \"label\": \"Input Moderation\",\n            \"description\": \"Detect text that could generate harmful output and prevent it from being sent to the language model\",\n            \"name\": \"inputModeration\",\n            \"type\": \"Moderation\",\n            \"optional\": true,\n            \"list\": true,\n            \"id\": \"conversationalRetrievalQAChain_0-input-inputModeration-Moderation\"\n          }\n        ],\n        \"inputs\": {\n          \"inputModeration\": \"\",\n          \"model\": \"{{chatOpenAI_0.data.instance}}\",\n          \"vectorStoreRetriever\": \"{{pinecone_0.data.instance}}\",\n          \"memory\": \"\",\n          \"returnSourceDocuments\": true,\n          \"rephrasePrompt\": \"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone Question:\",\n          \"responsePrompt\": \"You are a helpful assistant. Using the provided context, answer the user's question to the best of your ability using the resources provided.\\nIf there is nothing in the context relevant to the question at hand, just say \\\"Hmm, I'm not sure.\\\" Don't try to make up an answer.\\n------------\\n{context}\\n------------\\nREMEMBER: If there is no relevant information within the context, just say \\\"Hmm, I'm not sure.\\\" Don't try to make up an answer.\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"conversationalRetrievalQAChain_0-output-conversationalRetrievalQAChain-ConversationalRetrievalQAChain|BaseChain|Runnable\",\n            \"name\": \"conversationalRetrievalQAChain\",\n            \"label\": \"ConversationalRetrievalQAChain\",\n            \"type\": \"ConversationalRetrievalQAChain | BaseChain | Runnable\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 1892.82894546983,\n        \"y\": 282.2572649522094\n      },\n      \"dragging\": false\n    },\n    {\n      \"width\": 300,\n      \"height\": 669,\n      \"id\": \"chatOpenAI_0\",\n      \"position\": {\n        \"x\": 1532.4907022314349,\n        \"y\": -270.38662863532466\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"chatOpenAI_0\",\n        \"label\": \"ChatOpenAI\",\n        \"version\": 6,\n        \"name\": \"chatOpenAI\",\n        \"type\": \"ChatOpenAI\",\n        \"baseClasses\": [\n          \"ChatOpenAI\",\n          \"BaseChatModel\",\n          \"BaseLanguageModel\",\n          \"Runnable\"\n        ],\n        \"category\": \"Chat Models\",\n        \"description\": \"Wrapper around OpenAI large language models that use the Chat endpoint\",\n        \"inputParams\": [\n          {\n            \"label\": \"Connect Credential\",\n            \"name\": \"credential\",\n            \"type\": \"credential\",\n            \"credentialNames\": [\n              \"openAIApi\"\n            ],\n            \"id\": \"chatOpenAI_0-input-credential-credential\"\n          },\n          {\n            \"label\": \"Model Name\",\n            \"name\": \"modelName\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"default\": \"gpt-3.5-turbo\",\n            \"id\": \"chatOpenAI_0-input-modelName-options\"\n          },\n          {\n            \"label\": \"Temperature\",\n            \"name\": \"temperature\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"default\": 0.9,\n            \"optional\": true,\n            \"id\": \"chatOpenAI_0-input-temperature-number\"\n          },\n          {\n            \"label\": \"Max Tokens\",\n            \"name\": \"maxTokens\",\n            \"type\": \"number\",\n            \"step\": 1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_0-input-maxTokens-number\"\n          },\n          {\n            \"label\": \"Top Probability\",\n            \"name\": \"topP\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_0-input-topP-number\"\n          },\n          {\n            \"label\": \"Frequency Penalty\",\n            \"name\": \"frequencyPenalty\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_0-input-frequencyPenalty-number\"\n          },\n          {\n            \"label\": \"Presence Penalty\",\n            \"name\": \"presencePenalty\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_0-input-presencePenalty-number\"\n          },\n          {\n            \"label\": \"Timeout\",\n            \"name\": \"timeout\",\n            \"type\": \"number\",\n            \"step\": 1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_0-input-timeout-number\"\n          },\n          {\n            \"label\": \"BasePath\",\n            \"name\": \"basepath\",\n            \"type\": \"string\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_0-input-basepath-string\"\n          },\n          {\n            \"label\": \"BaseOptions\",\n            \"name\": \"baseOptions\",\n            \"type\": \"json\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_0-input-baseOptions-json\"\n          },\n          {\n            \"label\": \"Allow Image Uploads\",\n            \"name\": \"allowImageUploads\",\n            \"type\": \"boolean\",\n            \"description\": \"Automatically uses gpt-4-vision-preview when image is being uploaded from chat. Only works with LLMChain, Conversation Chain, ReAct Agent, and Conversational Agent\",\n            \"default\": false,\n            \"optional\": true,\n            \"id\": \"chatOpenAI_0-input-allowImageUploads-boolean\"\n          },\n          {\n            \"label\": \"Image Resolution\",\n            \"description\": \"This parameter controls the resolution in which the model views the image.\",\n            \"name\": \"imageResolution\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"Low\",\n                \"name\": \"low\"\n              },\n              {\n                \"label\": \"High\",\n                \"name\": \"high\"\n              },\n              {\n                \"label\": \"Auto\",\n                \"name\": \"auto\"\n              }\n            ],\n            \"default\": \"low\",\n            \"optional\": false,\n            \"additionalParams\": true,\n            \"id\": \"chatOpenAI_0-input-imageResolution-options\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Cache\",\n            \"name\": \"cache\",\n            \"type\": \"BaseCache\",\n            \"optional\": true,\n            \"id\": \"chatOpenAI_0-input-cache-BaseCache\"\n          }\n        ],\n        \"inputs\": {\n          \"cache\": \"\",\n          \"modelName\": \"gpt-3.5-turbo\",\n          \"temperature\": \"0.7\",\n          \"maxTokens\": \"\",\n          \"topP\": \"\",\n          \"frequencyPenalty\": \"\",\n          \"presencePenalty\": \"\",\n          \"timeout\": \"\",\n          \"basepath\": \"\",\n          \"baseOptions\": \"\",\n          \"allowImageUploads\": true,\n          \"imageResolution\": \"low\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"chatOpenAI_0-output-chatOpenAI-ChatOpenAI|BaseChatModel|BaseLanguageModel|Runnable\",\n            \"name\": \"chatOpenAI\",\n            \"label\": \"ChatOpenAI\",\n            \"type\": \"ChatOpenAI | BaseChatModel | BaseLanguageModel | Runnable\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 1532.4907022314349,\n        \"y\": -270.38662863532466\n      },\n      \"dragging\": false\n    },\n    {\n      \"width\": 300,\n      \"height\": 553,\n      \"id\": \"pinecone_0\",\n      \"position\": {\n        \"x\": 1200.2048805168179,\n        \"y\": 158.19351393379816\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"pinecone_0\",\n        \"label\": \"Pinecone\",\n        \"version\": 3,\n        \"name\": \"pinecone\",\n        \"type\": \"Pinecone\",\n        \"baseClasses\": [\n          \"Pinecone\",\n          \"VectorStoreRetriever\",\n          \"BaseRetriever\"\n        ],\n        \"category\": \"Vector Stores\",\n        \"description\": \"Upsert embedded data and perform similarity or mmr search using Pinecone, a leading fully managed hosted vector database\",\n        \"inputParams\": [\n          {\n            \"label\": \"Connect Credential\",\n            \"name\": \"credential\",\n            \"type\": \"credential\",\n            \"credentialNames\": [\n              \"pineconeApi\"\n            ],\n            \"id\": \"pinecone_0-input-credential-credential\"\n          },\n          {\n            \"label\": \"Pinecone Index\",\n            \"name\": \"pineconeIndex\",\n            \"type\": \"string\",\n            \"id\": \"pinecone_0-input-pineconeIndex-string\"\n          },\n          {\n            \"label\": \"Pinecone Namespace\",\n            \"name\": \"pineconeNamespace\",\n            \"type\": \"string\",\n            \"placeholder\": \"my-first-namespace\",\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"id\": \"pinecone_0-input-pineconeNamespace-string\"\n          },\n          {\n            \"label\": \"Pinecone Metadata Filter\",\n            \"name\": \"pineconeMetadataFilter\",\n            \"type\": \"json\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"pinecone_0-input-pineconeMetadataFilter-json\"\n          },\n          {\n            \"label\": \"Top K\",\n            \"name\": \"topK\",\n            \"description\": \"Number of top results to fetch. Default to 4\",\n            \"placeholder\": \"4\",\n            \"type\": \"number\",\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"id\": \"pinecone_0-input-topK-number\"\n          },\n          {\n            \"label\": \"Search Type\",\n            \"name\": \"searchType\",\n            \"type\": \"options\",\n            \"default\": \"similarity\",\n            \"options\": [\n              {\n                \"label\": \"Similarity\",\n                \"name\": \"similarity\"\n              },\n              {\n                \"label\": \"Max Marginal Relevance\",\n                \"name\": \"mmr\"\n              }\n            ],\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"id\": \"pinecone_0-input-searchType-options\"\n          },\n          {\n            \"label\": \"Fetch K (for MMR Search)\",\n            \"name\": \"fetchK\",\n            \"description\": \"Number of initial documents to fetch for MMR reranking. Default to 20. Used only when the search type is MMR\",\n            \"placeholder\": \"20\",\n            \"type\": \"number\",\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"id\": \"pinecone_0-input-fetchK-number\"\n          },\n          {\n            \"label\": \"Lambda (for MMR Search)\",\n            \"name\": \"lambda\",\n            \"description\": \"Number between 0 and 1 that determines the degree of diversity among the results, where 0 corresponds to maximum diversity and 1 to minimum diversity. Used only when the search type is MMR\",\n            \"placeholder\": \"0.5\",\n            \"type\": \"number\",\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"id\": \"pinecone_0-input-lambda-number\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Document\",\n            \"name\": \"document\",\n            \"type\": \"Document\",\n            \"list\": true,\n            \"optional\": true,\n            \"id\": \"pinecone_0-input-document-Document\"\n          },\n          {\n            \"label\": \"Embeddings\",\n            \"name\": \"embeddings\",\n            \"type\": \"Embeddings\",\n            \"id\": \"pinecone_0-input-embeddings-Embeddings\"\n          }\n        ],\n        \"inputs\": {\n          \"document\": [\n            \"{{cheerioWebScraper_0.data.instance}}\"\n          ],\n          \"embeddings\": \"{{openAIEmbeddings_0.data.instance}}\",\n          \"pineconeIndex\": \"webscraper\",\n          \"pineconeNamespace\": \"\",\n          \"pineconeMetadataFilter\": \"\",\n          \"topK\": \"\",\n          \"searchType\": \"similarity\",\n          \"fetchK\": \"\",\n          \"lambda\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"name\": \"output\",\n            \"label\": \"Output\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"id\": \"pinecone_0-output-retriever-Pinecone|VectorStoreRetriever|BaseRetriever\",\n                \"name\": \"retriever\",\n                \"label\": \"Pinecone Retriever\",\n                \"type\": \"Pinecone | VectorStoreRetriever | BaseRetriever\"\n              },\n              {\n                \"id\": \"pinecone_0-output-vectorStore-Pinecone|VectorStore\",\n                \"name\": \"vectorStore\",\n                \"label\": \"Pinecone Vector Store\",\n                \"type\": \"Pinecone | VectorStore\"\n              }\n            ],\n            \"default\": \"retriever\"\n          }\n        ],\n        \"outputs\": {\n          \"output\": \"retriever\"\n        },\n        \"selected\": false\n      },\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 1200.2048805168179,\n        \"y\": 158.19351393379816\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"cheerioWebScraper_0\",\n      \"position\": {\n        \"x\": 757.9199325985811,\n        \"y\": -210.07668781794746\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"cheerioWebScraper_0\",\n        \"label\": \"Cheerio Web Scraper\",\n        \"version\": 2,\n        \"name\": \"cheerioWebScraper\",\n        \"type\": \"Document\",\n        \"baseClasses\": [\n          \"Document\"\n        ],\n        \"category\": \"Document Loaders\",\n        \"description\": \"Load data from webpages\",\n        \"inputParams\": [\n          {\n            \"label\": \"URL\",\n            \"name\": \"url\",\n            \"type\": \"string\",\n            \"id\": \"cheerioWebScraper_0-input-url-string\"\n          },\n          {\n            \"label\": \"Get Relative Links Method\",\n            \"name\": \"relativeLinksMethod\",\n            \"type\": \"options\",\n            \"description\": \"Select a method to retrieve relative links\",\n            \"options\": [\n              {\n                \"label\": \"Web Crawl\",\n                \"name\": \"webCrawl\",\n                \"description\": \"Crawl relative links from HTML URL\"\n              },\n              {\n                \"label\": \"Scrape XML Sitemap\",\n                \"name\": \"scrapeXMLSitemap\",\n                \"description\": \"Scrape relative links from XML sitemap URL\"\n              }\n            ],\n            \"default\": \"webCrawl\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"cheerioWebScraper_0-input-relativeLinksMethod-options\"\n          },\n          {\n            \"label\": \"Get Relative Links Limit\",\n            \"name\": \"limit\",\n            \"type\": \"number\",\n            \"optional\": true,\n            \"default\": \"10\",\n            \"additionalParams\": true,\n            \"description\": \"Only used when \\\"Get Relative Links Method\\\" is selected. Set 0 to retrieve all relative links, default limit is 10.\",\n            \"warning\": \"Retrieving all links might take long time, and all links will be upserted again if the flow's state changed (eg: different URL, chunk size, etc)\",\n            \"id\": \"cheerioWebScraper_0-input-limit-number\"\n          },\n          {\n            \"label\": \"Selector (CSS)\",\n            \"name\": \"selector\",\n            \"type\": \"string\",\n            \"description\": \"Specify a CSS selector to select the content to be extracted\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"cheerioWebScraper_0-input-selector-string\"\n          },\n          {\n            \"label\": \"Additional Metadata\",\n            \"name\": \"metadata\",\n            \"type\": \"json\",\n            \"description\": \"Additional metadata to be added to the extracted documents\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"cheerioWebScraper_0-input-metadata-json\"\n          },\n          {\n            \"label\": \"Omit Metadata Keys\",\n            \"name\": \"omitMetadataKeys\",\n            \"type\": \"string\",\n            \"rows\": 4,\n            \"description\": \"Each document loader comes with a default set of metadata keys that are extracted from the document. You can use this field to omit some of the default metadata keys. The value should be a list of keys, seperated by comma. Use * to omit all metadata keys execept the ones you specify in the Additional Metadata field\",\n            \"placeholder\": \"key1, key2, key3.nestedKey1\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"cheerioWebScraper_0-input-omitMetadataKeys-string\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Text Splitter\",\n            \"name\": \"textSplitter\",\n            \"type\": \"TextSplitter\",\n            \"optional\": true,\n            \"id\": \"cheerioWebScraper_0-input-textSplitter-TextSplitter\"\n          }\n        ],\n        \"inputs\": {\n          \"url\": \"https://vpt.shipping.gov.in\",\n          \"textSplitter\": \"{{htmlToMarkdownTextSplitter_0.data.instance}}\",\n          \"relativeLinksMethod\": \"webCrawl\",\n          \"limit\": \"10\",\n          \"selector\": \"\",\n          \"metadata\": \"\",\n          \"omitMetadataKeys\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"name\": \"output\",\n            \"label\": \"Output\",\n            \"type\": \"options\",\n            \"description\": \"Array of document objects containing metadata and pageContent\",\n            \"options\": [\n              {\n                \"id\": \"cheerioWebScraper_0-output-document-Document|json\",\n                \"name\": \"document\",\n                \"label\": \"Document\",\n                \"description\": \"Array of document objects containing metadata and pageContent\",\n                \"type\": \"Document | json\"\n              },\n              {\n                \"id\": \"cheerioWebScraper_0-output-text-string|json\",\n                \"name\": \"text\",\n                \"label\": \"Text\",\n                \"description\": \"Concatenated string from pageContent of documents\",\n                \"type\": \"string | json\"\n              }\n            ],\n            \"default\": \"document\"\n          }\n        ],\n        \"outputs\": {\n          \"output\": \"document\"\n        },\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 451,\n      \"selected\": false,\n      \"dragging\": false,\n      \"positionAbsolute\": {\n        \"x\": 757.9199325985811,\n        \"y\": -210.07668781794746\n      }\n    }\n  ],\n  \"edges\": [\n    {\n      \"source\": \"chatOpenAI_0\",\n      \"sourceHandle\": \"chatOpenAI_0-output-chatOpenAI-ChatOpenAI|BaseChatModel|BaseLanguageModel|Runnable\",\n      \"target\": \"conversationalRetrievalQAChain_0\",\n      \"targetHandle\": \"conversationalRetrievalQAChain_0-input-model-BaseChatModel\",\n      \"type\": \"buttonedge\",\n      \"id\": \"chatOpenAI_0-chatOpenAI_0-output-chatOpenAI-ChatOpenAI|BaseChatModel|BaseLanguageModel|Runnable-conversationalRetrievalQAChain_0-conversationalRetrievalQAChain_0-input-model-BaseChatModel\",\n      \"data\": {\n        \"label\": \"\"\n      }\n    },\n    {\n      \"source\": \"openAIEmbeddings_0\",\n      \"sourceHandle\": \"openAIEmbeddings_0-output-openAIEmbeddings-OpenAIEmbeddings|Embeddings\",\n      \"target\": \"pinecone_0\",\n      \"targetHandle\": \"pinecone_0-input-embeddings-Embeddings\",\n      \"type\": \"buttonedge\",\n      \"id\": \"openAIEmbeddings_0-openAIEmbeddings_0-output-openAIEmbeddings-OpenAIEmbeddings|Embeddings-pinecone_0-pinecone_0-input-embeddings-Embeddings\",\n      \"data\": {\n        \"label\": \"\"\n      }\n    },\n    {\n      \"source\": \"pinecone_0\",\n      \"sourceHandle\": \"pinecone_0-output-retriever-Pinecone|VectorStoreRetriever|BaseRetriever\",\n      \"target\": \"conversationalRetrievalQAChain_0\",\n      \"targetHandle\": \"conversationalRetrievalQAChain_0-input-vectorStoreRetriever-BaseRetriever\",\n      \"type\": \"buttonedge\",\n      \"id\": \"pinecone_0-pinecone_0-output-retriever-Pinecone|VectorStoreRetriever|BaseRetriever-conversationalRetrievalQAChain_0-conversationalRetrievalQAChain_0-input-vectorStoreRetriever-BaseRetriever\",\n      \"data\": {\n        \"label\": \"\"\n      }\n    },\n    {\n      \"source\": \"cheerioWebScraper_0\",\n      \"sourceHandle\": \"cheerioWebScraper_0-output-document-Document|json\",\n      \"target\": \"pinecone_0\",\n      \"targetHandle\": \"pinecone_0-input-document-Document\",\n      \"type\": \"buttonedge\",\n      \"id\": \"cheerioWebScraper_0-cheerioWebScraper_0-output-document-Document|json-pinecone_0-pinecone_0-input-document-Document\"\n    },\n    {\n      \"source\": \"htmlToMarkdownTextSplitter_0\",\n      \"sourceHandle\": \"htmlToMarkdownTextSplitter_0-output-htmlToMarkdownTextSplitter-HtmlToMarkdownTextSplitter|MarkdownTextSplitter|RecursiveCharacterTextSplitter|TextSplitter|BaseDocumentTransformer\",\n      \"target\": \"cheerioWebScraper_0\",\n      \"targetHandle\": \"cheerioWebScraper_0-input-textSplitter-TextSplitter\",\n      \"type\": \"buttonedge\",\n      \"id\": \"htmlToMarkdownTextSplitter_0-htmlToMarkdownTextSplitter_0-output-htmlToMarkdownTextSplitter-HtmlToMarkdownTextSplitter|MarkdownTextSplitter|RecursiveCharacterTextSplitter|TextSplitter|BaseDocumentTransformer-cheerioWebScraper_0-cheerioWebScraper_0-input-textSplitter-TextSplitter\"\n    }\n  ]\n}",
      "type": "CHATFLOW"
    },
    {
      "id": "fa3ab1bc-358d-4c65-91d7-d58431735d9e",
      "name": "Deepseek",
      "flowData": "{\n  \"nodes\": [\n    {\n      \"id\": \"chatDeepseek_0\",\n      \"position\": {\n        \"x\": 635.1536362284911,\n        \"y\": -805.4736139730819\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"chatDeepseek_0\",\n        \"label\": \"ChatDeepseek\",\n        \"version\": 1,\n        \"name\": \"chatDeepseek\",\n        \"type\": \"chatDeepseek\",\n        \"baseClasses\": [\n          \"chatDeepseek\",\n          \"BaseChatModel\",\n          \"BaseLanguageModel\",\n          \"Runnable\"\n        ],\n        \"category\": \"Chat Models\",\n        \"description\": \"Wrapper around Deepseek large language models that use the Chat endpoint\",\n        \"inputParams\": [\n          {\n            \"label\": \"Connect Credential\",\n            \"name\": \"credential\",\n            \"type\": \"credential\",\n            \"credentialNames\": [\n              \"deepseekApi\"\n            ],\n            \"id\": \"chatDeepseek_0-input-credential-credential\"\n          },\n          {\n            \"label\": \"Model Name\",\n            \"name\": \"modelName\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"default\": \"deepseek-chat\",\n            \"id\": \"chatDeepseek_0-input-modelName-asyncOptions\"\n          },\n          {\n            \"label\": \"Temperature\",\n            \"name\": \"temperature\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"default\": 0.7,\n            \"optional\": true,\n            \"id\": \"chatDeepseek_0-input-temperature-number\"\n          },\n          {\n            \"label\": \"Streaming\",\n            \"name\": \"streaming\",\n            \"type\": \"boolean\",\n            \"default\": true,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatDeepseek_0-input-streaming-boolean\"\n          },\n          {\n            \"label\": \"Max Tokens\",\n            \"name\": \"maxTokens\",\n            \"type\": \"number\",\n            \"step\": 1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatDeepseek_0-input-maxTokens-number\"\n          },\n          {\n            \"label\": \"Top Probability\",\n            \"name\": \"topP\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatDeepseek_0-input-topP-number\"\n          },\n          {\n            \"label\": \"Frequency Penalty\",\n            \"name\": \"frequencyPenalty\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatDeepseek_0-input-frequencyPenalty-number\"\n          },\n          {\n            \"label\": \"Presence Penalty\",\n            \"name\": \"presencePenalty\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatDeepseek_0-input-presencePenalty-number\"\n          },\n          {\n            \"label\": \"Timeout\",\n            \"name\": \"timeout\",\n            \"type\": \"number\",\n            \"step\": 1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatDeepseek_0-input-timeout-number\"\n          },\n          {\n            \"label\": \"Stop Sequence\",\n            \"name\": \"stopSequence\",\n            \"type\": \"string\",\n            \"rows\": 4,\n            \"optional\": true,\n            \"description\": \"List of stop words to use when generating. Use comma to separate multiple stop words.\",\n            \"additionalParams\": true,\n            \"id\": \"chatDeepseek_0-input-stopSequence-string\"\n          },\n          {\n            \"label\": \"Base Options\",\n            \"name\": \"baseOptions\",\n            \"type\": \"json\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"description\": \"Additional options to pass to the Deepseek client. This should be a JSON object.\",\n            \"id\": \"chatDeepseek_0-input-baseOptions-json\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Cache\",\n            \"name\": \"cache\",\n            \"type\": \"BaseCache\",\n            \"optional\": true,\n            \"id\": \"chatDeepseek_0-input-cache-BaseCache\"\n          }\n        ],\n        \"inputs\": {\n          \"cache\": \"\",\n          \"modelName\": \"deepseek-reasoner\",\n          \"temperature\": \"0.7\",\n          \"streaming\": true,\n          \"maxTokens\": \"\",\n          \"topP\": \"\",\n          \"frequencyPenalty\": \"\",\n          \"presencePenalty\": \"\",\n          \"timeout\": \"\",\n          \"stopSequence\": \"\",\n          \"baseOptions\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"chatDeepseek_0-output-chatDeepseek-chatDeepseek|BaseChatModel|BaseLanguageModel|Runnable\",\n            \"name\": \"chatDeepseek\",\n            \"label\": \"chatDeepseek\",\n            \"description\": \"Wrapper around Deepseek large language models that use the Chat endpoint\",\n            \"type\": \"chatDeepseek | BaseChatModel | BaseLanguageModel | Runnable\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 573,\n      \"selected\": false,\n      \"dragging\": false,\n      \"positionAbsolute\": {\n        \"x\": 635.1536362284911,\n        \"y\": -805.4736139730819\n      }\n    },\n    {\n      \"id\": \"bufferMemory_0\",\n      \"position\": {\n        \"x\": 642.9970770009231,\n        \"y\": -217.11038703936197\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"bufferMemory_0\",\n        \"label\": \"Buffer Memory\",\n        \"version\": 2,\n        \"name\": \"bufferMemory\",\n        \"type\": \"BufferMemory\",\n        \"baseClasses\": [\n          \"BufferMemory\",\n          \"BaseChatMemory\",\n          \"BaseMemory\"\n        ],\n        \"category\": \"Memory\",\n        \"description\": \"Retrieve chat messages stored in database\",\n        \"inputParams\": [\n          {\n            \"label\": \"Session Id\",\n            \"name\": \"sessionId\",\n            \"type\": \"string\",\n            \"description\": \"If not specified, a random id will be used. Learn <a target=\\\"_blank\\\" href=\\\"https://docs.flowiseai.com/memory#ui-and-embedded-chat\\\">more</a>\",\n            \"default\": \"\",\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"id\": \"bufferMemory_0-input-sessionId-string\"\n          },\n          {\n            \"label\": \"Memory Key\",\n            \"name\": \"memoryKey\",\n            \"type\": \"string\",\n            \"default\": \"chat_history\",\n            \"additionalParams\": true,\n            \"id\": \"bufferMemory_0-input-memoryKey-string\"\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"sessionId\": \"\",\n          \"memoryKey\": \"chat_history\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"bufferMemory_0-output-bufferMemory-BufferMemory|BaseChatMemory|BaseMemory\",\n            \"name\": \"bufferMemory\",\n            \"label\": \"BufferMemory\",\n            \"description\": \"Retrieve chat messages stored in database\",\n            \"type\": \"BufferMemory | BaseChatMemory | BaseMemory\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 252,\n      \"positionAbsolute\": {\n        \"x\": 642.9970770009231,\n        \"y\": -217.11038703936197\n      },\n      \"selected\": false,\n      \"dragging\": false\n    },\n    {\n      \"id\": \"conversationChain_0\",\n      \"position\": {\n        \"x\": 1122.3956322343704,\n        \"y\": -564.150533530099\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"conversationChain_0\",\n        \"label\": \"Conversation Chain\",\n        \"version\": 3,\n        \"name\": \"conversationChain\",\n        \"type\": \"ConversationChain\",\n        \"baseClasses\": [\n          \"ConversationChain\",\n          \"LLMChain\",\n          \"BaseChain\",\n          \"Runnable\"\n        ],\n        \"category\": \"Chains\",\n        \"description\": \"Chat models specific conversational chain with memory\",\n        \"inputParams\": [\n          {\n            \"label\": \"System Message\",\n            \"name\": \"systemMessagePrompt\",\n            \"type\": \"string\",\n            \"rows\": 4,\n            \"description\": \"If Chat Prompt Template is provided, this will be ignored\",\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"default\": \"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\",\n            \"placeholder\": \"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\",\n            \"id\": \"conversationChain_0-input-systemMessagePrompt-string\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Chat Model\",\n            \"name\": \"model\",\n            \"type\": \"BaseChatModel\",\n            \"id\": \"conversationChain_0-input-model-BaseChatModel\"\n          },\n          {\n            \"label\": \"Memory\",\n            \"name\": \"memory\",\n            \"type\": \"BaseMemory\",\n            \"id\": \"conversationChain_0-input-memory-BaseMemory\"\n          },\n          {\n            \"label\": \"Chat Prompt Template\",\n            \"name\": \"chatPromptTemplate\",\n            \"type\": \"ChatPromptTemplate\",\n            \"description\": \"Override existing prompt with Chat Prompt Template. Human Message must includes {input} variable\",\n            \"optional\": true,\n            \"id\": \"conversationChain_0-input-chatPromptTemplate-ChatPromptTemplate\"\n          },\n          {\n            \"label\": \"Input Moderation\",\n            \"description\": \"Detect text that could generate harmful output and prevent it from being sent to the language model\",\n            \"name\": \"inputModeration\",\n            \"type\": \"Moderation\",\n            \"optional\": true,\n            \"list\": true,\n            \"id\": \"conversationChain_0-input-inputModeration-Moderation\"\n          }\n        ],\n        \"inputs\": {\n          \"model\": \"{{chatDeepseek_0.data.instance}}\",\n          \"memory\": \"{{bufferMemory_0.data.instance}}\",\n          \"chatPromptTemplate\": \"\",\n          \"inputModeration\": \"\",\n          \"systemMessagePrompt\": \"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"conversationChain_0-output-conversationChain-ConversationChain|LLMChain|BaseChain|Runnable\",\n            \"name\": \"conversationChain\",\n            \"label\": \"ConversationChain\",\n            \"description\": \"Chat models specific conversational chain with memory\",\n            \"type\": \"ConversationChain | LLMChain | BaseChain | Runnable\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 434,\n      \"positionAbsolute\": {\n        \"x\": 1122.3956322343704,\n        \"y\": -564.150533530099\n      },\n      \"selected\": false,\n      \"dragging\": false\n    }\n  ],\n  \"edges\": [\n    {\n      \"source\": \"chatDeepseek_0\",\n      \"sourceHandle\": \"chatDeepseek_0-output-chatDeepseek-chatDeepseek|BaseChatModel|BaseLanguageModel|Runnable\",\n      \"target\": \"conversationChain_0\",\n      \"targetHandle\": \"conversationChain_0-input-model-BaseChatModel\",\n      \"type\": \"buttonedge\",\n      \"id\": \"chatDeepseek_0-chatDeepseek_0-output-chatDeepseek-chatDeepseek|BaseChatModel|BaseLanguageModel|Runnable-conversationChain_0-conversationChain_0-input-model-BaseChatModel\"\n    },\n    {\n      \"source\": \"bufferMemory_0\",\n      \"sourceHandle\": \"bufferMemory_0-output-bufferMemory-BufferMemory|BaseChatMemory|BaseMemory\",\n      \"target\": \"conversationChain_0\",\n      \"targetHandle\": \"conversationChain_0-input-memory-BaseMemory\",\n      \"type\": \"buttonedge\",\n      \"id\": \"bufferMemory_0-bufferMemory_0-output-bufferMemory-BufferMemory|BaseChatMemory|BaseMemory-conversationChain_0-conversationChain_0-input-memory-BaseMemory\"\n    }\n  ]\n}",
      "type": "CHATFLOW"
    },
    {
      "id": "af3943d9-899b-427d-a754-5ef6f54580ee",
      "name": "websiteq/a",
      "flowData": "{\n  \"nodes\": [\n    {\n      \"id\": \"conversationalRetrievalQAChain_0\",\n      \"position\": {\n        \"x\": 1586.2583595564618,\n        \"y\": -346.391589748445\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"conversationalRetrievalQAChain_0\",\n        \"label\": \"Conversational Retrieval QA Chain\",\n        \"version\": 3,\n        \"name\": \"conversationalRetrievalQAChain\",\n        \"type\": \"ConversationalRetrievalQAChain\",\n        \"baseClasses\": [\n          \"ConversationalRetrievalQAChain\",\n          \"BaseChain\",\n          \"Runnable\"\n        ],\n        \"category\": \"Chains\",\n        \"description\": \"Document QA - built on RetrievalQAChain to provide a chat history component\",\n        \"inputParams\": [\n          {\n            \"label\": \"Return Source Documents\",\n            \"name\": \"returnSourceDocuments\",\n            \"type\": \"boolean\",\n            \"optional\": true,\n            \"id\": \"conversationalRetrievalQAChain_0-input-returnSourceDocuments-boolean\"\n          },\n          {\n            \"label\": \"Rephrase Prompt\",\n            \"name\": \"rephrasePrompt\",\n            \"type\": \"string\",\n            \"description\": \"Using previous chat history, rephrase question into a standalone question\",\n            \"warning\": \"Prompt must include input variables: {chat_history} and {question}\",\n            \"rows\": 4,\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"default\": \"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone Question:\",\n            \"id\": \"conversationalRetrievalQAChain_0-input-rephrasePrompt-string\"\n          },\n          {\n            \"label\": \"Response Prompt\",\n            \"name\": \"responsePrompt\",\n            \"type\": \"string\",\n            \"description\": \"Taking the rephrased question, search for answer from the provided context\",\n            \"warning\": \"Prompt must include input variable: {context}\",\n            \"rows\": 4,\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"default\": \"I want you to act as a document that I am having a conversation with. Your name is \\\"AI Assistant\\\". Using the provided context, answer the user's question to the best of your ability using the resources provided.\\nIf there is nothing in the context relevant to the question at hand, just say \\\"Hmm, I'm not sure\\\" and stop after that. Refuse to answer any question not about the info. Never break character.\\n------------\\n{context}\\n------------\\nREMEMBER: If there is no relevant information within the context, just say \\\"Hmm, I'm not sure\\\". Don't try to make up an answer. Never break character.\",\n            \"id\": \"conversationalRetrievalQAChain_0-input-responsePrompt-string\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Chat Model\",\n            \"name\": \"model\",\n            \"type\": \"BaseChatModel\",\n            \"id\": \"conversationalRetrievalQAChain_0-input-model-BaseChatModel\"\n          },\n          {\n            \"label\": \"Vector Store Retriever\",\n            \"name\": \"vectorStoreRetriever\",\n            \"type\": \"BaseRetriever\",\n            \"id\": \"conversationalRetrievalQAChain_0-input-vectorStoreRetriever-BaseRetriever\"\n          },\n          {\n            \"label\": \"Memory\",\n            \"name\": \"memory\",\n            \"type\": \"BaseMemory\",\n            \"optional\": true,\n            \"description\": \"If left empty, a default BufferMemory will be used\",\n            \"id\": \"conversationalRetrievalQAChain_0-input-memory-BaseMemory\"\n          },\n          {\n            \"label\": \"Input Moderation\",\n            \"description\": \"Detect text that could generate harmful output and prevent it from being sent to the language model\",\n            \"name\": \"inputModeration\",\n            \"type\": \"Moderation\",\n            \"optional\": true,\n            \"list\": true,\n            \"id\": \"conversationalRetrievalQAChain_0-input-inputModeration-Moderation\"\n          }\n        ],\n        \"inputs\": {\n          \"model\": \"{{chatGoogleGenerativeAI_0.data.instance}}\",\n          \"vectorStoreRetriever\": \"{{faiss_0.data.instance}}\",\n          \"memory\": \"{{bufferMemory_0.data.instance}}\",\n          \"returnSourceDocuments\": \"\",\n          \"rephrasePrompt\": \"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone Question:\",\n          \"responsePrompt\": \"I want you to act as a document that I am having a conversation with. Your name is \\\"AI Assistant\\\". Using the provided context, answer the user's question to the best of your ability using the resources provided.\\nIf there is nothing in the context relevant to the question at hand, just say \\\"Hmm, I'm not sure\\\" and stop after that. Refuse to answer any question not about the info. Never break character.\\n------------\\n{context}\\n------------\\nREMEMBER: If there is no relevant information within the context, just say \\\"Hmm, I'm not sure\\\". Don't try to make up an answer. Never break character.\",\n          \"inputModeration\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"conversationalRetrievalQAChain_0-output-conversationalRetrievalQAChain-ConversationalRetrievalQAChain|BaseChain|Runnable\",\n            \"name\": \"conversationalRetrievalQAChain\",\n            \"label\": \"ConversationalRetrievalQAChain\",\n            \"description\": \"Document QA - built on RetrievalQAChain to provide a chat history component\",\n            \"type\": \"ConversationalRetrievalQAChain | BaseChain | Runnable\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 531,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 1586.2583595564618,\n        \"y\": -346.391589748445\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"bufferMemory_0\",\n      \"position\": {\n        \"x\": 1113.1393471175927,\n        \"y\": 404.6709462523729\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"bufferMemory_0\",\n        \"label\": \"Buffer Memory\",\n        \"version\": 2,\n        \"name\": \"bufferMemory\",\n        \"type\": \"BufferMemory\",\n        \"baseClasses\": [\n          \"BufferMemory\",\n          \"BaseChatMemory\",\n          \"BaseMemory\"\n        ],\n        \"category\": \"Memory\",\n        \"description\": \"Retrieve chat messages stored in database\",\n        \"inputParams\": [\n          {\n            \"label\": \"Session Id\",\n            \"name\": \"sessionId\",\n            \"type\": \"string\",\n            \"description\": \"If not specified, a random id will be used. Learn <a target=\\\"_blank\\\" href=\\\"https://docs.flowiseai.com/memory#ui-and-embedded-chat\\\">more</a>\",\n            \"default\": \"\",\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"id\": \"bufferMemory_0-input-sessionId-string\"\n          },\n          {\n            \"label\": \"Memory Key\",\n            \"name\": \"memoryKey\",\n            \"type\": \"string\",\n            \"default\": \"chat_history\",\n            \"additionalParams\": true,\n            \"id\": \"bufferMemory_0-input-memoryKey-string\"\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"sessionId\": \"\",\n          \"memoryKey\": \"chat_history\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"bufferMemory_0-output-bufferMemory-BufferMemory|BaseChatMemory|BaseMemory\",\n            \"name\": \"bufferMemory\",\n            \"label\": \"BufferMemory\",\n            \"description\": \"Retrieve chat messages stored in database\",\n            \"type\": \"BufferMemory | BaseChatMemory | BaseMemory\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 252,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 1113.1393471175927,\n        \"y\": 404.6709462523729\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"faiss_0\",\n      \"position\": {\n        \"x\": 493.06111388483555,\n        \"y\": 184.5027818887146\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"faiss_0\",\n        \"label\": \"Faiss\",\n        \"version\": 1,\n        \"name\": \"faiss\",\n        \"type\": \"Faiss\",\n        \"baseClasses\": [\n          \"Faiss\",\n          \"VectorStoreRetriever\",\n          \"BaseRetriever\"\n        ],\n        \"category\": \"Vector Stores\",\n        \"description\": \"Upsert embedded data and perform similarity search upon query using Faiss library from Meta\",\n        \"inputParams\": [\n          {\n            \"label\": \"Base Path to load\",\n            \"name\": \"basePath\",\n            \"description\": \"Path to load faiss.index file\",\n            \"placeholder\": \"C:\\\\Users\\\\User\\\\Desktop\",\n            \"type\": \"string\",\n            \"id\": \"faiss_0-input-basePath-string\"\n          },\n          {\n            \"label\": \"Top K\",\n            \"name\": \"topK\",\n            \"description\": \"Number of top results to fetch. Default to 4\",\n            \"placeholder\": \"4\",\n            \"type\": \"number\",\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"id\": \"faiss_0-input-topK-number\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Document\",\n            \"name\": \"document\",\n            \"type\": \"Document\",\n            \"list\": true,\n            \"optional\": true,\n            \"id\": \"faiss_0-input-document-Document\"\n          },\n          {\n            \"label\": \"Embeddings\",\n            \"name\": \"embeddings\",\n            \"type\": \"Embeddings\",\n            \"id\": \"faiss_0-input-embeddings-Embeddings\"\n          }\n        ],\n        \"inputs\": {\n          \"document\": [\n            \"{{puppeteerWebScraper_0.data.instance}}\"\n          ],\n          \"embeddings\": \"{{googleGenerativeAiEmbeddings_0.data.instance}}\",\n          \"basePath\": \"/opt/render/.flowise/1177\",\n          \"topK\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"name\": \"output\",\n            \"label\": \"Output\",\n            \"type\": \"options\",\n            \"description\": \"\",\n            \"options\": [\n              {\n                \"id\": \"faiss_0-output-retriever-Faiss|VectorStoreRetriever|BaseRetriever\",\n                \"name\": \"retriever\",\n                \"label\": \"Faiss Retriever\",\n                \"description\": \"\",\n                \"type\": \"Faiss | VectorStoreRetriever | BaseRetriever\"\n              },\n              {\n                \"id\": \"faiss_0-output-vectorStore-Faiss|SaveableVectorStore|VectorStore\",\n                \"name\": \"vectorStore\",\n                \"label\": \"Faiss Vector Store\",\n                \"description\": \"\",\n                \"type\": \"Faiss | SaveableVectorStore | VectorStore\"\n              }\n            ],\n            \"default\": \"retriever\"\n          }\n        ],\n        \"outputs\": {\n          \"output\": \"retriever\"\n        },\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 458,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 493.06111388483555,\n        \"y\": 184.5027818887146\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"puppeteerWebScraper_0\",\n      \"position\": {\n        \"x\": 19.557258846108112,\n        \"y\": -487.01177865519253\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"puppeteerWebScraper_0\",\n        \"label\": \"Puppeteer Web Scraper\",\n        \"version\": 2,\n        \"name\": \"puppeteerWebScraper\",\n        \"type\": \"Document\",\n        \"baseClasses\": [\n          \"Document\"\n        ],\n        \"category\": \"Document Loaders\",\n        \"description\": \"Load data from webpages\",\n        \"inputParams\": [\n          {\n            \"label\": \"URL\",\n            \"name\": \"url\",\n            \"type\": \"string\",\n            \"id\": \"puppeteerWebScraper_0-input-url-string\"\n          },\n          {\n            \"label\": \"Get Relative Links Method\",\n            \"name\": \"relativeLinksMethod\",\n            \"type\": \"options\",\n            \"description\": \"Select a method to retrieve relative links\",\n            \"options\": [\n              {\n                \"label\": \"Web Crawl\",\n                \"name\": \"webCrawl\",\n                \"description\": \"Crawl relative links from HTML URL\"\n              },\n              {\n                \"label\": \"Scrape XML Sitemap\",\n                \"name\": \"scrapeXMLSitemap\",\n                \"description\": \"Scrape relative links from XML sitemap URL\"\n              }\n            ],\n            \"default\": \"webCrawl\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"puppeteerWebScraper_0-input-relativeLinksMethod-options\"\n          },\n          {\n            \"label\": \"Get Relative Links Limit\",\n            \"name\": \"limit\",\n            \"type\": \"number\",\n            \"optional\": true,\n            \"default\": \"10\",\n            \"additionalParams\": true,\n            \"description\": \"Only used when \\\"Get Relative Links Method\\\" is selected. Set 0 to retrieve all relative links, default limit is 10.\",\n            \"warning\": \"Retrieving all links might take long time, and all links will be upserted again if the flow's state changed (eg: different URL, chunk size, etc)\",\n            \"id\": \"puppeteerWebScraper_0-input-limit-number\"\n          },\n          {\n            \"label\": \"Wait Until\",\n            \"name\": \"waitUntilGoToOption\",\n            \"type\": \"options\",\n            \"description\": \"Select a go to wait until option\",\n            \"options\": [\n              {\n                \"label\": \"Load\",\n                \"name\": \"load\",\n                \"description\": \"When the initial HTML document's DOM has been loaded and parsed\"\n              },\n              {\n                \"label\": \"DOM Content Loaded\",\n                \"name\": \"domcontentloaded\",\n                \"description\": \"When the complete HTML document's DOM has been loaded and parsed\"\n              },\n              {\n                \"label\": \"Network Idle 0\",\n                \"name\": \"networkidle0\",\n                \"description\": \"Navigation is finished when there are no more than 0 network connections for at least 500 ms\"\n              },\n              {\n                \"label\": \"Network Idle 2\",\n                \"name\": \"networkidle2\",\n                \"description\": \"Navigation is finished when there are no more than 2 network connections for at least 500 ms\"\n              }\n            ],\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"puppeteerWebScraper_0-input-waitUntilGoToOption-options\"\n          },\n          {\n            \"label\": \"Wait for selector to load\",\n            \"name\": \"waitForSelector\",\n            \"type\": \"string\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"description\": \"CSS selectors like .div or #div\",\n            \"id\": \"puppeteerWebScraper_0-input-waitForSelector-string\"\n          },\n          {\n            \"label\": \"Additional Metadata\",\n            \"name\": \"metadata\",\n            \"type\": \"json\",\n            \"description\": \"Additional metadata to be added to the extracted documents\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"puppeteerWebScraper_0-input-metadata-json\"\n          },\n          {\n            \"label\": \"Omit Metadata Keys\",\n            \"name\": \"omitMetadataKeys\",\n            \"type\": \"string\",\n            \"rows\": 4,\n            \"description\": \"Each document loader comes with a default set of metadata keys that are extracted from the document. You can use this field to omit some of the default metadata keys. The value should be a list of keys, seperated by comma. Use * to omit all metadata keys execept the ones you specify in the Additional Metadata field\",\n            \"placeholder\": \"key1, key2, key3.nestedKey1\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"puppeteerWebScraper_0-input-omitMetadataKeys-string\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Text Splitter\",\n            \"name\": \"textSplitter\",\n            \"type\": \"TextSplitter\",\n            \"optional\": true,\n            \"id\": \"puppeteerWebScraper_0-input-textSplitter-TextSplitter\"\n          }\n        ],\n        \"inputs\": {\n          \"url\": \"https://balaswamyportfolio.netlify.app/\",\n          \"textSplitter\": \"{{htmlToMarkdownTextSplitter_0.data.instance}}\",\n          \"relativeLinksMethod\": \"webCrawl\",\n          \"limit\": \"10\",\n          \"waitUntilGoToOption\": \"\",\n          \"waitForSelector\": \"\",\n          \"metadata\": \"\",\n          \"omitMetadataKeys\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"name\": \"output\",\n            \"label\": \"Output\",\n            \"type\": \"options\",\n            \"description\": \"Array of document objects containing metadata and pageContent\",\n            \"options\": [\n              {\n                \"id\": \"puppeteerWebScraper_0-output-document-Document|json\",\n                \"name\": \"document\",\n                \"label\": \"Document\",\n                \"description\": \"Array of document objects containing metadata and pageContent\",\n                \"type\": \"Document | json\"\n              },\n              {\n                \"id\": \"puppeteerWebScraper_0-output-text-string|json\",\n                \"name\": \"text\",\n                \"label\": \"Text\",\n                \"description\": \"Concatenated string from pageContent of documents\",\n                \"type\": \"string | json\"\n              }\n            ],\n            \"default\": \"document\"\n          }\n        ],\n        \"outputs\": {\n          \"output\": \"document\"\n        },\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 452,\n      \"selected\": false,\n      \"dragging\": false,\n      \"positionAbsolute\": {\n        \"x\": 19.557258846108112,\n        \"y\": -487.01177865519253\n      }\n    },\n    {\n      \"id\": \"chatDeepseek_0\",\n      \"position\": {\n        \"x\": -745.8788204153421,\n        \"y\": 185.6569431675053\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"chatDeepseek_0\",\n        \"label\": \"ChatDeepseek\",\n        \"version\": 1,\n        \"name\": \"chatDeepseek\",\n        \"type\": \"chatDeepseek\",\n        \"baseClasses\": [\n          \"chatDeepseek\",\n          \"BaseChatModel\",\n          \"BaseLanguageModel\",\n          \"Runnable\"\n        ],\n        \"category\": \"Chat Models\",\n        \"description\": \"Wrapper around Deepseek large language models that use the Chat endpoint\",\n        \"inputParams\": [\n          {\n            \"label\": \"Connect Credential\",\n            \"name\": \"credential\",\n            \"type\": \"credential\",\n            \"credentialNames\": [\n              \"deepseekApi\"\n            ],\n            \"id\": \"chatDeepseek_0-input-credential-credential\"\n          },\n          {\n            \"label\": \"Model Name\",\n            \"name\": \"modelName\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"default\": \"deepseek-chat\",\n            \"id\": \"chatDeepseek_0-input-modelName-asyncOptions\"\n          },\n          {\n            \"label\": \"Temperature\",\n            \"name\": \"temperature\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"default\": 0.7,\n            \"optional\": true,\n            \"id\": \"chatDeepseek_0-input-temperature-number\"\n          },\n          {\n            \"label\": \"Streaming\",\n            \"name\": \"streaming\",\n            \"type\": \"boolean\",\n            \"default\": true,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatDeepseek_0-input-streaming-boolean\"\n          },\n          {\n            \"label\": \"Max Tokens\",\n            \"name\": \"maxTokens\",\n            \"type\": \"number\",\n            \"step\": 1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatDeepseek_0-input-maxTokens-number\"\n          },\n          {\n            \"label\": \"Top Probability\",\n            \"name\": \"topP\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatDeepseek_0-input-topP-number\"\n          },\n          {\n            \"label\": \"Frequency Penalty\",\n            \"name\": \"frequencyPenalty\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatDeepseek_0-input-frequencyPenalty-number\"\n          },\n          {\n            \"label\": \"Presence Penalty\",\n            \"name\": \"presencePenalty\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatDeepseek_0-input-presencePenalty-number\"\n          },\n          {\n            \"label\": \"Timeout\",\n            \"name\": \"timeout\",\n            \"type\": \"number\",\n            \"step\": 1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatDeepseek_0-input-timeout-number\"\n          },\n          {\n            \"label\": \"Stop Sequence\",\n            \"name\": \"stopSequence\",\n            \"type\": \"string\",\n            \"rows\": 4,\n            \"optional\": true,\n            \"description\": \"List of stop words to use when generating. Use comma to separate multiple stop words.\",\n            \"additionalParams\": true,\n            \"id\": \"chatDeepseek_0-input-stopSequence-string\"\n          },\n          {\n            \"label\": \"Base Options\",\n            \"name\": \"baseOptions\",\n            \"type\": \"json\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"description\": \"Additional options to pass to the Deepseek client. This should be a JSON object.\",\n            \"id\": \"chatDeepseek_0-input-baseOptions-json\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Cache\",\n            \"name\": \"cache\",\n            \"type\": \"BaseCache\",\n            \"optional\": true,\n            \"id\": \"chatDeepseek_0-input-cache-BaseCache\"\n          }\n        ],\n        \"inputs\": {\n          \"cache\": \"\",\n          \"modelName\": \"deepseek-chat\",\n          \"temperature\": 0.7,\n          \"streaming\": true,\n          \"maxTokens\": \"\",\n          \"topP\": \"\",\n          \"frequencyPenalty\": \"\",\n          \"presencePenalty\": \"\",\n          \"timeout\": \"\",\n          \"stopSequence\": \"\",\n          \"baseOptions\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"chatDeepseek_0-output-chatDeepseek-chatDeepseek|BaseChatModel|BaseLanguageModel|Runnable\",\n            \"name\": \"chatDeepseek\",\n            \"label\": \"chatDeepseek\",\n            \"description\": \"Wrapper around Deepseek large language models that use the Chat endpoint\",\n            \"type\": \"chatDeepseek | BaseChatModel | BaseLanguageModel | Runnable\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 573,\n      \"selected\": false,\n      \"dragging\": false,\n      \"positionAbsolute\": {\n        \"x\": -745.8788204153421,\n        \"y\": 185.6569431675053\n      }\n    },\n    {\n      \"id\": \"chatGoogleGenerativeAI_0\",\n      \"position\": {\n        \"x\": 700.5725881017097,\n        \"y\": -631.4450408339453\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"chatGoogleGenerativeAI_0\",\n        \"label\": \"ChatGoogleGenerativeAI\",\n        \"version\": 3,\n        \"name\": \"chatGoogleGenerativeAI\",\n        \"type\": \"ChatGoogleGenerativeAI\",\n        \"baseClasses\": [\n          \"ChatGoogleGenerativeAI\",\n          \"LangchainChatGoogleGenerativeAI\",\n          \"BaseChatModel\",\n          \"BaseLanguageModel\",\n          \"Runnable\"\n        ],\n        \"category\": \"Chat Models\",\n        \"description\": \"Wrapper around Google Gemini large language models that use the Chat endpoint\",\n        \"inputParams\": [\n          {\n            \"label\": \"Connect Credential\",\n            \"name\": \"credential\",\n            \"type\": \"credential\",\n            \"credentialNames\": [\n              \"googleGenerativeAI\"\n            ],\n            \"optional\": false,\n            \"description\": \"Google Generative AI credential.\",\n            \"id\": \"chatGoogleGenerativeAI_0-input-credential-credential\"\n          },\n          {\n            \"label\": \"Model Name\",\n            \"name\": \"modelName\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"default\": \"gemini-1.5-flash-latest\",\n            \"id\": \"chatGoogleGenerativeAI_0-input-modelName-asyncOptions\"\n          },\n          {\n            \"label\": \"Custom Model Name\",\n            \"name\": \"customModelName\",\n            \"type\": \"string\",\n            \"placeholder\": \"gemini-1.5-pro-exp-0801\",\n            \"description\": \"Custom model name to use. If provided, it will override the model selected\",\n            \"additionalParams\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-customModelName-string\"\n          },\n          {\n            \"label\": \"Temperature\",\n            \"name\": \"temperature\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"default\": 0.9,\n            \"optional\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-temperature-number\"\n          },\n          {\n            \"label\": \"Streaming\",\n            \"name\": \"streaming\",\n            \"type\": \"boolean\",\n            \"default\": true,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-streaming-boolean\"\n          },\n          {\n            \"label\": \"Max Output Tokens\",\n            \"name\": \"maxOutputTokens\",\n            \"type\": \"number\",\n            \"step\": 1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-maxOutputTokens-number\"\n          },\n          {\n            \"label\": \"Top Probability\",\n            \"name\": \"topP\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-topP-number\"\n          },\n          {\n            \"label\": \"Top Next Highest Probability Tokens\",\n            \"name\": \"topK\",\n            \"type\": \"number\",\n            \"description\": \"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive\",\n            \"step\": 1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-topK-number\"\n          },\n          {\n            \"label\": \"Harm Category\",\n            \"name\": \"harmCategory\",\n            \"type\": \"multiOptions\",\n            \"description\": \"Refer to <a target=\\\"_blank\\\" href=\\\"https://cloud.google.com/vertex-ai/docs/generative-ai/multimodal/configure-safety-attributes#safety_attribute_definitions\\\">official guide</a> on how to use Harm Category\",\n            \"options\": [\n              {\n                \"label\": \"Dangerous\",\n                \"name\": \"HARM_CATEGORY_DANGEROUS_CONTENT\"\n              },\n              {\n                \"label\": \"Harassment\",\n                \"name\": \"HARM_CATEGORY_HARASSMENT\"\n              },\n              {\n                \"label\": \"Hate Speech\",\n                \"name\": \"HARM_CATEGORY_HATE_SPEECH\"\n              },\n              {\n                \"label\": \"Sexually Explicit\",\n                \"name\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\"\n              }\n            ],\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-harmCategory-multiOptions\"\n          },\n          {\n            \"label\": \"Harm Block Threshold\",\n            \"name\": \"harmBlockThreshold\",\n            \"type\": \"multiOptions\",\n            \"description\": \"Refer to <a target=\\\"_blank\\\" href=\\\"https://cloud.google.com/vertex-ai/docs/generative-ai/multimodal/configure-safety-attributes#safety_setting_thresholds\\\">official guide</a> on how to use Harm Block Threshold\",\n            \"options\": [\n              {\n                \"label\": \"Low and Above\",\n                \"name\": \"BLOCK_LOW_AND_ABOVE\"\n              },\n              {\n                \"label\": \"Medium and Above\",\n                \"name\": \"BLOCK_MEDIUM_AND_ABOVE\"\n              },\n              {\n                \"label\": \"None\",\n                \"name\": \"BLOCK_NONE\"\n              },\n              {\n                \"label\": \"Only High\",\n                \"name\": \"BLOCK_ONLY_HIGH\"\n              },\n              {\n                \"label\": \"Threshold Unspecified\",\n                \"name\": \"HARM_BLOCK_THRESHOLD_UNSPECIFIED\"\n              }\n            ],\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-harmBlockThreshold-multiOptions\"\n          },\n          {\n            \"label\": \"Allow Image Uploads\",\n            \"name\": \"allowImageUploads\",\n            \"type\": \"boolean\",\n            \"description\": \"Allow image input. Refer to the <a href=\\\"https://docs.flowiseai.com/using-flowise/uploads#image\\\" target=\\\"_blank\\\">docs</a> for more details.\",\n            \"default\": false,\n            \"optional\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-allowImageUploads-boolean\"\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Cache\",\n            \"name\": \"cache\",\n            \"type\": \"BaseCache\",\n            \"optional\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-cache-BaseCache\"\n          }\n        ],\n        \"inputs\": {\n          \"cache\": \"\",\n          \"modelName\": \"gemini-2.0-flash-001\",\n          \"customModelName\": \"\",\n          \"temperature\": \"0.7\",\n          \"streaming\": true,\n          \"maxOutputTokens\": \"\",\n          \"topP\": \"\",\n          \"topK\": \"\",\n          \"harmCategory\": \"\",\n          \"harmBlockThreshold\": \"\",\n          \"allowImageUploads\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"chatGoogleGenerativeAI_0-output-chatGoogleGenerativeAI-ChatGoogleGenerativeAI|LangchainChatGoogleGenerativeAI|BaseChatModel|BaseLanguageModel|Runnable\",\n            \"name\": \"chatGoogleGenerativeAI\",\n            \"label\": \"ChatGoogleGenerativeAI\",\n            \"description\": \"Wrapper around Google Gemini large language models that use the Chat endpoint\",\n            \"type\": \"ChatGoogleGenerativeAI | LangchainChatGoogleGenerativeAI | BaseChatModel | BaseLanguageModel | Runnable\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 669,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 700.5725881017097,\n        \"y\": -631.4450408339453\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"htmlToMarkdownTextSplitter_0\",\n      \"position\": {\n        \"x\": -413.6345126132562,\n        \"y\": -350.15902143817084\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"htmlToMarkdownTextSplitter_0\",\n        \"label\": \"HtmlToMarkdown Text Splitter\",\n        \"version\": 1,\n        \"name\": \"htmlToMarkdownTextSplitter\",\n        \"type\": \"HtmlToMarkdownTextSplitter\",\n        \"baseClasses\": [\n          \"HtmlToMarkdownTextSplitter\",\n          \"MarkdownTextSplitter\",\n          \"RecursiveCharacterTextSplitter\",\n          \"TextSplitter\",\n          \"BaseDocumentTransformer\",\n          \"Runnable\"\n        ],\n        \"category\": \"Text Splitters\",\n        \"description\": \"Converts Html to Markdown and then split your content into documents based on the Markdown headers\",\n        \"inputParams\": [\n          {\n            \"label\": \"Chunk Size\",\n            \"name\": \"chunkSize\",\n            \"type\": \"number\",\n            \"description\": \"Number of characters in each chunk. Default is 1000.\",\n            \"default\": 1000,\n            \"optional\": true,\n            \"id\": \"htmlToMarkdownTextSplitter_0-input-chunkSize-number\"\n          },\n          {\n            \"label\": \"Chunk Overlap\",\n            \"name\": \"chunkOverlap\",\n            \"type\": \"number\",\n            \"description\": \"Number of characters to overlap between chunks. Default is 200.\",\n            \"default\": 200,\n            \"optional\": true,\n            \"id\": \"htmlToMarkdownTextSplitter_0-input-chunkOverlap-number\"\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"chunkSize\": 1000,\n          \"chunkOverlap\": 200\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"htmlToMarkdownTextSplitter_0-output-htmlToMarkdownTextSplitter-HtmlToMarkdownTextSplitter|MarkdownTextSplitter|RecursiveCharacterTextSplitter|TextSplitter|BaseDocumentTransformer|Runnable\",\n            \"name\": \"htmlToMarkdownTextSplitter\",\n            \"label\": \"HtmlToMarkdownTextSplitter\",\n            \"description\": \"Converts Html to Markdown and then split your content into documents based on the Markdown headers\",\n            \"type\": \"HtmlToMarkdownTextSplitter | MarkdownTextSplitter | RecursiveCharacterTextSplitter | TextSplitter | BaseDocumentTransformer | Runnable\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 377,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": -413.6345126132562,\n        \"y\": -350.15902143817084\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"googleGenerativeAiEmbeddings_0\",\n      \"position\": {\n        \"x\": -62.05748013689855,\n        \"y\": 162.83758228678468\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"googleGenerativeAiEmbeddings_0\",\n        \"label\": \"GoogleGenerativeAI Embeddings\",\n        \"version\": 2,\n        \"name\": \"googleGenerativeAiEmbeddings\",\n        \"type\": \"GoogleGenerativeAiEmbeddings\",\n        \"baseClasses\": [\n          \"GoogleGenerativeAiEmbeddings\",\n          \"Embeddings\"\n        ],\n        \"category\": \"Embeddings\",\n        \"description\": \"Google Generative API to generate embeddings for a given text\",\n        \"inputParams\": [\n          {\n            \"label\": \"Connect Credential\",\n            \"name\": \"credential\",\n            \"type\": \"credential\",\n            \"credentialNames\": [\n              \"googleGenerativeAI\"\n            ],\n            \"optional\": false,\n            \"description\": \"Google Generative AI credential.\",\n            \"id\": \"googleGenerativeAiEmbeddings_0-input-credential-credential\"\n          },\n          {\n            \"label\": \"Model Name\",\n            \"name\": \"modelName\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"default\": \"embedding-001\",\n            \"id\": \"googleGenerativeAiEmbeddings_0-input-modelName-asyncOptions\"\n          },\n          {\n            \"label\": \"Task Type\",\n            \"name\": \"tasktype\",\n            \"type\": \"options\",\n            \"description\": \"Type of task for which the embedding will be used\",\n            \"options\": [\n              {\n                \"label\": \"TASK_TYPE_UNSPECIFIED\",\n                \"name\": \"TASK_TYPE_UNSPECIFIED\"\n              },\n              {\n                \"label\": \"RETRIEVAL_QUERY\",\n                \"name\": \"RETRIEVAL_QUERY\"\n              },\n              {\n                \"label\": \"RETRIEVAL_DOCUMENT\",\n                \"name\": \"RETRIEVAL_DOCUMENT\"\n              },\n              {\n                \"label\": \"SEMANTIC_SIMILARITY\",\n                \"name\": \"SEMANTIC_SIMILARITY\"\n              },\n              {\n                \"label\": \"CLASSIFICATION\",\n                \"name\": \"CLASSIFICATION\"\n              },\n              {\n                \"label\": \"CLUSTERING\",\n                \"name\": \"CLUSTERING\"\n              }\n            ],\n            \"default\": \"TASK_TYPE_UNSPECIFIED\",\n            \"id\": \"googleGenerativeAiEmbeddings_0-input-tasktype-options\"\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"modelName\": \"text-embedding-004\",\n          \"tasktype\": \"CLASSIFICATION\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"googleGenerativeAiEmbeddings_0-output-googleGenerativeAiEmbeddings-GoogleGenerativeAiEmbeddings|Embeddings\",\n            \"name\": \"googleGenerativeAiEmbeddings\",\n            \"label\": \"GoogleGenerativeAiEmbeddings\",\n            \"description\": \"Google Generative API to generate embeddings for a given text\",\n            \"type\": \"GoogleGenerativeAiEmbeddings | Embeddings\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 467,\n      \"positionAbsolute\": {\n        \"x\": -62.05748013689855,\n        \"y\": 162.83758228678468\n      },\n      \"selected\": false,\n      \"dragging\": false\n    }\n  ],\n  \"edges\": [\n    {\n      \"source\": \"bufferMemory_0\",\n      \"sourceHandle\": \"bufferMemory_0-output-bufferMemory-BufferMemory|BaseChatMemory|BaseMemory\",\n      \"target\": \"conversationalRetrievalQAChain_0\",\n      \"targetHandle\": \"conversationalRetrievalQAChain_0-input-memory-BaseMemory\",\n      \"type\": \"buttonedge\",\n      \"id\": \"bufferMemory_0-bufferMemory_0-output-bufferMemory-BufferMemory|BaseChatMemory|BaseMemory-conversationalRetrievalQAChain_0-conversationalRetrievalQAChain_0-input-memory-BaseMemory\"\n    },\n    {\n      \"source\": \"puppeteerWebScraper_0\",\n      \"sourceHandle\": \"puppeteerWebScraper_0-output-document-Document|json\",\n      \"target\": \"faiss_0\",\n      \"targetHandle\": \"faiss_0-input-document-Document\",\n      \"type\": \"buttonedge\",\n      \"id\": \"puppeteerWebScraper_0-puppeteerWebScraper_0-output-document-Document|json-faiss_0-faiss_0-input-document-Document\"\n    },\n    {\n      \"source\": \"faiss_0\",\n      \"sourceHandle\": \"faiss_0-output-retriever-Faiss|VectorStoreRetriever|BaseRetriever\",\n      \"target\": \"conversationalRetrievalQAChain_0\",\n      \"targetHandle\": \"conversationalRetrievalQAChain_0-input-vectorStoreRetriever-BaseRetriever\",\n      \"type\": \"buttonedge\",\n      \"id\": \"faiss_0-faiss_0-output-retriever-Faiss|VectorStoreRetriever|BaseRetriever-conversationalRetrievalQAChain_0-conversationalRetrievalQAChain_0-input-vectorStoreRetriever-BaseRetriever\"\n    },\n    {\n      \"source\": \"htmlToMarkdownTextSplitter_0\",\n      \"sourceHandle\": \"htmlToMarkdownTextSplitter_0-output-htmlToMarkdownTextSplitter-HtmlToMarkdownTextSplitter|MarkdownTextSplitter|RecursiveCharacterTextSplitter|TextSplitter|BaseDocumentTransformer|Runnable\",\n      \"target\": \"puppeteerWebScraper_0\",\n      \"targetHandle\": \"puppeteerWebScraper_0-input-textSplitter-TextSplitter\",\n      \"type\": \"buttonedge\",\n      \"id\": \"htmlToMarkdownTextSplitter_0-htmlToMarkdownTextSplitter_0-output-htmlToMarkdownTextSplitter-HtmlToMarkdownTextSplitter|MarkdownTextSplitter|RecursiveCharacterTextSplitter|TextSplitter|BaseDocumentTransformer|Runnable-puppeteerWebScraper_0-puppeteerWebScraper_0-input-textSplitter-TextSplitter\"\n    },\n    {\n      \"source\": \"chatGoogleGenerativeAI_0\",\n      \"sourceHandle\": \"chatGoogleGenerativeAI_0-output-chatGoogleGenerativeAI-ChatGoogleGenerativeAI|LangchainChatGoogleGenerativeAI|BaseChatModel|BaseLanguageModel|Runnable\",\n      \"target\": \"conversationalRetrievalQAChain_0\",\n      \"targetHandle\": \"conversationalRetrievalQAChain_0-input-model-BaseChatModel\",\n      \"type\": \"buttonedge\",\n      \"id\": \"chatGoogleGenerativeAI_0-chatGoogleGenerativeAI_0-output-chatGoogleGenerativeAI-ChatGoogleGenerativeAI|LangchainChatGoogleGenerativeAI|BaseChatModel|BaseLanguageModel|Runnable-conversationalRetrievalQAChain_0-conversationalRetrievalQAChain_0-input-model-BaseChatModel\"\n    },\n    {\n      \"source\": \"googleGenerativeAiEmbeddings_0\",\n      \"sourceHandle\": \"googleGenerativeAiEmbeddings_0-output-googleGenerativeAiEmbeddings-GoogleGenerativeAiEmbeddings|Embeddings\",\n      \"target\": \"faiss_0\",\n      \"targetHandle\": \"faiss_0-input-embeddings-Embeddings\",\n      \"type\": \"buttonedge\",\n      \"id\": \"googleGenerativeAiEmbeddings_0-googleGenerativeAiEmbeddings_0-output-googleGenerativeAiEmbeddings-GoogleGenerativeAiEmbeddings|Embeddings-faiss_0-faiss_0-input-embeddings-Embeddings\"\n    }\n  ]\n}",
      "type": "CHATFLOW"
    }
  ],
  "AgentFlow": [],
  "Variable": [],
  "Assistant": [
    {
      "id": "783f467e-7dad-4af5-bd9e-a5e5bf737b58",
      "details": "{\"name\":\"bala\",\"chatModel\":{\"loadMethods\":{},\"label\":\"ChatCohere\",\"name\":\"chatCohere\",\"version\":2,\"type\":\"ChatCohere\",\"icon\":\"C:/Users/balas/AppData/Roaming/npm/node_modules/flowise/node_modules/flowise-components/dist/nodes/chatmodels/ChatCohere/Cohere.svg\",\"category\":\"Chat Models\",\"description\":\"Wrapper around Cohere Chat Endpoints\",\"baseClasses\":[\"ChatCohere\",\"BaseChatModel\",\"BaseLanguageModel\",\"Runnable\"],\"credential\":\"55192755-4e0b-4fb8-b261-d9f274b8711f\",\"inputs\":{\"cache\":\"\",\"modelName\":\"command-r\",\"temperature\":0.7,\"streaming\":true,\"FLOWISE_CREDENTIAL_ID\":\"55192755-4e0b-4fb8-b261-d9f274b8711f\"},\"filePath\":\"C:\\\\Users\\\\balas\\\\AppData\\\\Roaming\\\\npm\\\\node_modules\\\\flowise\\\\node_modules\\\\flowise-components\\\\dist\\\\nodes\\\\chatmodels\\\\ChatCohere\\\\ChatCohere.js\",\"inputAnchors\":[{\"label\":\"Cache\",\"name\":\"cache\",\"type\":\"BaseCache\",\"optional\":true,\"id\":\"chatCohere_0-input-cache-BaseCache\"}],\"inputParams\":[{\"label\":\"Connect Credential\",\"name\":\"credential\",\"type\":\"credential\",\"credentialNames\":[\"cohereApi\"],\"id\":\"chatCohere_0-input-credential-credential\"},{\"label\":\"Model Name\",\"name\":\"modelName\",\"type\":\"asyncOptions\",\"loadMethod\":\"listModels\",\"default\":\"command-r\",\"id\":\"chatCohere_0-input-modelName-asyncOptions\"},{\"label\":\"Temperature\",\"name\":\"temperature\",\"type\":\"number\",\"step\":0.1,\"default\":0.7,\"optional\":true,\"id\":\"chatCohere_0-input-temperature-number\"},{\"label\":\"Streaming\",\"name\":\"streaming\",\"type\":\"boolean\",\"default\":true,\"optional\":true,\"id\":\"chatCohere_0-input-streaming-boolean\"}],\"outputs\":{},\"outputAnchors\":[{\"id\":\"chatCohere_0-output-chatCohere-ChatCohere|BaseChatModel|BaseLanguageModel|Runnable\",\"name\":\"chatCohere\",\"label\":\"ChatCohere\",\"description\":\"Wrapper around Cohere Chat Endpoints\",\"type\":\"ChatCohere | BaseChatModel | BaseLanguageModel | Runnable\"}],\"id\":\"chatCohere_0\"},\"instruction\":\"You are helpful assistant\",\"flowId\":\"783f467e-7dad-4af5-bd9e-a5e5bf737b58\",\"documentStores\":[],\"tools\":[]}",
      "credential": "a01316db-2ce9-4dd5-8a08-83b35cdf91b1",
      "iconSrc": null
    }
  ]
}